[
  {
    "title": "FastAPI lifespan event for startup and shutdown tasks",
    "context": "I need to initialize resources (database connection pool, Redis client, HTTP client) when my FastAPI app starts and clean them up when it shuts down. The old @app.on_event('startup') pattern is deprecated in newer FastAPI versions.",
    "solution": "Use the `lifespan` context manager pattern (FastAPI 0.93+):\n\n```python\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI\nimport httpx\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup: initialize resources\n    app.state.http_client = httpx.AsyncClient(timeout=10.0)\n    app.state.redis = await create_redis_connection()\n    yield\n    # Shutdown: close resources\n    await app.state.http_client.aclose()\n    await app.state.redis.aclose()\n\napp = FastAPI(lifespan=lifespan)\n```\n\nAccess in dependencies:\n```python\nfrom fastapi import Request\n\ndef get_http_client(request: Request) -> httpx.AsyncClient:\n    return request.app.state.http_client\n```\n\nKey points:\n- Everything before `yield` runs at startup, after `yield` at shutdown\n- Resources on `app.state` are accessible from any route via `request.app.state`\n- Lifespan replaces both `@app.on_event('startup')` and `@app.on_event('shutdown')`\n- FastAPI wraps the lifespan in an asynccontextmanager automatically",
    "tags": [
      "python",
      "fastapi",
      "async"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI dependency injection with typed dependencies",
    "context": "I want to use FastAPI's dependency injection system cleanly across route handlers. I have database sessions, current user extraction from JWT, and Redis clients that need to be injected. I want to use type aliases to keep route signatures readable.",
    "solution": "Use `Annotated` type aliases for clean dependency injection:\n\n```python\nfrom typing import Annotated\nfrom fastapi import Depends, HTTPException\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.database import get_db\nfrom app.auth import get_current_user\n\n# Type aliases \u2014 these are the core pattern\nDbSession = Annotated[AsyncSession, Depends(get_db)]\nCurrentUser = Annotated[User, Depends(get_current_user)]\n\n# Sub-dependencies compose naturally\nasync def get_admin_user(user: CurrentUser) -> User:\n    if not user.is_admin:\n        raise HTTPException(403, 'Admin required')\n    return user\n\nAdminUser = Annotated[User, Depends(get_admin_user)]\n\n# Route handlers stay clean:\n@router.post('/traces')\nasync def create_trace(body: TraceCreate, db: DbSession, user: CurrentUser):\n    trace = Trace(contributor_id=user.id, ...)\n    db.add(trace)\n    await db.commit()\n    return trace\n```\n\nKey points:\n- `Annotated[T, Depends(fn)]` is the v2 pattern \u2014 cleaner than `= Depends(fn)` defaults\n- Type aliases are importable, so define them in `app/dependencies.py`\n- FastAPI caches dependencies per request by default (same instance within one request)",
    "tags": [
      "python",
      "fastapi",
      "dependency-injection"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI background tasks for async post-response processing",
    "context": "After handling an API request (e.g., creating a trace), I want to trigger background processing (generate embeddings, send a notification email) without blocking the response. I need this to run after the response is sent to the client.",
    "solution": "Use FastAPI's `BackgroundTasks`:\n\n```python\nfrom fastapi import BackgroundTasks\n\nasync def generate_embedding(trace_id: str) -> None:\n    \"\"\"Runs after the response is returned to the client.\"\"\"\n    embedding = await call_openai_embeddings(trace_id)\n    await store_embedding(trace_id, embedding)\n\n@router.post('/traces', status_code=201)\nasync def create_trace(\n    body: TraceCreate,\n    background_tasks: BackgroundTasks,\n    db: DbSession,\n):\n    trace = Trace(**body.model_dump())\n    db.add(trace)\n    await db.commit()\n    await db.refresh(trace)\n    \n    # Schedule background work AFTER commit\n    background_tasks.add_task(generate_embedding, str(trace.id))\n    \n    return trace\n```\n\nFor heavier workloads, prefer Celery or arq:\n```python\n# arq worker task\nasync def process_embedding(ctx, trace_id: str):\n    await generate_and_store_embedding(trace_id)\n\n# Enqueue from route:\nawait redis.enqueue_job('process_embedding', str(trace.id))\n```\n\nKey points:\n- `BackgroundTasks` run in the same process after the response is sent\n- Not suitable for tasks that take >30s or need retry logic \u2014 use arq/Celery\n- Always commit to DB before scheduling background tasks that read that data",
    "tags": [
      "python",
      "fastapi",
      "async",
      "background-tasks"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pydantic v2 Settings with environment variable validation",
    "context": "I need to manage configuration for a FastAPI app across local dev, CI, and production environments. I use environment variables and .env files. I want type-safe settings with validation, defaults, and support for secrets stored in env vars.",
    "solution": "Use `pydantic-settings` for type-safe configuration:\n\n```python\nfrom pydantic import AnyHttpUrl, field_validator, model_validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom typing import Self\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file='.env',\n        env_file_encoding='utf-8',\n        case_sensitive=False,\n    )\n    \n    database_url: str\n    redis_url: str = 'redis://localhost:6379'\n    secret_key: str\n    debug: bool = False\n    allowed_hosts: list[str] = ['localhost']\n    openai_api_key: str = ''\n    \n    @field_validator('database_url')\n    @classmethod\n    def validate_db_url(cls, v: str) -> str:\n        if not v.startswith('postgresql+asyncpg://'):\n            raise ValueError('database_url must use asyncpg driver')\n        return v\n    \n    @model_validator(mode='after')\n    def warn_missing_keys(self) -> Self:\n        if not self.openai_api_key:\n            import warnings\n            warnings.warn('OPENAI_API_KEY not set \u2014 embeddings disabled')\n        return self\n\nsettings = Settings()  # Singleton \u2014 import from here\n```\n\nKey points:\n- `BaseSettings` reads from env vars first, then `.env` file, then defaults\n- Validators run AFTER env var parsing \u2014 use `mode='before'` to transform raw strings\n- Sensitive values use `SecretStr` type: `secret_key: SecretStr` (prevents logging)\n- Use `settings = Settings()` as module-level singleton to fail fast at import",
    "tags": [
      "python",
      "pydantic",
      "configuration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy 2.0 async bulk insert with returning",
    "context": "I need to bulk insert thousands of rows into PostgreSQL using SQLAlchemy 2.0 async. I want to do it efficiently with a single query (not one INSERT per row) and get back the generated IDs without a second SELECT.",
    "solution": "Use `insert().values()` with `returning()` for efficient bulk inserts:\n\n```python\nfrom sqlalchemy import insert, select\nfrom app.models.trace import Trace\n\nasync def bulk_insert_traces(session: AsyncSession, traces: list[dict]) -> list[uuid.UUID]:\n    if not traces:\n        return []\n    \n    # Single bulk INSERT with RETURNING id\n    stmt = (\n        insert(Trace)\n        .values(traces)  # list of dicts\n        .returning(Trace.id)\n    )\n    result = await session.execute(stmt)\n    inserted_ids = result.scalars().all()\n    await session.commit()\n    return inserted_ids\n\n# Usage:\nrows = [\n    {\n        'title': f'Trace {i}',\n        'context_text': 'Context...',\n        'solution_text': 'Solution...',\n        'contributor_id': user_id,\n        'status': 'validated',\n    }\n    for i in range(1000)\n]\nids = await bulk_insert_traces(session, rows)\n```\n\nFor upsert (INSERT ... ON CONFLICT DO UPDATE):\n```python\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\n\nstmt = pg_insert(Tag).values(name='python')\nstmt = stmt.on_conflict_do_nothing(index_elements=['name'])\nawait session.execute(stmt)\n```\n\nKey points:\n- Single bulk INSERT is 10-100x faster than a loop of individual inserts\n- `RETURNING` avoids a second SELECT query to get generated IDs\n- PostgreSQL-specific `pg_insert` for upsert (`on_conflict_do_update`)\n- Batch by 500-1000 rows to avoid hitting PostgreSQL parameter limits",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql",
      "async"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy 2.0 async relationship loading strategies",
    "context": "I'm getting 'MissingGreenlet' or 'greenlet_spawn has not been called' errors when accessing related objects on my SQLAlchemy models in an async context. I need to understand which lazy loading strategies work with async and how to eagerly load related data.",
    "solution": "Use `selectin` or `joined` loading \u2014 lazy loading is broken in async:\n\n```python\nfrom sqlalchemy.orm import selectinload, joinedload\nfrom sqlalchemy import select\n\n# selectinload \u2014 runs a separate SELECT IN query (best for to-many)\nresult = await session.execute(\n    select(Trace)\n    .options(selectinload(Trace.tags))\n    .where(Trace.id == trace_id)\n)\ntrace = result.scalar_one_or_none()\n# trace.tags is now loaded \u2014 safe to access\n\n# joinedload \u2014 uses LEFT OUTER JOIN (best for to-one)\nresult = await session.execute(\n    select(Trace)\n    .options(joinedload(Trace.contributor))\n    .where(Trace.id == trace_id)\n)\ntrace = result.scalar_one()  # use scalar_one() not scalar_one_or_none() with joinedload\n\n# Nested loading:\nresult = await session.execute(\n    select(User)\n    .options(selectinload(User.traces).selectinload(Trace.tags))\n)\n```\n\nPrevent accidental lazy loads with `lazy='raise'`:\n```python\nclass Trace(Base):\n    contributor: Mapped['User'] = relationship('User', lazy='raise')\n```\n\nKey points:\n- Never use default `lazy='select'` in async (raises `MissingGreenlet`)\n- `selectinload` for one-to-many and many-to-many (separate query, efficient)\n- `joinedload` for many-to-one (JOIN, may cause row multiplication on collections)\n- `lazy='raise'` in model definition catches accidental lazy access at test time",
    "tags": [
      "python",
      "sqlalchemy",
      "async",
      "orm"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI API versioning with router prefixes",
    "context": "I need to version my FastAPI API (v1, v2 eventually) without breaking existing clients when I introduce breaking changes. I want to organize routes in separate router modules per version and apply versioned URL prefixes.",
    "solution": "Use router includes with version prefixes:\n\n```python\n# api/app/routers/v1/__init__.py\nfrom fastapi import APIRouter\nfrom .traces import router as traces_router\nfrom .search import router as search_router\nfrom .votes import router as votes_router\n\nv1_router = APIRouter(prefix='/v1')\nv1_router.include_router(traces_router, prefix='/traces', tags=['traces'])\nv1_router.include_router(search_router, prefix='/traces', tags=['search'])\nv1_router.include_router(votes_router, prefix='/votes', tags=['votes'])\n\n# api/app/main.py\nfrom fastapi import FastAPI\nfrom app.routers.v1 import v1_router\n\napp = FastAPI()\napp.include_router(v1_router, prefix='/api')\n# Routes are now at /api/v1/traces, /api/v1/search, etc.\n\n# For v2 (future):\n# app.include_router(v2_router, prefix='/api')\n```\n\nShared dependencies across versions:\n```python\n# Shared auth works for both:\nasync def require_auth(api_key: str = Header(alias='X-API-Key')):\n    ...\n\n# Apply at router level:\nv1_router = APIRouter(prefix='/v1', dependencies=[Depends(require_auth)])\n```\n\nKey points:\n- Include `prefix='/api'` on the app level so all API routes are under `/api`\n- Separate router modules per resource keep files manageable\n- Per-router `tags` keeps OpenAPI documentation organized\n- Apply auth at router level with `dependencies=[]` to avoid repeating it per route",
    "tags": [
      "python",
      "fastapi",
      "api-design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "structlog structured logging setup for FastAPI",
    "context": "I need structured JSON logging in my FastAPI application. I want request IDs correlated across log lines, log levels configurable via env var, and logs formatted as JSON in production but human-readable in development.",
    "solution": "Configure structlog for structured logging:\n\n```python\n# app/logging.py\nimport logging\nimport sys\nimport structlog\n\ndef configure_logging(log_level: str = 'INFO', json_logs: bool = False) -> None:\n    shared_processors = [\n        structlog.contextvars.merge_contextvars,\n        structlog.processors.add_log_level,\n        structlog.processors.TimeStamper(fmt='iso'),\n    ]\n    \n    if json_logs:\n        renderer = structlog.processors.JSONRenderer()\n    else:\n        renderer = structlog.dev.ConsoleRenderer()\n    \n    structlog.configure(\n        processors=shared_processors + [renderer],\n        logger_factory=structlog.PrintLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n    logging.basicConfig(level=log_level, stream=sys.stdout, format='%(message)s')\n\n# Middleware to inject request_id into every log in a request:\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport uuid\n\nclass RequestIDMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        request_id = str(uuid.uuid4())\n        structlog.contextvars.bind_contextvars(request_id=request_id)\n        response = await call_next(request)\n        structlog.contextvars.unbind_contextvars('request_id')\n        response.headers['X-Request-ID'] = request_id\n        return response\n\n# Usage:\nlog = structlog.get_logger(__name__)\nawait log.ainfo('trace_created', trace_id=str(trace.id), user_id=str(user_id))\n```\n\nKey points:\n- `merge_contextvars` injects request-scoped data (request_id) into every log line\n- `cache_logger_on_first_use=True` for performance in production\n- Set `json_logs=True` in production for log aggregation tools (Datadog, CloudWatch)\n- Use `structlog.get_logger()` not `logging.getLogger()` for structured output",
    "tags": [
      "python",
      "logging",
      "fastapi",
      "structlog"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "asyncio TaskGroup for concurrent async operations",
    "context": "I need to run multiple async operations concurrently in Python and collect all their results. Some operations may fail and I want to handle errors per-task. I'm using Python 3.11+ and want to use the modern approach.",
    "solution": "Use `asyncio.TaskGroup` (Python 3.11+) for structured concurrency:\n\n```python\nimport asyncio\nfrom typing import Any\n\nasync def fetch_trace_details(trace_id: str) -> dict:\n    \"\"\"Concurrently fetch trace data, tags, and vote counts.\"\"\"\n    async with asyncio.TaskGroup() as tg:\n        trace_task = tg.create_task(get_trace(trace_id))\n        tags_task = tg.create_task(get_tags(trace_id))\n        votes_task = tg.create_task(get_vote_count(trace_id))\n    # All tasks done here \u2014 any exception propagates as ExceptionGroup\n    return {\n        'trace': trace_task.result(),\n        'tags': tags_task.result(),\n        'votes': votes_task.result(),\n    }\n\n# For Python 3.10 and earlier, use asyncio.gather:\nresults = await asyncio.gather(\n    get_trace(trace_id),\n    get_tags(trace_id),\n    get_vote_count(trace_id),\n    return_exceptions=True,\n)\n\n# Handle per-task errors with gather:\nfor result in results:\n    if isinstance(result, Exception):\n        log.error('Task failed', exc_info=result)\n    else:\n        process(result)\n```\n\nKey points:\n- `TaskGroup` cancels all tasks if any raises \u2014 use `gather(return_exceptions=True)` for independent failures\n- `ExceptionGroup` (Python 3.11+) wraps multiple failures \u2014 catch with `except*`\n- Use `asyncio.timeout()` inside tasks to prevent indefinite hangs\n- `TaskGroup` is preferred over `gather` when all tasks must succeed together",
    "tags": [
      "python",
      "async",
      "asyncio",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest async test fixtures with SQLAlchemy and FastAPI",
    "context": "I need to write async pytest tests for my FastAPI application. I need a test database that's rolled back after each test (not truncated), and I need to override FastAPI's database dependency so tests use the test session. Currently my tests are slow because they recreate the DB on every test.",
    "solution": "Use transaction rollback for fast isolated tests:\n\n```python\n# conftest.py\nimport pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\nfrom app.main import app\nfrom app.database import get_db\nfrom app.models.base import Base\n\nTEST_DB_URL = 'postgresql+asyncpg://test:test@localhost:5432/test_db'\n\n@pytest.fixture(scope='session')\ndef event_loop_policy():\n    return asyncio.DefaultEventLoopPolicy()\n\n@pytest_asyncio.fixture(scope='session')\nasync def engine():\n    engine = create_async_engine(TEST_DB_URL)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    await engine.dispose()\n\n@pytest_asyncio.fixture\nasync def session(engine):\n    async with engine.begin() as conn:\n        async with async_sessionmaker(conn, expire_on_commit=False)() as sess:\n            yield sess\n            await sess.rollback()  # <- rollback after each test\n\n@pytest_asyncio.fixture\nasync def client(session):\n    app.dependency_overrides[get_db] = lambda: session\n    async with AsyncClient(transport=ASGITransport(app=app), base_url='http://test') as c:\n        yield c\n    app.dependency_overrides.clear()\n```\n\nKey points:\n- Transaction rollback is 10-50x faster than DROP/CREATE or TRUNCATE between tests\n- `dependency_overrides` swaps out the real DB session for the test session\n- `scope='session'` on engine shares the connection pool across all tests\n- Use `pytest-asyncio` with `asyncio_mode = 'auto'` in pytest.ini",
    "tags": [
      "python",
      "pytest",
      "testing",
      "sqlalchemy",
      "fastapi"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI middleware for request timing and metrics",
    "context": "I want to measure API request duration and expose it as a Prometheus metric. I need middleware that times every request, records the route, method, and status code, and exposes a /metrics endpoint for Prometheus scraping.",
    "solution": "Add Prometheus middleware with request duration histogram:\n\n```python\n# pip install prometheus-client\nfrom prometheus_client import Histogram, Counter, generate_latest, CONTENT_TYPE_LATEST\nfrom fastapi import FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\nimport time\n\nREQUEST_DURATION = Histogram(\n    'http_request_duration_seconds',\n    'HTTP request duration',\n    ['method', 'route', 'status_code'],\n    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0],\n)\n\n@app.middleware('http')\nasync def metrics_middleware(request: Request, call_next):\n    start = time.perf_counter()\n    response = await call_next(request)\n    duration = time.perf_counter() - start\n    \n    # Get route template (e.g., /traces/{trace_id} not /traces/abc-123)\n    route = request.scope.get('route')\n    path = route.path if isinstance(route, APIRoute) else request.url.path\n    \n    REQUEST_DURATION.labels(\n        method=request.method,\n        route=path,\n        status_code=str(response.status_code),\n    ).observe(duration)\n    return response\n\n@app.get('/metrics', include_in_schema=False)\nasync def metrics():\n    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n```\n\nKey points:\n- Use route template (`/traces/{id}`) not actual URL to avoid high cardinality\n- `time.perf_counter()` is more accurate than `time.time()` for short durations\n- Avoid creating new metrics inside request handlers \u2014 define at module level\n- Exclude `/metrics` and `/health` from your own metrics to avoid noise",
    "tags": [
      "python",
      "fastapi",
      "prometheus",
      "monitoring"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy 2.0 select with filters and pagination",
    "context": "I need to implement a paginated list endpoint in FastAPI. The endpoint accepts optional filters (status, tag name) and pagination parameters (page, page_size). I want to avoid N+1 queries and get total count efficiently.",
    "solution": "Use a single query with COUNT in a subquery for efficient pagination:\n\n```python\nfrom sqlalchemy import select, func\nfrom sqlalchemy.orm import selectinload\n\nasync def list_traces(\n    session: AsyncSession,\n    status: str | None = None,\n    tag: str | None = None,\n    page: int = 1,\n    page_size: int = 20,\n) -> tuple[list[Trace], int]:\n    # Base query\n    base_q = select(Trace).options(selectinload(Trace.tags))\n    \n    if status:\n        base_q = base_q.where(Trace.status == status)\n    if tag:\n        base_q = base_q.where(\n            Trace.tags.any(Tag.name == normalize_tag(tag))\n        )\n    \n    # Count query\n    count_q = select(func.count()).select_from(base_q.subquery())\n    total = (await session.execute(count_q)).scalar_one()\n    \n    # Paginated results\n    offset = (page - 1) * page_size\n    result = await session.execute(\n        base_q.order_by(Trace.created_at.desc())\n              .offset(offset)\n              .limit(page_size)\n    )\n    traces = result.scalars().unique().all()\n    \n    return traces, total\n```\n\nRoute:\n```python\n@router.get('/traces')\nasync def list_traces_route(page: int = 1, page_size: int = Query(20, le=100)):\n    traces, total = await list_traces(session, page=page, page_size=page_size)\n    return {'items': traces, 'total': total, 'page': page, 'pages': math.ceil(total / page_size)}\n```\n\nKey points:\n- `.scalars().unique().all()` deduplicates rows from JOIN-based selectinload\n- `Query(20, le=100)` caps page size at 100 to prevent abuse\n- Build filter conditions dynamically by chaining `.where()` calls\n- Use `OFFSET/LIMIT` for simple cases; keyset pagination for >10k rows",
    "tags": [
      "python",
      "sqlalchemy",
      "fastapi",
      "pagination"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pydantic v2 model for API request and response with field aliasing",
    "context": "My API uses snake_case internally (Python convention) but the frontend expects camelCase JSON. I need Pydantic models that accept and output camelCase JSON while keeping snake_case attribute names in Python code.",
    "solution": "Use `model_config` with `alias_generator` for automatic camelCase conversion:\n\n```python\nfrom pydantic import BaseModel, ConfigDict, Field\nfrom pydantic.alias_generators import to_camel\nimport uuid\nfrom datetime import datetime\n\nclass APIModel(BaseModel):\n    \"\"\"Base model for all API request/response schemas.\"\"\"\n    model_config = ConfigDict(\n        alias_generator=to_camel,\n        populate_by_name=True,  # Allow snake_case in Python code\n    )\n\nclass TraceResponse(APIModel):\n    id: uuid.UUID\n    title: str\n    context_text: str   # -> contextText in JSON\n    solution_text: str  # -> solutionText in JSON\n    trust_score: float  # -> trustScore in JSON\n    created_at: datetime\n\n# Serialize with aliases:\ntrace_response.model_dump(by_alias=True)\n# -> {'id': ..., 'contextText': ..., 'solutionText': ...}\n\n# FastAPI: tell it to use aliases in responses:\n@router.get('/traces/{trace_id}', response_model=TraceResponse)\nasync def get_trace(trace_id: uuid.UUID):\n    trace = await fetch_trace(trace_id)\n    return JSONResponse(content=trace_response.model_dump(by_alias=True))\n```\n\nOr set globally in FastAPI:\n```python\napp = FastAPI(generate_unique_id_function=lambda route: route.name)\n# Use response_model_by_alias=True per route, or override in app default\n```\n\nKey points:\n- `populate_by_name=True` allows both `context_text` and `contextText` as input\n- `to_camel` from pydantic handles snake_case -> camelCase automatically\n- Use `model_dump(by_alias=True)` when serializing for JSON response\n- Separate request (input) and response (output) models for flexibility",
    "tags": [
      "python",
      "pydantic",
      "fastapi",
      "api-design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest parametrize for testing multiple input cases",
    "context": "I have a function that handles multiple edge cases and I want to test all of them without writing a separate test function per case. I want to use pytest's parametrize decorator to test with different inputs and expected outputs, including error cases.",
    "solution": "Use `@pytest.mark.parametrize` for data-driven tests:\n\n```python\nimport pytest\nfrom app.services.tags import normalize_tag, validate_tag\n\n@pytest.mark.parametrize('raw,expected', [\n    ('Python',      'python'),\n    ('  React  ',   'react'),\n    ('Node.js',     'node.js'),\n    ('type-script', 'type-script'),\n    ('A' * 60,      'a' * 50),  # truncated to 50\n])\ndef test_normalize_tag(raw: str, expected: str):\n    assert normalize_tag(raw) == expected\n\n@pytest.mark.parametrize('tag,valid', [\n    ('python',    True),\n    ('node.js',   True),\n    ('my-tag',    True),\n    ('',          False),\n    ('has space', False),\n    ('hello!',    False),\n])\ndef test_validate_tag(tag: str, valid: bool):\n    assert validate_tag(tag) == valid\n\n# Testing exceptions:\n@pytest.mark.parametrize('input,exc_type,match', [\n    ('',    ValueError, 'empty'),\n    (None,  TypeError,  'string'),\n])\ndef test_errors(input, exc_type, match):\n    with pytest.raises(exc_type, match=match):\n        process_input(input)\n\n# IDs for readable output:\n@pytest.mark.parametrize('n', [0, 1, 100], ids=['zero', 'one', 'hundred'])\ndef test_count(n):\n    assert count_items(n) >= 0\n```\n\nKey points:\n- Each parametrize tuple becomes a separate test case in the report\n- Use `ids=` for human-readable test names instead of `input0, input1, ...`\n- Combine multiple `@parametrize` decorators for combinatorial testing (n*m cases)\n- Use `pytest.param(..., marks=pytest.mark.xfail)` to mark expected failures",
    "tags": [
      "python",
      "pytest",
      "testing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI rate limiting with Redis token bucket",
    "context": "I need to rate limit my FastAPI API endpoints. I want different limits for read vs write operations (e.g., 60 reads/min, 20 writes/min per API key). The rate limiter must be atomic to handle concurrent requests correctly and must not block the request if Redis is down.",
    "solution": "Use a Lua-based token bucket in Redis for atomic rate limiting:\n\n```lua\n-- rate_limit.lua (embed as string in Python)\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])  -- tokens per second\nlocal now = tonumber(ARGV[3])  -- current time in ms\n\nlocal data = redis.call('HMGET', key, 'tokens', 'last_refill')\nlocal tokens = tonumber(data[1]) or capacity\nlocal last_refill = tonumber(data[2]) or now\n\nlocal elapsed = (now - last_refill) / 1000.0\nlocal new_tokens = math.min(capacity, tokens + elapsed * refill_rate)\n\nif new_tokens >= 1 then\n    redis.call('HMSET', key, 'tokens', new_tokens - 1, 'last_refill', now)\n    redis.call('EXPIRE', key, 120)\n    return 1  -- allowed\nelse\n    return 0  -- rate limited\nend\n```\n\n```python\nasync def check_rate_limit(redis, api_key: str, capacity: int, per_minute: int) -> bool:\n    \"\"\"Returns True if request is allowed.\"\"\"\n    try:\n        key = f'rl:{api_key}'\n        now = int(time.time() * 1000)\n        result = await redis.eval(LUA_SCRIPT, 1, key, capacity, per_minute/60.0, now)\n        return bool(result)\n    except Exception:\n        return True  # Fail open if Redis unavailable\n\n# Dependency:\nasync def require_write_limit(api_key: CurrentAPIKey, redis=Depends(get_redis)):\n    allowed = await check_rate_limit(redis, api_key, capacity=20, per_minute=20)\n    if not allowed:\n        raise HTTPException(429, 'Write rate limit exceeded')\n```\n\nKey points:\n- Lua scripts run atomically on Redis \u2014 no race conditions\n- Fail open (return True) when Redis is unavailable \u2014 prefer availability over strict limits\n- Token bucket is smoother than fixed window (allows bursts up to capacity)\n- Use separate keys per API key and per endpoint category (read vs write)",
    "tags": [
      "python",
      "redis",
      "rate-limiting",
      "fastapi"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy async session scoping for background workers",
    "context": "I have a background worker (not FastAPI) that processes traces in a loop. Each batch of traces needs its own database session. I need to avoid session reuse across batches, handle connection pool exhaustion, and ensure sessions are properly closed even if an exception occurs.",
    "solution": "Use `async_sessionmaker` as a context manager per unit of work:\n\n```python\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession\n\nengine = create_async_engine(\n    settings.database_url,\n    pool_size=5,\n    max_overflow=10,\n    pool_pre_ping=True,  # Test connection before use\n)\nasync_session = async_sessionmaker(engine, expire_on_commit=False)\n\nasync def process_batch(trace_ids: list[str]) -> None:\n    # New session per batch \u2014 automatically closed on exit\n    async with async_session() as session:\n        async with session.begin():  # Auto-commit on success, rollback on exception\n            for trace_id in trace_ids:\n                trace = await session.get(Trace, trace_id)\n                if trace:\n                    trace.embedding = await generate_embedding(trace)\n\nasync def run_worker():\n    while True:\n        pending = await fetch_pending_traces()\n        if pending:\n            await process_batch(pending)\n        else:\n            await asyncio.sleep(5)\n\n# Graceful shutdown:\nasync def shutdown():\n    await engine.dispose()  # Close all connections in pool\n```\n\nKey points:\n- Use `async with session.begin()` for automatic commit/rollback\n- `pool_pre_ping=True` re-establishes dropped connections automatically\n- One session per batch/transaction \u2014 never share across concurrent tasks\n- `engine.dispose()` cleanly closes all connections on shutdown\n- `max_overflow` controls burst capacity beyond `pool_size`",
    "tags": [
      "python",
      "sqlalchemy",
      "async",
      "background-tasks"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI custom exception handlers for consistent error responses",
    "context": "My FastAPI application raises various exceptions in different layers (validation errors, database errors, business logic errors) and I want all errors to return the same JSON structure. I also want to map internal exceptions to appropriate HTTP status codes.",
    "solution": "Define custom exceptions and register handlers:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi.exceptions import RequestValidationError\nfrom pydantic import ValidationError\n\n# Custom exception hierarchy\nclass AppError(Exception):\n    status_code: int = 500\n    code: str = 'internal_error'\n    def __init__(self, message: str):\n        self.message = message\n\nclass NotFoundError(AppError):\n    status_code = 404\n    code = 'not_found'\n\nclass ConflictError(AppError):\n    status_code = 409\n    code = 'conflict'\n\nclass ForbiddenError(AppError):\n    status_code = 403\n    code = 'forbidden'\n\n# Register handlers:\ndef add_exception_handlers(app: FastAPI) -> None:\n    @app.exception_handler(AppError)\n    async def app_error_handler(request: Request, exc: AppError):\n        return JSONResponse(\n            status_code=exc.status_code,\n            content={'error': exc.code, 'message': exc.message},\n        )\n    \n    @app.exception_handler(RequestValidationError)\n    async def validation_error_handler(request: Request, exc: RequestValidationError):\n        return JSONResponse(\n            status_code=422,\n            content={'error': 'validation_error', 'detail': exc.errors()},\n        )\n\n# Usage in service layer:\nasync def get_trace_or_404(session, trace_id):\n    trace = await session.get(Trace, trace_id)\n    if not trace:\n        raise NotFoundError(f'Trace {trace_id} not found')\n    return trace\n```\n\nKey points:\n- Custom exception hierarchy lets you `except AppError` to catch any app error\n- Register handlers with `@app.exception_handler` or `app.add_exception_handler()`\n- FastAPI's built-in `RequestValidationError` handler returns 422 \u2014 override for custom format\n- Never expose internal error details in production (stack traces, DB errors)",
    "tags": [
      "python",
      "fastapi",
      "error-handling"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python httpx async client for external API calls",
    "context": "I need to make HTTP calls to external APIs from my FastAPI application. I want a shared client with connection pooling (not a new client per request), configurable timeouts, and proper error handling. I'm using httpx for async support.",
    "solution": "Create a shared httpx client in the app lifespan:\n\n```python\nimport httpx\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Shared client with connection pooling\n    app.state.http_client = httpx.AsyncClient(\n        timeout=httpx.Timeout(connect=5.0, read=30.0, write=10.0, pool=5.0),\n        limits=httpx.Limits(max_connections=20, max_keepalive_connections=10),\n        headers={'User-Agent': 'MyApp/1.0'},\n    )\n    yield\n    await app.state.http_client.aclose()\n\n# Dependency:\ndef get_http_client(request: Request) -> httpx.AsyncClient:\n    return request.app.state.http_client\n\nHTTPClient = Annotated[httpx.AsyncClient, Depends(get_http_client)]\n\n# Usage with error handling:\nasync def call_external_api(client: httpx.AsyncClient, url: str) -> dict:\n    try:\n        response = await client.get(url)\n        response.raise_for_status()  # Raises httpx.HTTPStatusError on 4xx/5xx\n        return response.json()\n    except httpx.TimeoutException:\n        raise ExternalServiceTimeout(f'Timeout calling {url}')\n    except httpx.HTTPStatusError as e:\n        raise ExternalServiceError(f'HTTP {e.response.status_code} from {url}')\n    except httpx.RequestError as e:\n        raise ExternalServiceError(f'Request failed: {e}')\n```\n\nKey points:\n- One shared client per app \u2014 connection pooling is the main benefit\n- `raise_for_status()` converts 4xx/5xx to exceptions\n- Set separate timeouts for connect, read, write phases\n- `AsyncClient(base_url='https://api.example.com')` for consistent base URL",
    "tags": [
      "python",
      "httpx",
      "async",
      "api"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python async context manager for resource cleanup",
    "context": "I need to create an async context manager for managing resources that need cleanup (database connections, temp files, locks). I want to use it with both `async with` syntax and as a decorator.",
    "solution": "Use `@asynccontextmanager` or implement `__aenter__`/`__aexit__`:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncIterator\n\n# Option 1: Generator-based (simpler for most cases)\n@asynccontextmanager\nasync def managed_transaction(session: AsyncSession) -> AsyncIterator[AsyncSession]:\n    async with session.begin():\n        try:\n            yield session\n        except Exception:\n            await session.rollback()\n            raise\n\n# Usage:\nasync with managed_transaction(session) as txn:\n    await txn.execute(insert_stmt)\n\n# Option 2: Class-based (reusable, configurable)\nclass DistributedLock:\n    def __init__(self, redis, name: str, ttl: int = 30):\n        self.redis = redis\n        self.name = f'lock:{name}'\n        self.ttl = ttl\n    \n    async def __aenter__(self) -> 'DistributedLock':\n        acquired = await self.redis.set(\n            self.name, '1', nx=True, ex=self.ttl\n        )\n        if not acquired:\n            raise LockError(f'Could not acquire lock: {self.name}')\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        await self.redis.delete(self.name)\n        return False  # Don't suppress exceptions\n\n# Usage:\nasync with DistributedLock(redis, 'embedding-worker'):\n    await process_batch()\n```\n\nKey points:\n- `@asynccontextmanager` is simpler for one-off contexts\n- Class-based is better when you need configuration or reuse\n- Always return `False` (or `None`) from `__aexit__` unless you want to suppress exceptions\n- `async with` composes cleanly \u2014 use nested contexts for multiple resources",
    "tags": [
      "python",
      "async",
      "context-manager"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI response model with computed fields and excludes",
    "context": "I have a SQLAlchemy ORM model with sensitive fields (api_key_hash, internal_flags) that I don't want to expose in API responses. I also want to add computed fields to the response (e.g., is_verified combining multiple conditions). I'm using Pydantic v2.",
    "solution": "Use Pydantic response models with `from_orm` and `@computed_field`:\n\n```python\nfrom pydantic import BaseModel, ConfigDict, computed_field\nfrom datetime import datetime\nimport uuid\n\nclass TraceResponse(BaseModel):\n    model_config = ConfigDict(from_attributes=True)  # Enable ORM mode\n    \n    id: uuid.UUID\n    title: str\n    context_text: str\n    solution_text: str\n    status: str\n    trust_score: float\n    created_at: datetime\n    # NOTE: api_key_hash, is_flagged, internal_fields NOT included\n    \n    @computed_field\n    @property\n    def is_validated(self) -> bool:\n        return self.status == 'validated'\n\n# For nested models:\nclass TagResponse(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n    id: uuid.UUID\n    name: str\n\nclass TraceWithTagsResponse(TraceResponse):\n    tags: list[TagResponse] = []\n\n# In route handler:\n@router.get('/traces/{trace_id}', response_model=TraceWithTagsResponse)\nasync def get_trace(trace_id: uuid.UUID, db: DbSession):\n    trace = await get_trace_with_tags(db, trace_id)\n    return trace  # FastAPI serializes via response_model automatically\n```\n\nKey points:\n- `from_attributes=True` (Pydantic v2) replaces `orm_mode=True` (Pydantic v1)\n- Only fields declared in the response model are included \u2014 sensitive fields are excluded by default\n- `@computed_field` computes dynamic fields at serialization time\n- Use `response_model_exclude_unset=True` in the route to exclude None fields",
    "tags": [
      "python",
      "pydantic",
      "fastapi",
      "api-design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy 2.0 composite unique constraints and indexes",
    "context": "I need to enforce a composite unique constraint across multiple columns in SQLAlchemy (e.g., one vote per user per trace, unique domain reputation per user per tag). I also need a partial unique index for conditional uniqueness.",
    "solution": "Define constraints at the table level using `__table_args__`:\n\n```python\nfrom sqlalchemy import UniqueConstraint, Index, CheckConstraint\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass Vote(Base):\n    __tablename__ = 'votes'\n    __table_args__ = (\n        # One vote per user per trace:\n        UniqueConstraint('voter_id', 'trace_id', name='uq_votes_voter_trace'),\n        # Index for fast lookup:\n        Index('ix_votes_trace_id', 'trace_id'),\n        # Partial index \u2014 only for confirmed votes:\n        Index(\n            'ix_votes_confirmed',\n            'trace_id',\n            postgresql_where='vote_type = \\'confirmed\\'',\n        ),\n    )\n    \n    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)\n    voter_id: Mapped[uuid.UUID] = mapped_column(ForeignKey('users.id'))\n    trace_id: Mapped[uuid.UUID] = mapped_column(ForeignKey('traces.id'))\n    vote_type: Mapped[str] = mapped_column(String(20))\n\n# Handling IntegrityError for duplicate votes:\nfrom sqlalchemy.exc import IntegrityError\n\ntry:\n    session.add(vote)\n    await session.commit()\nexcept IntegrityError as e:\n    await session.rollback()\n    if 'uq_votes_voter_trace' in str(e.orig):\n        raise HTTPException(409, 'You have already voted on this trace')\n    raise\n```\n\nKey points:\n- `__table_args__` must be a tuple \u2014 add a trailing comma if only one item\n- Name constraints explicitly (`name=...`) for meaningful error messages\n- `postgresql_where=` creates partial indexes (PostgreSQL-specific)\n- Catch `IntegrityError` by constraint name to distinguish different violations",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "asyncio semaphore for concurrency limiting",
    "context": "I have a background worker that processes items concurrently, but I don't want to overwhelm the database or external API. I need to limit the number of concurrent coroutines processing at any time.",
    "solution": "Use `asyncio.Semaphore` to cap concurrent operations:\n\n```python\nimport asyncio\nfrom typing import Coroutine, TypeVar\n\nT = TypeVar('T')\n\nasync def process_with_limit(\n    items: list,\n    processor: callable,\n    max_concurrent: int = 5,\n) -> list:\n    \"\"\"Process items concurrently with a cap on parallelism.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def bounded_process(item):\n        async with semaphore:\n            return await processor(item)\n    \n    return await asyncio.gather(*[bounded_process(item) for item in items])\n\n# Usage \u2014 process 100 traces, max 5 at a time:\nresults = await process_with_limit(\n    pending_trace_ids,\n    generate_embedding,\n    max_concurrent=5,\n)\n\n# With error handling:\nasync def process_one(trace_id: str) -> tuple[str, bool]:\n    async with semaphore:\n        try:\n            await generate_embedding(trace_id)\n            return trace_id, True\n        except Exception as e:\n            log.error('Failed', trace_id=trace_id, error=str(e))\n            return trace_id, False\n```\n\nKey points:\n- `asyncio.Semaphore(n)` allows at most n coroutines in the critical section\n- Create the semaphore outside the coroutine so it's shared across all tasks\n- `asyncio.gather()` starts all tasks immediately \u2014 semaphore controls entry\n- For producer/consumer patterns, prefer `asyncio.Queue` over semaphore",
    "tags": [
      "python",
      "async",
      "asyncio",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Alembic migration for adding a column with a default value",
    "context": "I need to add a new column to an existing PostgreSQL table that already has data. The column has a default value. I want to avoid locking the table and need to understand the migration ordering to handle NOT NULL constraints safely.",
    "solution": "Add column with default in stages to avoid locking:\n\n```python\n# migrations/versions/0005_add_is_seed_column.py\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade() -> None:\n    # Step 1: Add column as nullable first (no lock, no backfill needed)\n    op.add_column('traces', sa.Column('is_seed', sa.Boolean(), nullable=True))\n    \n    # Step 2: Backfill existing rows\n    op.execute(\"UPDATE traces SET is_seed = FALSE WHERE is_seed IS NULL\")\n    \n    # Step 3: Set NOT NULL constraint (fast if no NULLs exist)\n    op.alter_column('traces', 'is_seed', nullable=False)\n    \n    # Step 4: Set server default for future inserts\n    op.alter_column('traces', 'is_seed', server_default=sa.false())\n\ndef downgrade() -> None:\n    op.drop_column('traces', 'is_seed')\n```\n\nFor simple cases where table is small or can tolerate a brief lock:\n```python\ndef upgrade() -> None:\n    op.add_column(\n        'traces',\n        sa.Column('is_seed', sa.Boolean(), nullable=False, server_default=sa.false())\n    )\n    # Remove server_default after migration (to keep model and DB in sync)\n    op.alter_column('traces', 'is_seed', server_default=None)\n```\n\nKey points:\n- Adding NOT NULL column with default backfills all rows and locks table in PG < 11\n- PostgreSQL 11+ supports `ADD COLUMN ... DEFAULT` without a full table rewrite\n- Use `CONCURRENTLY` for indexes; for columns, stage as nullable -> backfill -> NOT NULL\n- Always test migrations on a copy of production data before running in prod",
    "tags": [
      "python",
      "alembic",
      "postgresql",
      "migrations"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI WebSocket endpoint for real-time updates",
    "context": "I need to push real-time updates to browser clients when traces are validated or new search results arrive. I want to use WebSockets with FastAPI, handle connection cleanup when clients disconnect, and broadcast to multiple connected clients.",
    "solution": "Implement a WebSocket manager for broadcast:\n\n```python\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom typing import Any\nimport json\n\nclass ConnectionManager:\n    def __init__(self):\n        self.connections: list[WebSocket] = []\n    \n    async def connect(self, ws: WebSocket) -> None:\n        await ws.accept()\n        self.connections.append(ws)\n    \n    def disconnect(self, ws: WebSocket) -> None:\n        self.connections.remove(ws)\n    \n    async def broadcast(self, message: Any) -> None:\n        data = json.dumps(message)\n        disconnected = []\n        for ws in self.connections:\n            try:\n                await ws.send_text(data)\n            except Exception:\n                disconnected.append(ws)\n        for ws in disconnected:\n            self.connections.remove(ws)\n\nmanager = ConnectionManager()\n\n@app.websocket('/ws/traces')\nasync def traces_websocket(ws: WebSocket):\n    await manager.connect(ws)\n    try:\n        while True:\n            # Keep connection alive \u2014 wait for client messages or ping\n            await ws.receive_text()  # or receive_json()\n    except WebSocketDisconnect:\n        manager.disconnect(ws)\n\n# Broadcast from any route:\n@router.post('/traces')\nasync def create_trace(body: TraceCreate):\n    trace = await save_trace(body)\n    await manager.broadcast({'type': 'trace_created', 'id': str(trace.id)})\n    return trace\n```\n\nKey points:\n- Always handle `WebSocketDisconnect` \u2014 clients can disconnect at any time\n- Track disconnected sockets during broadcast \u2014 don't modify list while iterating\n- For production scale, use Redis pub/sub as the broadcast backend\n- Consider authentication: check API key in query param or first message",
    "tags": [
      "python",
      "fastapi",
      "websocket",
      "real-time"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python dataclass vs Pydantic model \u2014 when to use each",
    "context": "I'm building a Python service and not sure whether to use Python dataclasses, Pydantic models, or attrs for different data structures. I need to understand the tradeoffs for API schemas, internal data transfer objects, and configuration.",
    "solution": "Use each type for its strengths:\n\n```python\n# Pydantic BaseModel \u2014 for API requests/responses and config\n# Pros: validation, serialization, OpenAPI schema generation\n# Cons: slower to create, heavier than dataclasses\nfrom pydantic import BaseModel\n\nclass TraceCreate(BaseModel):  # API request body\n    title: str\n    context_text: str\n    tags: list[str] = []\n\n# Python dataclass \u2014 for internal data transfer objects\n# Pros: fast, simple, stdlib (no deps), good for pure data carriers\n# Cons: no validation, no serialization helpers\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass EmbeddingResult:  # Internal DTO between worker layers\n    trace_id: str\n    embedding: list[float]\n    model: str\n    tokens_used: int = 0\n\n# TypedDict \u2014 for dict-compatible type hints (JSON-like structures)\nfrom typing import TypedDict\n\nclass SearchFilters(TypedDict, total=False):  # Optional keys\n    status: str\n    tag: str\n    min_trust: float\n\n# Named tuples \u2014 for simple immutable records\nfrom typing import NamedTuple\n\nclass PaginationMeta(NamedTuple):\n    page: int\n    page_size: int\n    total: int\n```\n\nDecision guide:\n- API boundary (in/out): **Pydantic** \u2014 validation + schema generation\n- Configuration: **Pydantic Settings** \u2014 env var support\n- Internal DTOs: **dataclass** \u2014 fast, simple, no overhead\n- Dict-like JSON structures: **TypedDict** \u2014 type hints without instantiation\n- Small immutable tuples: **NamedTuple** \u2014 readable, hashable",
    "tags": [
      "python",
      "pydantic",
      "dataclasses",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest mock for external HTTP API calls",
    "context": "My application makes calls to the OpenAI API and other external services. I want to write unit tests that don't make real HTTP calls, can test different response scenarios (success, error, timeout), and don't require API keys in CI.",
    "solution": "Use `pytest-httpx` or `respx` to mock httpx calls:\n\n```python\n# pip install respx pytest-httpx\nimport pytest\nfrom unittest.mock import AsyncMock, patch\n\n# Option 1: respx for httpx mocking\nimport respx\nimport httpx\n\n@pytest.mark.asyncio\nasync def test_embedding_success():\n    mock_response = {'data': [{'embedding': [0.1] * 1536}], 'usage': {'total_tokens': 10}}\n    \n    with respx.mock:\n        respx.post('https://api.openai.com/v1/embeddings').mock(\n            return_value=httpx.Response(200, json=mock_response)\n        )\n        result = await generate_embedding('test text')\n        assert len(result) == 1536\n\n@pytest.mark.asyncio\nasync def test_embedding_timeout():\n    with respx.mock:\n        respx.post('https://api.openai.com/v1/embeddings').mock(\n            side_effect=httpx.TimeoutException('Timeout')\n        )\n        result = await generate_embedding('test text')\n        assert result is None  # Should fail gracefully\n\n# Option 2: patch for simple cases\n@pytest.mark.asyncio\nasync def test_with_patch():\n    with patch('app.services.embeddings.openai_client.embeddings.create') as mock_create:\n        mock_create.return_value = AsyncMock(\n            data=[AsyncMock(embedding=[0.1] * 1536)]\n        )\n        result = await generate_embedding('test')\n        assert result is not None\n```\n\nKey points:\n- `respx` integrates with httpx at the transport level \u2014 no real HTTP requests\n- Test both success and failure paths \u2014 timeouts, 429s, 500s\n- `AsyncMock` is required for async functions in `unittest.mock`\n- Use `pytest.fixture` to share mock setups across multiple tests",
    "tags": [
      "python",
      "pytest",
      "testing",
      "mocking"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python type hints for complex generic types",
    "context": "I'm writing Python code with strict type checking (mypy) and need to properly annotate functions that work with generic types: functions returning different types based on input, TypeVar with bounds, Protocol for structural typing, and ParamSpec for decorator type safety.",
    "solution": "Use TypeVar, Protocol, and ParamSpec for advanced typing:\n\n```python\nfrom typing import TypeVar, Protocol, ParamSpec, Callable, overload, runtime_checkable\nfrom collections.abc import Awaitable\n\n# TypeVar with bound \u2014 T must be a subtype of BaseModel\nT = TypeVar('T', bound='BaseModel')\n\nasync def parse_response(response: httpx.Response, model: type[T]) -> T:\n    \"\"\"Parse an HTTP response into a Pydantic model.\"\"\"\n    return model.model_validate(response.json())\n\n# Protocol for structural typing (duck typing with type safety)\n@runtime_checkable\nclass Identifiable(Protocol):\n    @property\n    def id(self) -> uuid.UUID: ...\n    \ndef get_id(obj: Identifiable) -> str:\n    return str(obj.id)  # Works for any class with .id\n\n# ParamSpec for type-safe decorators:\nP = ParamSpec('P')\nR = TypeVar('R')\n\ndef retry(max_attempts: int = 3) -> Callable[[Callable[P, Awaitable[R]]], Callable[P, Awaitable[R]]]:\n    def decorator(func: Callable[P, Awaitable[R]]) -> Callable[P, Awaitable[R]]:\n        async def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n            for attempt in range(max_attempts):\n                try:\n                    return await func(*args, **kwargs)\n                except Exception:\n                    if attempt == max_attempts - 1:\n                        raise\n                    await asyncio.sleep(2 ** attempt)\n        return wrapper\n    return decorator\n\n@retry(max_attempts=3)\nasync def call_api(url: str, timeout: float = 10.0) -> dict:\n    ...\n```\n\nKey points:\n- `TypeVar(bound=T)` constrains the type variable to subtypes of T\n- `Protocol` enables structural typing without inheritance\n- `ParamSpec` preserves the wrapped function's signature in decorators\n- Use `overload` for functions with different return types based on input type",
    "tags": [
      "python",
      "typing",
      "mypy"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI pagination with cursor-based (keyset) pagination",
    "context": "My API has endpoints that return large datasets (100k+ rows). Offset-based pagination becomes slow at high page numbers. I need to implement cursor-based pagination that's efficient at any position in the dataset.",
    "solution": "Use keyset pagination with an opaque cursor:\n\n```python\nimport base64\nimport json\nfrom datetime import datetime\nfrom pydantic import BaseModel\n\ndef encode_cursor(created_at: datetime, id: str) -> str:\n    data = {'ts': created_at.isoformat(), 'id': id}\n    return base64.urlsafe_b64encode(json.dumps(data).encode()).decode()\n\ndef decode_cursor(cursor: str) -> tuple[datetime, str]:\n    data = json.loads(base64.urlsafe_b64decode(cursor))\n    return datetime.fromisoformat(data['ts']), data['id']\n\nclass PagedResponse(BaseModel):\n    items: list\n    next_cursor: str | None\n    has_more: bool\n\nasync def list_traces_keyset(\n    session: AsyncSession,\n    limit: int = 20,\n    cursor: str | None = None,\n) -> PagedResponse:\n    query = select(Trace).order_by(Trace.created_at.desc(), Trace.id.desc())\n    \n    if cursor:\n        ts, last_id = decode_cursor(cursor)\n        # Keyset condition: next page starts after the cursor\n        query = query.where(\n            (Trace.created_at < ts) | \n            ((Trace.created_at == ts) & (Trace.id < last_id))\n        )\n    \n    # Fetch one extra to detect if there's a next page\n    result = await session.execute(query.limit(limit + 1))\n    traces = result.scalars().all()\n    \n    has_more = len(traces) > limit\n    items = traces[:limit]\n    next_cursor = encode_cursor(items[-1].created_at, str(items[-1].id)) if has_more else None\n    \n    return PagedResponse(items=items, next_cursor=next_cursor, has_more=has_more)\n```\n\nKey points:\n- Keyset pagination is O(log n) regardless of position \u2014 offsets are O(n)\n- Always sort by (timestamp, id) \u2014 pure timestamp sort is non-deterministic for same-second rows\n- Encode cursor as opaque base64 \u2014 clients shouldn't need to parse it\n- Fetch `limit + 1` rows to detect `has_more` without a COUNT query",
    "tags": [
      "python",
      "fastapi",
      "postgresql",
      "pagination"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy upsert with ON CONFLICT DO UPDATE",
    "context": "I need to insert records but update them if they already exist (upsert). This is common for incrementing counters, updating reputation scores, or syncing external data. I need the PostgreSQL-specific ON CONFLICT DO UPDATE pattern.",
    "solution": "Use PostgreSQL dialect's insert with on_conflict_do_update:\n\n```python\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\nfrom sqlalchemy import select\n\nasync def upsert_domain_reputation(\n    session: AsyncSession,\n    user_id: uuid.UUID,\n    domain: str,\n    delta: float,\n) -> None:\n    stmt = pg_insert(ContributorDomainReputation).values(\n        contributor_id=user_id,\n        domain_tag=domain,\n        reputation_score=delta,\n        vote_count=1,\n    )\n    stmt = stmt.on_conflict_do_update(\n        index_elements=['contributor_id', 'domain_tag'],\n        set_={\n            'reputation_score': ContributorDomainReputation.reputation_score + delta,\n            'vote_count': ContributorDomainReputation.vote_count + 1,\n            'updated_at': func.now(),\n        }\n    )\n    await session.execute(stmt)\n\n# ON CONFLICT DO NOTHING (idempotent insert):\nstmt = pg_insert(Tag).values(name='python')\nstmt = stmt.on_conflict_do_nothing(index_elements=['name'])\nawait session.execute(stmt)\n\n# Get the value after upsert (RETURNING):\nstmt = pg_insert(Tag).values(name='python').on_conflict_do_update(\n    index_elements=['name'],\n    set_={'name': 'python'},  # no-op update to trigger RETURNING\n).returning(Tag.id)\nresult = await session.execute(stmt)\ntag_id = result.scalar_one()\n```\n\nKey points:\n- `index_elements` must match a unique constraint or index\n- `set_` uses the model class for column references (not string column names)\n- `excluded` pseudo-table contains the values that would have been inserted\n- Atomically increment counters: `col = col + excluded.col`",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI OpenAPI custom documentation and schema",
    "context": "I want to customize FastAPI's automatically generated OpenAPI documentation: add authentication scheme to Swagger UI, add custom descriptions and examples to request/response models, group endpoints by tags, and configure the docs to use a custom URL.",
    "solution": "Customize OpenAPI schema via FastAPI parameters and Pydantic Field:\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi.openapi.utils import get_openapi\nfrom pydantic import BaseModel, Field\n\napp = FastAPI(\n    title='CommonTrace API',\n    description='Trace sharing and search for AI agents.',\n    version='1.0.0',\n    docs_url='/docs',\n    redoc_url='/redoc',\n    openapi_url='/openapi.json',\n)\n\n# Add API key auth to Swagger UI:\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    schema = get_openapi(\n        title=app.title,\n        version=app.version,\n        routes=app.routes,\n    )\n    schema['components']['securitySchemes'] = {\n        'ApiKeyAuth': {'type': 'apiKey', 'in': 'header', 'name': 'X-API-Key'}\n    }\n    schema['security'] = [{'ApiKeyAuth': []}]\n    app.openapi_schema = schema\n    return schema\n\napp.openapi = custom_openapi\n\n# Rich schema in models:\nclass TraceCreate(BaseModel):\n    title: str = Field(\n        min_length=1,\n        max_length=500,\n        description='Descriptive, specific title for the trace',\n        examples=['FastAPI JWT authentication with refresh tokens'],\n    )\n    tags: list[str] = Field(\n        default=[],\n        description='Normalized tags (lowercase, hyphens)',\n        examples=[['python', 'fastapi', 'auth']],\n    )\n\n# Tag grouping:\nrouter = APIRouter(tags=['traces'])\n```\n\nKey points:\n- `examples` on `Field` (Pydantic v2) shows example values in Swagger UI\n- Override `app.openapi` to inject security schemes or modify the schema\n- Use router `tags` to group endpoints in Swagger UI\n- Set `include_in_schema=False` on internal endpoints (health checks, metrics)",
    "tags": [
      "python",
      "fastapi",
      "openapi",
      "api-design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python async generator for streaming large datasets",
    "context": "I need to stream large query results from PostgreSQL without loading everything into memory. I want to use a Python async generator that yields rows in batches, and I want to support both streaming HTTP responses and background processing.",
    "solution": "Use SQLAlchemy's `stream_results` with async generators:\n\n```python\nfrom sqlalchemy import select\nfrom collections.abc import AsyncIterator\n\nasync def stream_traces(\n    session: AsyncSession,\n    batch_size: int = 100,\n) -> AsyncIterator[list[Trace]]:\n    \"\"\"Yield traces in batches without loading all into memory.\"\"\"\n    # stream_results uses server-side cursor (postgresql)\n    async with session.stream(\n        select(Trace).where(Trace.status == 'validated')\n    ) as result:\n        async for batch in result.partitions(batch_size):\n            yield [row[0] for row in batch]\n\n# Usage in background worker:\nasync def reindex_all():\n    async with async_session() as session:\n        async for batch in stream_traces(session):\n            await process_batch(batch)\n\n# Streaming HTTP response with FastAPI:\nfrom fastapi.responses import StreamingResponse\nimport json\n\n@router.get('/traces/export')\nasync def export_traces(db: DbSession):\n    async def generate():\n        async for batch in stream_traces(db):\n            for trace in batch:\n                yield json.dumps({'id': str(trace.id), 'title': trace.title}) + '\\n'\n    \n    return StreamingResponse(generate(), media_type='application/x-ndjson')\n```\n\nKey points:\n- `session.stream()` uses PostgreSQL server-side cursor \u2014 constant memory usage\n- `partitions(n)` yields in chunks \u2014 don't set this too small (overhead per round trip)\n- `StreamingResponse` with async generator streams HTTP response incrementally\n- Use NDJSON (newline-delimited JSON) for streaming \u2014 easier to parse than one big array",
    "tags": [
      "python",
      "sqlalchemy",
      "async",
      "streaming"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pytest conftest.py organization for large test suites",
    "context": "My pytest test suite is growing and I'm having trouble managing shared fixtures. I need to organize conftest.py files across a test hierarchy, understand fixture scoping, and share fixtures between different test modules without circular imports.",
    "solution": "Use scoped conftest.py files at different directory levels:\n\n```\ntests/\n  conftest.py          # session-scoped: engine, db schema\n  fixtures/\n    users.py           # user factories\n    traces.py          # trace factories\n  unit/\n    conftest.py        # unit test specific overrides\n    test_tags.py\n  integration/\n    conftest.py        # integration-specific: real DB session\n    test_search.py\n```\n\n```python\n# tests/conftest.py \u2014 session-scoped shared fixtures\nimport pytest\nimport pytest_asyncio\n\n@pytest_asyncio.fixture(scope='session')\nasync def engine():\n    engine = create_async_engine(TEST_DB_URL)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    await engine.dispose()\n\n@pytest_asyncio.fixture(scope='session')\nasync def seed_user(engine):\n    async with async_sessionmaker(engine)() as sess:\n        user = User(email='test@example.com', is_seed=True)\n        sess.add(user)\n        await sess.commit()\n        await sess.refresh(user)\n        return user\n\n# tests/fixtures/traces.py \u2014 factory fixtures\nimport pytest_asyncio\n\n@pytest_asyncio.fixture\nasync def sample_trace(session, seed_user):\n    trace = Trace(\n        title='Test trace',\n        context_text='Test context',\n        solution_text='Test solution',\n        contributor_id=seed_user.id,\n        status='validated',\n    )\n    session.add(trace)\n    await session.flush()\n    return trace\n```\n\n```ini\n# pytest.ini\n[pytest]\nasyncio_mode = auto\ntestpaths = tests\n```\n\nKey points:\n- `scope='session'` fixtures run once for the entire test session \u2014 use for expensive setup\n- `scope='function'` (default) resets between tests \u2014 use for DB state\n- Fixtures can be in separate files and imported into conftest.py\n- `pytest_asyncio.fixture` required for async fixtures (not just `pytest.fixture`)",
    "tags": [
      "python",
      "pytest",
      "testing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI health check endpoint with dependency checks",
    "context": "I need a /health endpoint for Docker healthchecks and load balancer probes. The endpoint should verify the database connection and Redis connection are alive, return structured health status, and respond quickly (under 1 second).",
    "solution": "Implement a health check that tests real connections:\n\n```python\nfrom fastapi import APIRouter, Depends\nfrom pydantic import BaseModel\nimport asyncio\n\nrouter = APIRouter()\n\nclass HealthStatus(BaseModel):\n    status: str  # 'healthy' | 'degraded' | 'unhealthy'\n    database: str\n    redis: str\n    version: str = '1.0.0'\n\n@router.get('/health', response_model=HealthStatus, include_in_schema=False)\nasync def health_check(db: DbSession, redis=Depends(get_redis)):\n    db_status = 'unknown'\n    redis_status = 'unknown'\n    \n    # Check DB with timeout\n    try:\n        await asyncio.wait_for(\n            db.execute(text('SELECT 1')),\n            timeout=1.0\n        )\n        db_status = 'ok'\n    except Exception as e:\n        db_status = f'error: {type(e).__name__}'\n    \n    # Check Redis\n    try:\n        await asyncio.wait_for(redis.ping(), timeout=1.0)\n        redis_status = 'ok'\n    except Exception as e:\n        redis_status = f'error: {type(e).__name__}'\n    \n    all_ok = db_status == 'ok' and redis_status == 'ok'\n    return HealthStatus(\n        status='healthy' if all_ok else 'degraded',\n        database=db_status,\n        redis=redis_status,\n    )\n```\n\nDocker Compose healthcheck:\n```yaml\nhealthcheck:\n  test: [\"CMD-SHELL\", \"curl -f http://localhost:8000/health || exit 1\"]\n  interval: 10s\n  timeout: 5s\n  retries: 3\n```\n\nKey points:\n- Always timeout dependency checks \u2014 a hung DB check hangs your health endpoint\n- Return `degraded` not `unhealthy` when optional services (Redis) are down\n- Use `SELECT 1` for lightweight DB check \u2014 no actual data access\n- Exclude from OpenAPI schema with `include_in_schema=False`",
    "tags": [
      "python",
      "fastapi",
      "docker",
      "monitoring"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy event listeners for audit logging",
    "context": "I need to automatically track when rows are inserted or updated in my database tables for audit purposes. I want to log changes without modifying every query in my codebase.",
    "solution": "Use SQLAlchemy ORM event listeners:\n\n```python\nfrom sqlalchemy import event\nfrom sqlalchemy.orm import Session\nimport structlog\n\nlog = structlog.get_logger()\n\n# Per-model listener:\n@event.listens_for(Trace, 'after_insert')\ndef after_trace_insert(mapper, connection, target):\n    log.info(\n        'trace_created',\n        trace_id=str(target.id),\n        contributor_id=str(target.contributor_id),\n        is_seed=target.is_seed,\n    )\n\n@event.listens_for(Trace, 'after_update')\ndef after_trace_update(mapper, connection, target):\n    # Only log status changes\n    state = inspect(target)\n    history = state.attrs.status.history\n    if history.has_changes():\n        log.info(\n            'trace_status_changed',\n            trace_id=str(target.id),\n            old_status=history.deleted[0] if history.deleted else None,\n            new_status=target.status,\n        )\n\n# Session-level listener (all models):\n@event.listens_for(Session, 'after_bulk_delete')\ndef after_bulk_delete(delete_context):\n    log.warning('bulk_delete', table=delete_context.primary_table.name)\n```\n\nFor a mixin-based approach:\n```python\nclass AuditMixin:\n    @staticmethod\n    def _after_insert(mapper, connection, target):\n        log.info('inserted', model=type(target).__name__, id=str(target.id))\n    \n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        event.listen(cls, 'after_insert', cls._after_insert)\n\nclass Trace(AuditMixin, Base):\n    ...\n```\n\nKey points:\n- ORM events fire AFTER the SQL executes but BEFORE commit\n- Use `inspect(target).attrs.field.history` to get before/after values\n- Keep event listeners lightweight \u2014 they run synchronously in the DB transaction\n- For async sessions, use `async_object_session(target)` to get the session",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql",
      "logging"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python Enum usage patterns and SQLAlchemy integration",
    "context": "I'm using Python Enum classes for status fields and vote types in my application. I need them to work correctly with SQLAlchemy (stored as strings in PostgreSQL, not as database enum types), with Pydantic validation, and with proper JSON serialization.",
    "solution": "Use str-based Enums for compatibility with SQLAlchemy and Pydantic:\n\n```python\nimport enum\nfrom sqlalchemy import String\nfrom sqlalchemy.orm import mapped_column, Mapped\nfrom pydantic import BaseModel\n\n# str enum \u2014 value IS the string, no native DB enum type needed\nclass TraceStatus(str, enum.Enum):\n    pending = 'pending'\n    validated = 'validated'\n\nclass VoteType(str, enum.Enum):\n    confirmed = 'confirmed'\n    disputed = 'disputed'\n\n# SQLAlchemy \u2014 store as String, compare as string:\nclass Trace(Base):\n    status: Mapped[str] = mapped_column(String(20), default=TraceStatus.pending)\n\n# Works in queries:\nstmt = select(Trace).where(Trace.status == TraceStatus.validated)\n# Also works:\nstmt = select(Trace).where(Trace.status == 'validated')\n\n# Pydantic \u2014 accepts both 'validated' string and TraceStatus.validated:\nclass TraceResponse(BaseModel):\n    status: TraceStatus  # Validates and serializes as string\n\n# JSON serialization \u2014 str enum serializes as its value:\nimport json\nstatus = TraceStatus.validated\njson.dumps({'status': status})  # -> '{\"status\": \"validated\"}'\n\n# Transition validation:\ndef validate_status_transition(current: TraceStatus, new: TraceStatus) -> bool:\n    allowed = {\n        TraceStatus.pending: {TraceStatus.validated},\n    }\n    return new in allowed.get(current, set())\n```\n\nKey points:\n- `str` + `enum.Enum` means instances ARE strings \u2014 no `.value` access needed\n- Store as `String` column, not `Enum` \u2014 avoids PostgreSQL ENUM type migration complexity\n- Pydantic coerces string input to enum value automatically\n- `json.dumps` on `str` enums works without a custom encoder",
    "tags": [
      "python",
      "sqlalchemy",
      "pydantic",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI file upload handling with validation",
    "context": "I need an endpoint that accepts file uploads (CSV, JSON, images). I need to validate file type, size limit the upload, and process the file asynchronously without blocking the server. I want to handle both small in-memory files and large streaming uploads.",
    "solution": "Use FastAPI's `UploadFile` for file handling:\n\n```python\nfrom fastapi import FastAPI, UploadFile, File, HTTPException\nfrom pathlib import Path\nimport aiofiles\nimport magic  # pip install python-magic\n\nMAX_SIZE = 10 * 1024 * 1024  # 10 MB\nALLOWED_TYPES = {'application/json', 'text/csv', 'text/plain'}\n\n@router.post('/upload')\nasync def upload_file(file: UploadFile = File(...)):\n    # Check content type from header (can be spoofed)\n    if file.content_type not in ALLOWED_TYPES:\n        raise HTTPException(415, f'Unsupported type: {file.content_type}')\n    \n    # Read with size limit\n    content = b''\n    while chunk := await file.read(8192):\n        content += chunk\n        if len(content) > MAX_SIZE:\n            raise HTTPException(413, 'File too large (max 10 MB)')\n    \n    # Verify actual MIME type (not just header)\n    actual_type = magic.from_buffer(content[:1024], mime=True)\n    if actual_type not in ALLOWED_TYPES:\n        raise HTTPException(415, f'Actual content type {actual_type} not allowed')\n    \n    # Process content (e.g., parse JSON):\n    if file.content_type == 'application/json':\n        import json\n        data = json.loads(content)\n        return await process_json_upload(data)\n    \n    return {'filename': file.filename, 'size': len(content)}\n\n# For large files \u2014 stream to disk:\n@router.post('/upload/large')\nasync def upload_large(file: UploadFile):\n    path = Path('/tmp') / file.filename\n    async with aiofiles.open(path, 'wb') as f:\n        while chunk := await file.read(65536):\n            await f.write(chunk)\n    return {'path': str(path)}\n```\n\nKey points:\n- Always verify actual MIME type with `python-magic` \u2014 `content_type` header is user-controlled\n- Stream-read with chunks to avoid loading huge files into memory\n- `aiofiles` for non-blocking file I/O (don't use `open()` in async routes)\n- Set Nginx/proxy upload size limit (`client_max_body_size`) before FastAPI sees the request",
    "tags": [
      "python",
      "fastapi",
      "file-upload"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL EXPLAIN ANALYZE interpretation",
    "context": "I have a slow PostgreSQL query and I ran EXPLAIN ANALYZE. I need to understand how to read the output, identify why the query is slow (wrong index, bad join order, row estimate mismatch), and know what to fix.",
    "solution": "Systematic approach to reading EXPLAIN ANALYZE output:\n\n```sql\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT t.*, array_agg(tg.name) as tags\nFROM traces t\nJOIN trace_tags tt ON tt.trace_id = t.id\nJOIN tags tg ON tg.id = tt.tag_id\nWHERE t.status = 'validated'\nGROUP BY t.id\nORDER BY t.created_at DESC\nLIMIT 20;\n```\n\nReading the output \u2014 key nodes:\n```\nLimit  (cost=1200..1250 rows=20 width=512) (actual time=820..822 rows=20 loops=1)\n  -> Sort  (cost=1200..1350 rows=60000) (actual time=820..821 rows=20 loops=1)\n       Sort Key: t.created_at DESC\n       Sort Method: top-N heapsort  Memory: 35kB\n     -> Hash Join  (cost=500..900 rows=60000) (actual time=12..810 rows=60000 loops=1)\n          Hash Cond: tt.trace_id = t.id\n          Buffers: shared hit=2000 read=5000  <- 5000 disk reads!\n          -> Seq Scan on trace_tags  (cost=0..300) <- Missing index!\n```\n\nDiagnosis checklist:\n- `Seq Scan` on large tables \u2192 missing index\n- `cost=X rows=Y` vs `actual rows=Z` (Y >> Z) \u2192 stale statistics, run `ANALYZE`\n- `Buffers: read=N` high \u2192 cache miss, data not in shared_buffers\n- `Sort Method: external merge` \u2192 sort spilling to disk, increase `work_mem`\n- `Nested Loop` with large inner table \u2192 should be Hash Join; missing join index\n\nFixes:\n```sql\n-- Fix missing index on join column:\nCREATE INDEX CONCURRENTLY ON trace_tags(trace_id);\n\n-- Fix stale stats:\nANALYZE traces;\n\n-- Fix sort spill (session-level):\nSET work_mem = '64MB';\n```",
    "tags": [
      "postgresql",
      "performance",
      "sql"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL connection pooling with PgBouncer",
    "context": "My FastAPI application has 20 workers and each worker has a SQLAlchemy connection pool of 5. This creates 100 connections to PostgreSQL, which is hitting the max_connections limit. I need connection pooling at the database level.",
    "solution": "Run PgBouncer as a sidecar for connection multiplexing:\n\n```ini\n# pgbouncer.ini\n[databases]\nmydb = host=postgres port=5432 dbname=mydb\n\n[pgbouncer]\nlisten_addr = 0.0.0.0\nlisten_port = 6432\npool_mode = transaction  # Best for async apps (not session mode)\nmax_client_conn = 1000   # Connections FROM app to PgBouncer\ndefault_pool_size = 20   # Connections FROM PgBouncer to Postgres\nreserve_pool_size = 5\nserver_idle_timeout = 600\nauth_type = md5\nauth_file = /etc/pgbouncer/userlist.txt\n```\n\n```yaml\n# docker-compose.yml\nservices:\n  pgbouncer:\n    image: pgbouncer/pgbouncer\n    volumes:\n      - ./pgbouncer.ini:/etc/pgbouncer/pgbouncer.ini\n    environment:\n      DB_HOST: postgres\n      DB_USER: myapp\n      DB_PASSWORD: myapp\n  \n  api:\n    environment:\n      # Point to PgBouncer, not postgres directly\n      DATABASE_URL: postgresql+asyncpg://myapp:myapp@pgbouncer:6432/mydb\n```\n\nApp-level: reduce SQLAlchemy pool size (PgBouncer handles multiplexing):\n```python\nengine = create_async_engine(\n    settings.database_url,\n    pool_size=2,       # Small \u2014 PgBouncer multiplexes\n    max_overflow=3,\n)\n```\n\nKey points:\n- `transaction` mode: connection returned to pool after each transaction \u2014 best for async\n- `session` mode: connection held for entire session \u2014 incompatible with prepared statements\n- Prepared statements are NOT compatible with PgBouncer transaction mode \u2014 disable them\n- asyncpg: set `server_settings={'options': '-c statement_timeout=30000'}` not prepared statements",
    "tags": [
      "postgresql",
      "performance",
      "docker"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL partial indexes for filtered queries",
    "context": "I have a PostgreSQL table where most queries filter on a specific condition (e.g., status='validated'). Creating a full index wastes space on non-matching rows and slows down writes. I want to create a partial index that only covers the subset of rows I actually query.",
    "solution": "Create indexes with WHERE clause to cover only relevant rows:\n\n```sql\n-- Partial index: only validated traces (not pending)\n-- If 90% of queries filter WHERE status='validated', this index is 90% smaller\nCREATE INDEX CONCURRENTLY ix_traces_validated_embedding\nON traces (created_at DESC)\nWHERE status = 'validated' AND embedding IS NOT NULL;\n\n-- For NULL checks (embedding queue):\nCREATE INDEX CONCURRENTLY ix_traces_pending_embed\nON traces (created_at ASC)\nWHERE embedding IS NULL AND status = 'validated';\n\n-- Composite partial index for search:\nCREATE INDEX CONCURRENTLY ix_traces_seed_lookup\nON traces (title)\nWHERE is_seed = TRUE;\n```\n\nVerify the planner uses your partial index:\n```sql\nEXPLAIN SELECT * FROM traces\nWHERE status = 'validated' AND embedding IS NOT NULL\nORDER BY created_at DESC LIMIT 10;\n-- Should show: Index Scan using ix_traces_validated_embedding\n\n-- Force index usage for testing (bypass planner heuristics):\nSET enable_seqscan = off;\nEXPLAIN SELECT ...;\nSET enable_seqscan = on;\n```\n\nAlembic migration:\n```python\ndef upgrade():\n    op.create_index(\n        'ix_traces_pending_embed',\n        'traces',\n        ['created_at'],\n        postgresql_where=\"embedding IS NULL AND status = 'validated'\",\n    )\n```\n\nKey points:\n- Partial index size = full index size * (fraction of matching rows)\n- Query WHERE clause must match index WHERE clause for planner to use it\n- Partial indexes update faster than full indexes (fewer rows to maintain)\n- Use `CONCURRENTLY` in production \u2014 avoids `ACCESS EXCLUSIVE` table lock",
    "tags": [
      "postgresql",
      "indexing",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis pub/sub for real-time event broadcasting",
    "context": "I have multiple API instances behind a load balancer and need to broadcast events (trace validated, new search result) to all instances. Redis pub/sub will fan out messages to all subscribers. I need an async subscriber that processes events without blocking.",
    "solution": "Use Redis pub/sub with an async background subscriber:\n\n```python\nimport asyncio\nimport json\nfrom redis.asyncio import Redis\n\n# Publisher (in any service):\nasync def publish_event(redis: Redis, channel: str, event: dict) -> None:\n    await redis.publish(channel, json.dumps(event))\n\n# Example usage:\nawait publish_event(redis, 'traces', {\n    'type': 'trace_validated',\n    'trace_id': str(trace.id),\n    'timestamp': datetime.utcnow().isoformat(),\n})\n\n# Subscriber \u2014 run as background asyncio task:\nasync def subscribe_events(redis: Redis, handlers: dict) -> None:\n    pubsub = redis.pubsub()\n    await pubsub.subscribe('traces', 'votes')\n    \n    async for message in pubsub.listen():\n        if message['type'] != 'message':\n            continue  # Skip 'subscribe' confirmation messages\n        \n        try:\n            event = json.loads(message['data'])\n            handler = handlers.get(event.get('type'))\n            if handler:\n                await handler(event)\n        except Exception:\n            log.exception('Event processing failed', message=message)\n\n# Start in lifespan:\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    handlers = {\n        'trace_validated': on_trace_validated,\n        'vote_cast': on_vote_cast,\n    }\n    task = asyncio.create_task(subscribe_events(app.state.redis, handlers))\n    yield\n    task.cancel()\n    await asyncio.gather(task, return_exceptions=True)\n```\n\nKey points:\n- Pub/sub is fire-and-forget \u2014 no persistence, no delivery guarantees\n- Use Redis Streams (`XADD`/`XREAD`) if you need message persistence or replay\n- Each subscriber gets a copy \u2014 pub/sub is fan-out, not work queue\n- Always background the subscriber \u2014 listening is a blocking operation",
    "tags": [
      "redis",
      "async",
      "python",
      "real-time"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL deadlock prevention in concurrent writes",
    "context": "My application has concurrent requests that update the same rows in different orders, causing deadlocks (ERROR: deadlock detected). I need to understand why deadlocks happen and how to prevent them in SQLAlchemy async code.",
    "solution": "Prevent deadlocks by enforcing consistent lock ordering:\n\n```python\n# WRONG: Different transactions update rows in different orders -> deadlock risk\n# Transaction A: UPDATE votes WHERE id=1, then UPDATE traces WHERE id=2\n# Transaction B: UPDATE traces WHERE id=2, then UPDATE votes WHERE id=1\n\n# RIGHT: Always lock in the same canonical order\nasync def cast_vote_safe(\n    session: AsyncSession,\n    trace_id: uuid.UUID,\n    voter_id: uuid.UUID,\n    vote_type: str,\n) -> None:\n    # Always acquire locks in primary key order\n    ids_in_order = sorted([str(trace_id), str(voter_id)])\n    \n    # SELECT FOR UPDATE in consistent order:\n    for row_id in ids_in_order:\n        await session.execute(\n            select(Trace).where(Trace.id == row_id).with_for_update()\n        )\n    \n    # Now safe to update both:\n    vote = Vote(trace_id=trace_id, voter_id=voter_id, vote_type=vote_type)\n    session.add(vote)\n    await session.execute(\n        update(Trace)\n        .where(Trace.id == trace_id)\n        .values(confirmation_count=Trace.confirmation_count + 1)\n    )\n\n# Alternative: Use advisory locks for app-level locking:\nasync def with_advisory_lock(session: AsyncSession, lock_id: int):\n    await session.execute(text(f'SELECT pg_advisory_xact_lock({lock_id})'))\n    # Lock released automatically at transaction end\n\n# Retry on deadlock:\nfrom sqlalchemy.exc import DBAPIError\n\nasync def with_deadlock_retry(func, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await func()\n        except DBAPIError as e:\n            if 'deadlock detected' in str(e) and attempt < max_retries - 1:\n                await asyncio.sleep(0.1 * (2 ** attempt))\n                continue\n            raise\n```\n\nKey points:\n- Deadlocks are always caused by inconsistent lock ordering across transactions\n- `SELECT FOR UPDATE` acquires row-level locks explicitly\n- Advisory locks are simpler for business-logic serialization\n- Use exponential backoff retry \u2014 PostgreSQL automatically releases deadlocked transactions",
    "tags": [
      "postgresql",
      "async",
      "python",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker multi-stage build for slim Python images",
    "context": "My Docker image for a FastAPI app is 2GB and takes 5 minutes to build. I need a multi-stage Dockerfile producing a slim production image, using layer caching effectively to avoid reinstalling packages on every code change.",
    "solution": "Multi-stage Dockerfile with uv:\n\n```dockerfile\nFROM python:3.12-slim AS builder\nWORKDIR /build\nCOPY --from=ghcr.io/astral-sh/uv:0.5 /uv /usr/local/bin/uv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-install-project --no-dev\n\nFROM python:3.12-slim AS runtime\nWORKDIR /app\nCOPY --from=builder /build/.venv /app/.venv\nRUN addgroup --system app && adduser --system --ingroup app app\nUSER app\nCOPY --chown=app:app . .\nENV PATH=\"/app/.venv/bin:$PATH\" PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\nKey points:\n- Copy pyproject.toml + uv.lock before source code \u2014 cache only invalidates on dep changes\n- --no-install-project installs deps without the project (faster)\n- Multi-stage: builder can have gcc; runtime is slim\n- Run as non-root \u2014 reduces attack surface",
    "tags": [
      "docker",
      "python",
      "dockerfile"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose networking and service discovery",
    "context": "My Docker Compose setup has FastAPI, PostgreSQL, Redis, and a background worker. The services need to talk to each other. I am confused about when to use service names vs localhost, and how ports work inside vs outside the compose network.",
    "solution": "Docker Compose creates a default network where services communicate by service name:\n\n```yaml\nservices:\n  postgres:\n    image: pgvector/pgvector:pg17\n    # No ports: -- not exposed to host in production\n  redis:\n    image: redis:7-alpine\n  api:\n    build: ./api\n    ports:\n      - \"8000:8000\"  # Exposes to host machine\n    environment:\n      # Use SERVICE NAME, not localhost:\n      DATABASE_URL: postgresql+asyncpg://user:pass@postgres:5432/db\n      REDIS_URL: redis://redis:6379\n    depends_on:\n      postgres:\n        condition: service_healthy\n```\n\nKey points:\n- Inside Compose network: postgres:5432 not localhost:5432\n- ports: opens port to the HOST machine (laptop or internet)\n- Services without ports are only reachable within the Compose network\n- localhost inside a container refers to THAT container, not the host",
    "tags": [
      "docker",
      "docker-compose",
      "networking"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Nginx reverse proxy configuration for FastAPI with SSL",
    "context": "I am deploying FastAPI with Nginx as a reverse proxy. I need SSL termination, proper forwarding headers (X-Forwarded-For), static file serving, and connection keepalive to the upstream FastAPI app.",
    "solution": "Nginx reverse proxy config:\n\n```nginx\nupstream fastapi {\n    server 127.0.0.1:8000;\n    keepalive 32;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name myapp.com;\n    ssl_certificate /etc/letsencrypt/live/myapp.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/myapp.com/privkey.pem;\n    client_max_body_size 10m;\n    \n    location /static/ {\n        alias /app/static/;\n        expires 30d;\n    }\n    location / {\n        proxy_pass http://fastapi;\n        proxy_http_version 1.1;\n        proxy_set_header Connection \"\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_read_timeout 60s;\n    }\n}\n```\n\nIn FastAPI:\n```python\nfrom uvicorn.middleware.proxy_headers import ProxyHeadersMiddleware\napp.add_middleware(ProxyHeadersMiddleware, trusted_hosts=\"*\")\n```\n\nKey points:\n- proxy_http_version 1.1 + empty Connection enables HTTP keepalive upstream\n- X-Forwarded-Proto tells FastAPI the request came via HTTPS\n- client_max_body_size must be set in Nginx before FastAPI sees the request",
    "tags": [
      "nginx",
      "docker",
      "ssl",
      "deployment"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose environment variable management with .env files",
    "context": "I need to manage environment variables for Docker Compose across multiple environments without committing secrets. I want .env file support with per-environment overrides.",
    "solution": "Use .env files with compose variable substitution:\n\n```bash\n# .env (gitignored)\nPOSTGRES_PASSWORD=devpassword\nDATABASE_URL=postgresql+asyncpg://myapp:devpassword@postgres:5432/myapp\nDEBUG=true\n```\n\n```yaml\n# docker-compose.yml\nservices:\n  api:\n    environment:\n      DATABASE_URL: ${DATABASE_URL}\n      DEBUG: ${DEBUG:-false}  # Default if not set\n      SECRET_KEY: ${SECRET_KEY:?SECRET_KEY must be set}  # Fail if missing\n    env_file:\n      - .env  # Load all vars from file\n```\n\n```bash\n# Override for staging:\ndocker compose --env-file .env.staging up\n# Extend compose config for prod:\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up\n```\n\nKey points:\n- .env is automatically loaded by Compose -- no --env-file needed for default\n- ${VAR:-default} sets fallback; ${VAR:?error} fails on missing\n- env_file passes ALL vars from file; environment overrides specific ones\n- Commit .env.example (no secrets) as documentation",
    "tags": [
      "docker",
      "docker-compose",
      "configuration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React useState hook mental model and common mistakes",
    "context": "I keep running into React hooks issues: state not updating immediately after setState, stale values in event handlers, and batching behavior I do not understand.",
    "solution": "React useState state update semantics:\n\n```typescript\nimport { useState, useEffect } from 'react';\n\n// WRONG: Reading state immediately after setState\nconst [count, setCount] = useState(0);\nconst increment = () => {\n  setCount(count + 1);\n  console.log(count); // Still old value -- updates on next render\n};\n\n// RIGHT: Functional updates for state depending on previous\nconst safeIncrement = () => {\n  setCount(prev => prev + 1);\n};\n\n// Stale closure: always include state in effect deps\nuseEffect(() => {\n  const handler = () => console.log(items);\n  window.addEventListener('click', handler);\n  return () => window.removeEventListener('click', handler);\n}, [items]); // Re-runs when items changes\n\n// Complex state: always spread to preserve other fields\nconst [form, setForm] = useState({ name: '', email: '' });\nconst updateName = (name: string) => setForm(prev => ({ ...prev, name }));\n```\n\nKey points:\n- setState is async -- state is available on NEXT render, not immediately\n- Use functional form setState(prev => ...) when new state depends on previous\n- Missing dependencies in useEffect cause stale closure bugs\n- Multiple setState calls in same event are batched in React 18+",
    "tags": [
      "react",
      "typescript",
      "hooks"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React useEffect cleanup and dependency array best practices",
    "context": "My React components have useEffect hooks causing memory leaks, firing too often from missing dependencies, or not re-running when I change a filter object. I need to understand cleanup and dependency management.",
    "solution": "useEffect dependency and cleanup patterns:\n\n```typescript\nimport { useState, useEffect, useMemo, useRef } from 'react';\n\n// AbortController for fetch cancellation:\nuseEffect(() => {\n  const controller = new AbortController();\n  fetch(`/api/traces/${id}`, { signal: controller.signal })\n    .then(r => r.json())\n    .then(setTrace)\n    .catch(e => { if (e.name !== 'AbortError') setError(e.message); });\n  return () => controller.abort();\n}, [id]);\n\n// Stable object ref with useMemo to prevent infinite loop:\nconst filters = useMemo(() => ({ limit: 20, status: 'validated' }), []);\nuseEffect(() => fetchData(filters), [filters]);\n\n// Skip first run using ref:\nconst isFirst = useRef(true);\nuseEffect(() => {\n  if (isFirst.current) { isFirst.current = false; return; }\n  onFiltersChange(filters);\n}, [filters]);\n```\n\nKey points:\n- Always return cleanup function for subscriptions, timers, fetch requests\n- ESLint exhaustive-deps rule catches missing dependencies -- enable it\n- Objects in deps cause infinite re-renders -- stabilize with useMemo/useCallback\n- Empty [] runs once after initial render (componentDidMount equivalent)",
    "tags": [
      "react",
      "typescript",
      "hooks"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript discriminated unions for exhaustive state modeling",
    "context": "I have API call state with loading, success, and error variants. Using optional fields allows impossible states. I want discriminated unions with full TypeScript narrowing.",
    "solution": "Discriminated union state with useReducer:\n\n```typescript\ntype AsyncState<T> =\n  | { status: 'idle' }\n  | { status: 'loading' }\n  | { status: 'success'; data: T }\n  | { status: 'error'; error: string };\n\n// TypeScript narrows the type in each case branch:\nfunction render(state: AsyncState<Trace[]>) {\n  switch (state.status) {\n    case 'idle':    return <p>Enter a search query</p>;\n    case 'loading': return <Spinner />;\n    case 'success': return <TraceList traces={state.data} />; // data: Trace[]\n    case 'error':   return <ErrorMsg msg={state.error} />;   // error: string\n  }\n}\n\n// With useReducer:\ntype Action =\n  | { type: 'FETCH_START' }\n  | { type: 'FETCH_SUCCESS'; data: Trace[] }\n  | { type: 'FETCH_ERROR'; error: string };\n\nfunction reducer(state: AsyncState<Trace[]>, action: Action): AsyncState<Trace[]> {\n  switch (action.type) {\n    case 'FETCH_START':   return { status: 'loading' };\n    case 'FETCH_SUCCESS': return { status: 'success', data: action.data };\n    case 'FETCH_ERROR':   return { status: 'error', error: action.error };\n    default: return state;\n  }\n}\n\nconst [state, dispatch] = useReducer(reducer, { status: 'idle' });\n```\n\nKey points:\n- Discriminated unions make impossible states unrepresentable\n- switch on state.status enables TypeScript narrowing in each case\n- Pair with useReducer for complex state machines\n- Add const _: never = state.status at end of switch for exhaustive check",
    "tags": [
      "typescript",
      "react",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript utility types for deriving API types",
    "context": "I have full TypeScript types for my API responses and need derived types: making all fields optional for updates, picking a subset for list views, omitting sensitive fields, and requiring specific fields.",
    "solution": "TypeScript utility type composition:\n\n```typescript\ninterface Trace {\n  id: string;\n  title: string;\n  context_text: string;\n  solution_text: string;\n  trust_score: number;\n  is_flagged: boolean;    // internal\n  contributor_id: string; // sensitive\n  created_at: string;\n  tags: Tag[];\n}\n\n// Derived types:\ntype TraceListItem = Pick<Trace, 'id' | 'title' | 'trust_score' | 'created_at' | 'tags'>;\ntype PublicTrace = Omit<Trace, 'is_flagged' | 'contributor_id'>;\ntype TraceUpdate = Partial<Pick<Trace, 'title' | 'context_text' | 'solution_text'>>;\ntype TraceCreate = Required<Pick<Trace, 'title' | 'context_text' | 'solution_text'>> &\n  { tags?: string[]; agent_model?: string; };\n\n// Extract string literal union from const object:\nconst STATUS = { pending: 'pending', validated: 'validated' } as const;\ntype TraceStatus = typeof STATUS[keyof typeof STATUS]; // 'pending' | 'validated'\n\n// Deep partial for nested updates:\ntype DeepPartial<T> = { [K in keyof T]?: T[K] extends object ? DeepPartial<T[K]> : T[K] };\n```\n\nKey points:\n- Pick: keep specified keys. Omit: remove specified keys\n- Partial: all optional. Required: all required. Readonly: prevent mutation\n- Combine with & (intersection) to add new fields to derived types\n- typeof CONST[keyof typeof CONST] extracts union of values from const object",
    "tags": [
      "typescript",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React Context API for global state without prop drilling",
    "context": "I need to share authenticated user state across many components without prop drilling. I want React Context with a pattern that avoids unnecessary re-renders when the setter is called but the user data has not changed.",
    "solution": "Split value and setter contexts for performance:\n\n```typescript\nimport { createContext, useContext, useState, useMemo, ReactNode } from 'react';\n\ninterface AuthUser { id: string; email: string; apiKey: string; }\n\nconst AuthUserCtx = createContext<AuthUser | null>(null);\nconst AuthSetterCtx = createContext<(user: AuthUser | null) => void>(() => {});\n\nexport function AuthProvider({ children }: { children: ReactNode }) {\n  const [user, setUser] = useState<AuthUser | null>(null);\n  const setter = useMemo(() => setUser, []); // Stable reference\n  return (\n    <AuthSetterCtx.Provider value={setter}>\n      <AuthUserCtx.Provider value={user}>\n        {children}\n      </AuthUserCtx.Provider>\n    </AuthSetterCtx.Provider>\n  );\n}\n\nexport function useAuth(): AuthUser {\n  const user = useContext(AuthUserCtx);\n  if (!user) throw new Error('useAuth must be inside AuthProvider with logged-in user');\n  return user;\n}\n\nexport function useOptionalAuth() { return useContext(AuthUserCtx); }\nexport function useSetAuth() { return useContext(AuthSetterCtx); }\n```\n\nKey points:\n- Split value/setter contexts -- setter consumers won't re-render on value changes\n- Custom hooks provide type safety and error-on-misuse\n- For high-frequency updates, use Zustand or Jotai\n- Context is synchronous -- combine with useReducer for complex state",
    "tags": [
      "react",
      "typescript",
      "state-management"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Next.js App Router: server components vs client components",
    "context": "I am building with Next.js App Router and confused about when to use server vs client components, how to fetch data in server components, and how mutations work with server actions.",
    "solution": "Server vs client component patterns:\n\n```typescript\n// app/traces/page.tsx -- Server Component (default, no 'use client')\n// Runs on server: direct DB access, no client JS bundle\nasync function TracesPage({ searchParams }: { searchParams: { q?: string } }) {\n  const traces = await fetchTraces({ q: searchParams.q });\n  return (\n    <main>\n      <SearchBar />          {/* Client component */}\n      <TraceList traces={traces} />\n    </main>\n  );\n}\n\n// app/traces/SearchBar.tsx -- Client Component\n'use client';\nimport { useRouter } from 'next/navigation';\nexport function SearchBar() {\n  const router = useRouter();\n  const [q, setQ] = useState('');\n  return <input value={q} onChange={e => setQ(e.target.value)}\n    onKeyDown={e => e.key === 'Enter' && router.push(`/traces?q=${q}`)} />;\n}\n\n// Server Action:\nasync function createTrace(formData: FormData) {\n  'use server';\n  await db.save({ title: formData.get('title') as string });\n  revalidatePath('/traces');\n}\n```\n\nKey points:\n- Server components render on server only -- zero client JS, direct DB access\n- 'use client' marks the boundary -- children inherit client status\n- Server Actions handle form mutations without API routes\n- Pass server data to client components as props\n- next: { revalidate: 60 } in fetch() for ISR caching",
    "tags": [
      "typescript",
      "react",
      "nextjs"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions deployment with secrets and environments",
    "context": "I need GitHub Actions to deploy to staging and production with secrets (API keys, deploy tokens) scoped per environment, with production requiring manual approval before deploy.",
    "solution": "Environment-scoped secrets with approval gates:\n\n```yaml\nname: Deploy\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy-staging:\n    runs-on: ubuntu-latest\n    environment: staging\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to staging\n        env:\n          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}\n          DATABASE_URL: ${{ secrets.STAGING_DATABASE_URL }}\n        run: flyctl deploy --app myapp-staging\n  \n  deploy-production:\n    needs: deploy-staging\n    runs-on: ubuntu-latest\n    environment: production  # Requires manual approval in GitHub\n    steps:\n      - name: Deploy to production\n        env:\n          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}\n          DATABASE_URL: ${{ secrets.PROD_DATABASE_URL }}\n        run: flyctl deploy --app myapp-prod\n```\n\nKey points:\n- Repository secrets available to all environments; Environment secrets are scoped\n- environment: production enables required reviewers and protection rules\n- ${{ vars.NAME }} for non-secret config; ${{ secrets.NAME }} for sensitive values\n- Secrets are masked in logs -- avoid echoing them\n- needs: deploy-staging ensures sequential staging -> production flow",
    "tags": [
      "ci",
      "github-actions",
      "deployment"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "fly.io deployment with PostgreSQL and migration release command",
    "context": "I want to deploy my FastAPI application to fly.io with a managed PostgreSQL database. I need fly.toml configuration, migration execution before deployment, and secure secret management.",
    "solution": "fly.io deployment setup:\n\n```toml\n# fly.toml\napp = \"myapp-api\"\nprimary_region = \"ord\"\n\n[build]\n  dockerfile = \"Dockerfile\"\n\n[http_service]\n  internal_port = 8000\n  force_https = true\n  auto_stop_machines = true\n  min_machines_running = 0\n\n[deploy]\n  release_command = \"alembic upgrade head\"\n```\n\n```bash\n# One-time setup:\nfly launch --name myapp-api --no-deploy\nfly postgres create --name myapp-db --region ord\nfly postgres attach myapp-db  # Sets DATABASE_URL secret automatically\nfly secrets set OPENAI_API_KEY=sk-xxx REDIS_URL=redis://...\nfly deploy\n\n# Operations:\nfly logs --app myapp-api\nfly ssh console    # SSH into running machine\nfly scale count 2  # Scale to 2 instances\n```\n\nKey points:\n- release_command runs before traffic switches -- perfect for migrations\n- fly postgres attach auto-sets DATABASE_URL -- internal Fly network, no SSL overhead\n- auto_stop_machines reduces cost by stopping idle VMs\n- fly secrets set for sensitive values -- never store in fly.toml",
    "tags": [
      "deployment",
      "docker",
      "postgresql",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SSL certificate setup with Let's Encrypt and certbot",
    "context": "I need free HTTPS for my production server. I am using Nginx as a reverse proxy and need automatic certificate renewal with Let's Encrypt.",
    "solution": "Let's Encrypt with Certbot:\n\n```bash\nsudo apt install certbot python3-certbot-nginx\n\n# Obtain and auto-configure Nginx:\nsudo certbot --nginx -d myapp.com -d www.myapp.com \\\n  --email admin@myapp.com --agree-tos --no-eff-email\n\n# Test auto-renewal:\nsudo certbot renew --dry-run\n# Systemd timer handles renewal automatically:\nsystemctl status certbot.timer\n```\n\nManual Nginx SSL section:\n```nginx\nserver {\n    listen 443 ssl http2;\n    ssl_certificate /etc/letsencrypt/live/myapp.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/myapp.com/privkey.pem;\n    include /etc/letsencrypt/options-ssl-nginx.conf;\n    # HSTS -- only after HTTPS is stable:\n    add_header Strict-Transport-Security \"max-age=31536000\" always;\n}\n```\n\nFor Docker with Traefik:\n```yaml\nservices:\n  traefik:\n    image: traefik:v3\n    command:\n      - --providers.docker=true\n      - --certificatesresolvers.le.acme.email=admin@myapp.com\n      - --certificatesresolvers.le.acme.httpchallenge.entrypoint=web\n  api:\n    labels:\n      - traefik.http.routers.api.rule=Host(`myapp.com`)\n      - traefik.http.routers.api.tls.certresolver=le\n```\n\nKey points:\n- Certbot auto-renews via systemd timer -- 90-day certs, renewed at 30 days\n- fullchain.pem includes certificate + chain\n- HSTS tells browsers to always use HTTPS -- add only after HTTPS is working\n- Rate limit: 5 certs per domain per week -- use staging server for testing",
    "tags": [
      "ssl",
      "nginx",
      "deployment"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pgvector ANN search with trust re-ranking in SQLAlchemy",
    "context": "I have PostgreSQL with pgvector embeddings and a trust_score. I want semantic search that combines vector similarity with trust score for final ranking, without cutting off high-trust results before re-ranking.",
    "solution": "Over-fetch then re-rank:\n\n```python\nfrom sqlalchemy import select\n\nasync def search_traces(\n    session: AsyncSession,\n    query_embedding: list[float],\n    limit: int = 10,\n    ann_limit: int = 100,\n) -> list:\n    cosine_dist = Trace.embedding.op(\"<=>\")(\n        func.cast(query_embedding, Vector(1536))\n    )\n    \n    # ANN: over-fetch 100 candidates for re-ranking\n    ann_q = (\n        select(\n            Trace.id, Trace.title, Trace.trust_score,\n            (1 - cosine_dist).label(\"similarity_score\"),\n        )\n        .where(Trace.status == \"validated\")\n        .where(Trace.embedding.is_not(None))\n        .order_by(cosine_dist)\n        .limit(ann_limit)\n        .subquery()\n    )\n    \n    # Re-rank: 70% similarity + 30% trust\n    combined = (ann_q.c.similarity_score * 0.7 + ann_q.c.trust_score * 0.3).label(\"score\")\n    \n    result = await session.execute(\n        select(ann_q, combined).order_by(combined.desc()).limit(limit)\n    )\n    return result.all()\n\n# HNSW index:\n# CREATE INDEX ON traces USING hnsw (embedding vector_cosine_ops)\n# WITH (m=16, ef_construction=64);\n# SET hnsw.ef_search = 100; -- at query time for higher recall\n```\n\nKey points:\n- Fetch ann_limit=100 before trust re-ranking to avoid cutting off high-trust results\n- Wilson score returns [0,1] -- naturally normalized for combination with similarity\n- <=> is cosine distance; 1 - distance = similarity\n- Adjust 0.7/0.3 weights based on corpus maturity and user needs",
    "tags": [
      "postgresql",
      "pgvector",
      "search",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python Wilson score for statistically correct vote ranking",
    "context": "I need to rank traces by user votes. Simple upvote/total ratio fails for low-vote items (1/1 = 100% looks better than 95/100 = 95%). I need Wilson score lower bound for confidence-interval-aware ranking.",
    "solution": "Wilson score implementation:\n\n```python\nimport math\n\ndef wilson_score(confirmed: int, total: int, z: float = 1.9600) -> float:\n    \"\"\"Wilson score lower bound for ranking by vote ratio.\n    \n    Returns 0.0 for no votes. Returns [0, 1] otherwise.\n    z=1.96 for 95% CI (most common). Higher z = more conservative.\n    \"\"\"\n    if total == 0:\n        return 0.0\n    p = confirmed / total\n    d = 1 + z * z / total\n    c = p + z * z / (2 * total)\n    m = z * math.sqrt(p * (1 - p) / total + z * z / (4 * total * total))\n    return (c - m) / d\n\n# Examples:\nprint(wilson_score(1, 1))    # 0.206 -- not confident with just 1 vote\nprint(wilson_score(100, 100)) # 0.963 -- very confident with 100 votes\nprint(wilson_score(95, 100))  # 0.879 -- 95% with good confidence\nprint(wilson_score(0, 10))    # 0.0   -- no upvotes\n\n# Update after vote:\nasync def recompute_trust(session: AsyncSession, trace_id: str) -> float:\n    row = (await session.execute(\n        select(\n            func.count(case((Vote.vote_type == 'confirmed', 1))).label('confirmed'),\n            func.count(Vote.id).label('total'),\n        ).where(Vote.trace_id == trace_id)\n    )).one()\n    score = wilson_score(row.confirmed or 0, row.total or 0)\n    await session.execute(update(Trace).where(Trace.id == trace_id).values(trust_score=score))\n    return score\n```\n\nKey points:\n- Wilson score is used by Reddit for correct comment ranking\n- 1/1 scores ~0.21 not 1.0 -- reflects uncertainty with small sample\n- Score converges to true ratio as vote count increases\n- Set seed trace trust_score=1.0 explicitly -- they bypass the voting system",
    "tags": [
      "python",
      "statistics",
      "search"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL full-text search with GIN index and ts_rank",
    "context": "I need full-text search in PostgreSQL for finding traces by keywords. LIKE queries are too slow and do not support stemming. I want indexed text search with relevance ranking.",
    "solution": "Full-text search with generated tsvector column:\n\n```sql\n-- Generated column (auto-maintained):\nALTER TABLE traces ADD COLUMN search_vector tsvector\n    GENERATED ALWAYS AS (\n        to_tsvector('english',\n            coalesce(title, '') || ' ' ||\n            coalesce(context_text, '')\n        )\n    ) STORED;\n\n-- GIN index for O(log n) search:\nCREATE INDEX ix_traces_search_vector ON traces USING gin(search_vector);\n\n-- Search with relevance ranking:\nSELECT id, title, ts_rank(search_vector, query) AS rank\nFROM traces, to_tsquery('english', 'react & hooks') query\nWHERE search_vector @@ query AND status = 'validated'\nORDER BY rank DESC LIMIT 20;\n```\n\nSQLAlchemy equivalent:\n```python\nfrom sqlalchemy import func, text\n\nts_query = func.to_tsquery('english', 'react & hooks')\nstmt = (\n    select(Trace, func.ts_rank(Trace.search_vector, ts_query).label('rank'))\n    .where(Trace.search_vector.op('@@')(ts_query))\n    .where(Trace.status == 'validated')\n    .order_by(text('rank DESC'))\n    .limit(20)\n)\n\n# For user input (no boolean syntax required):\nts_query = func.plainto_tsquery('english', user_input)\n```\n\nKey points:\n- GENERATED ALWAYS AS STORED keeps tsvector in sync automatically\n- GIN index makes @@ search O(log n) not sequential scan\n- to_tsquery normalizes with stemming -- 'running' matches 'run'\n- Combine with pgvector for hybrid semantic + keyword search\n- Use plainto_tsquery for user input -- handles spaces without boolean operators",
    "tags": [
      "postgresql",
      "search",
      "indexing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis pub/sub for real-time cross-process event broadcasting",
    "context": "I have multiple API instances behind a load balancer and need to broadcast events (trace validated, vote cast) to all instances simultaneously. Redis pub/sub fans out to all subscribers.",
    "solution": "Async Redis pub/sub subscriber:\n\n```python\nimport asyncio\nimport json\nfrom redis.asyncio import Redis\n\nasync def publish_event(redis: Redis, channel: str, event: dict) -> None:\n    await redis.publish(channel, json.dumps(event))\n\nasync def subscribe_events(redis: Redis, handlers: dict) -> None:\n    pubsub = redis.pubsub()\n    await pubsub.subscribe('traces', 'votes')\n    \n    async for message in pubsub.listen():\n        if message['type'] != 'message':\n            continue  # Skip subscription confirmations\n        try:\n            event = json.loads(message['data'])\n            handler = handlers.get(event.get('type'))\n            if handler:\n                await handler(event)\n        except Exception:\n            log.exception('Event processing failed')\n\n# Start in app lifespan:\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    handlers = {\n        'trace_validated': on_trace_validated,\n        'vote_cast': on_vote_cast,\n    }\n    task = asyncio.create_task(subscribe_events(app.state.redis, handlers))\n    yield\n    task.cancel()\n    await asyncio.gather(task, return_exceptions=True)\n```\n\nKey points:\n- Pub/sub is fire-and-forget -- no persistence, no delivery guarantees\n- Use Redis Streams (XADD/XREAD) if you need message persistence or replay\n- Each subscriber gets a copy -- pub/sub is fan-out not a work queue\n- Always background the subscriber -- listening blocks the event loop",
    "tags": [
      "redis",
      "async",
      "python",
      "real-time"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose healthchecks for Redis and worker services",
    "context": "I have Redis and a Python background worker in Docker Compose. I need healthchecks so dependent services only start once Redis is ready, and I want the worker to report healthy only when it is actively connected.",
    "solution": "Healthchecks for Redis and worker:\n\n```yaml\nservices:\n  redis:\n    image: redis:7-alpine\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 3s\n      retries: 5\n      start_period: 10s\n  \n  worker:\n    build: ./api\n    command: python -m app.worker\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    healthcheck:\n      # Check heartbeat file updated within last 60 seconds:\n      test: [\"CMD-SHELL\", \"test -f /tmp/worker-heartbeat && [ $(($(date +%s) - $(stat -c %Y /tmp/worker-heartbeat))) -lt 60 ]\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n```\n\nWorker heartbeat:\n```python\nfrom pathlib import Path\nHEARTBEAT = Path('/tmp/worker-heartbeat')\n\nasync def run_worker():\n    while True:\n        try:\n            await process_batch()\n        except Exception:\n            log.exception('Batch failed')\n        finally:\n            HEARTBEAT.touch()  # Docker checks timestamp\n        await asyncio.sleep(5)\n```\n\nKey points:\n- start_period gives service time to initialize before checks begin\n- redis-cli ping returns PONG on success\n- Heartbeat file timestamp check is simpler than a health HTTP server\n- service_healthy in depends_on waits for healthcheck to pass",
    "tags": [
      "docker",
      "docker-compose",
      "redis",
      "monitoring"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python async retry with exponential backoff",
    "context": "I need to retry failing async operations (external API calls, database operations under transient load) with exponential backoff. I want a reusable decorator and to retry only on specific exception types.",
    "solution": "Async retry decorator with backoff:\n\n```python\nimport asyncio\nimport logging\nfrom typing import TypeVar, Callable, Awaitable, Type\nfrom functools import wraps\n\nlog = logging.getLogger(__name__)\nT = TypeVar('T')\n\ndef async_retry(\n    max_attempts: int = 3,\n    exceptions: tuple[Type[Exception], ...] = (Exception,),\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    backoff: float = 2.0,\n):\n    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:\n        @wraps(func)\n        async def wrapper(*args, **kwargs) -> T:\n            last_exc: Exception | None = None\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exc = e\n                    if attempt == max_attempts:\n                        break\n                    delay = min(base_delay * (backoff ** (attempt - 1)), max_delay)\n                    log.warning(\n                        'Retry attempt %d/%d after %.1fs: %s',\n                        attempt, max_attempts, delay, e\n                    )\n                    await asyncio.sleep(delay)\n            raise last_exc\n        return wrapper\n    return decorator\n\n# Usage:\n@async_retry(max_attempts=3, exceptions=(httpx.TimeoutException, httpx.HTTPStatusError))\nasync def call_embedding_api(text: str) -> list[float]:\n    response = await http_client.post('/embed', json={'text': text})\n    response.raise_for_status()\n    return response.json()['embedding']\n\n# Manual retry with jitter:\nasync def with_jitter_retry(func, max_attempts=3):\n    for attempt in range(max_attempts):\n        try:\n            return await func()\n        except Exception as e:\n            if attempt == max_attempts - 1: raise\n            delay = (2 ** attempt) + random.uniform(0, 1)  # Jitter\n            await asyncio.sleep(delay)\n```\n\nKey points:\n- Retry only on transient errors -- not on 4xx HTTP or validation errors\n- Exponential backoff prevents thundering herd on service recovery\n- Add jitter to prevent synchronized retries from multiple instances\n- @wraps preserves the original function name and docstring\n- Log each retry so you know when retries are happening in production",
    "tags": [
      "python",
      "async",
      "error-handling"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Alembic migration for adding column with default to existing table",
    "context": "I need to add a new NOT NULL column to an existing PostgreSQL table that has data. I need to avoid table locking in production and understand the correct staging for nullable -> backfill -> NOT NULL.",
    "solution": "Staged column addition to avoid locks:\n\n```python\n# migrations/versions/0005_add_is_seed_column.py\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade() -> None:\n    # Step 1: Add as nullable (no lock, no backfill needed)\n    op.add_column('traces', sa.Column('is_seed', sa.Boolean(), nullable=True))\n    \n    # Step 2: Backfill existing rows\n    op.execute(\"UPDATE traces SET is_seed = FALSE WHERE is_seed IS NULL\")\n    \n    # Step 3: Set NOT NULL (fast -- no nulls exist)\n    op.alter_column('traces', 'is_seed', nullable=False)\n    \n    # Step 4: Set server default for future inserts\n    op.alter_column('traces', 'is_seed', server_default=sa.false())\n\ndef downgrade() -> None:\n    op.drop_column('traces', 'is_seed')\n```\n\nFor small tables (safe to lock briefly):\n```python\ndef upgrade() -> None:\n    op.add_column(\n        'traces',\n        sa.Column('is_seed', sa.Boolean(), nullable=False, server_default=sa.false())\n    )\n    # Remove server_default after migration (keep ORM in sync):\n    op.alter_column('traces', 'is_seed', server_default=None)\n```\n\nKey points:\n- Adding NOT NULL column with default backfills all rows and locks table in PG < 11\n- PostgreSQL 11+ supports ADD COLUMN ... DEFAULT without full table rewrite\n- Stage as nullable -> backfill -> NOT NULL for zero-downtime on large tables\n- Always test migrations on a copy of production data first",
    "tags": [
      "python",
      "alembic",
      "postgresql",
      "migrations"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest async fixtures with database transaction rollback",
    "context": "I need fast async pytest tests for my FastAPI application using a real database. I want tests to roll back after each test rather than truncating tables, and override the FastAPI database dependency with the test session.",
    "solution": "Transaction rollback pattern for fast isolated tests:\n\n```python\n# conftest.py\nimport pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\nfrom app.main import app\nfrom app.database import get_db\nfrom app.models.base import Base\n\nTEST_DB = 'postgresql+asyncpg://test:test@localhost:5432/test_db'\n\n@pytest_asyncio.fixture(scope='session')\nasync def engine():\n    engine = create_async_engine(TEST_DB)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    await engine.dispose()\n\n@pytest_asyncio.fixture\nasync def session(engine):\n    async with engine.begin() as conn:\n        async with async_sessionmaker(conn, expire_on_commit=False)() as sess:\n            yield sess\n            await sess.rollback()  # Rollback after each test\n\n@pytest_asyncio.fixture\nasync def client(session):\n    app.dependency_overrides[get_db] = lambda: session\n    async with AsyncClient(transport=ASGITransport(app=app), base_url='http://test') as c:\n        yield c\n    app.dependency_overrides.clear()\n```\n\n```ini\n# pytest.ini\n[pytest]\nasyncio_mode = auto\n```\n\nKey points:\n- Transaction rollback is 10-50x faster than DROP/CREATE or TRUNCATE between tests\n- dependency_overrides swaps the real DB session for the test session\n- scope='session' on engine shares connection pool across all tests\n- Use pytest-asyncio with asyncio_mode = auto to avoid decorating every test",
    "tags": [
      "python",
      "pytest",
      "testing",
      "sqlalchemy",
      "fastapi"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React form handling with react-hook-form and Zod validation",
    "context": "I am building forms in React that have re-render performance issues from controlled inputs, complex validation, and async submission errors from the API. I want a clean performant solution.",
    "solution": "react-hook-form with Zod schema:\n\n```typescript\nimport { useForm } from 'react-hook-form';\nimport { zodResolver } from '@hookform/resolvers/zod';\nimport { z } from 'zod';\n\nconst TraceSchema = z.object({\n  title: z.string().min(5, 'At least 5 characters').max(500),\n  contextText: z.string().min(20, 'Be descriptive'),\n  solutionText: z.string().min(20, 'Include code and explanation'),\n  tags: z.array(z.string()).min(1, 'At least one tag').max(5),\n});\n\ntype TraceForm = z.infer<typeof TraceSchema>;\n\nexport function CreateTraceForm() {\n  const { register, handleSubmit, formState: { errors, isSubmitting }, setError } =\n    useForm<TraceForm>({ resolver: zodResolver(TraceSchema) });\n  \n  const onSubmit = async (data: TraceForm) => {\n    try {\n      await api.traces.create(data);\n    } catch (err) {\n      if (err instanceof ApiError && err.status === 409) {\n        setError('title', { message: 'Title already exists' });\n      } else {\n        setError('root', { message: 'Submission failed. Try again.' });\n      }\n    }\n  };\n  \n  return (\n    <form onSubmit={handleSubmit(onSubmit)}>\n      <input {...register('title')} />\n      {errors.title && <span>{errors.title.message}</span>}\n      {errors.root && <div>{errors.root.message}</div>}\n      <button disabled={isSubmitting}>{isSubmitting ? 'Saving...' : 'Save'}</button>\n    </form>\n  );\n}\n```\n\nKey points:\n- react-hook-form uses uncontrolled inputs -- no re-render per keystroke\n- zodResolver connects Zod schema validation to the form\n- setError('root', ...) for non-field API errors\n- isSubmitting prevents double submission\n- z.infer<typeof Schema> generates TypeScript type from Zod schema",
    "tags": [
      "react",
      "typescript",
      "forms"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "OpenAI streaming chat completions with FastAPI SSE",
    "context": "I want to stream OpenAI chat completion responses to users so tokens appear in real-time. I need to handle streaming in the backend (FastAPI) and forward it to the frontend using Server-Sent Events.",
    "solution": "Stream completions with StreamingResponse:\n\n```python\nimport json\nfrom openai import AsyncOpenAI\nfrom fastapi.responses import StreamingResponse\n\nclient = AsyncOpenAI()\n\nasync def stream_completion(prompt: str):\n    \"\"\"Async generator yielding SSE events.\"\"\"\n    try:\n        async with client.chat.completions.stream(\n            model='claude-opus-4-6',\n            messages=[{'role': 'user', 'content': prompt}],\n            max_tokens=1000,\n        ) as stream:\n            async for event in stream:\n                if event.type == 'content.delta':\n                    yield f'data: {json.dumps({\"text\": event.delta})}\\n\\n'\n        yield 'data: [DONE]\\n\\n'\n    except Exception as e:\n        yield f'data: {json.dumps({\"error\": str(e)})}\\n\\n'\n\n@router.post('/complete')\nasync def complete(body: CompletionRequest):\n    return StreamingResponse(\n        stream_completion(body.prompt),\n        media_type='text/event-stream',\n        headers={\n            'Cache-Control': 'no-cache',\n            'X-Accel-Buffering': 'no',  # Disable Nginx buffering\n        },\n    )\n```\n\nFrontend reader:\n```typescript\nconst response = await fetch('/api/complete', {\n  method: 'POST', body: JSON.stringify({ prompt }),\n  headers: { 'Content-Type': 'application/json' },\n});\nconst reader = response.body!.getReader();\nconst decoder = new TextDecoder();\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  const chunk = decoder.decode(value).replace('data: ', '').trim();\n  if (chunk !== '[DONE]') setOutput(prev => prev + JSON.parse(chunk).text);\n}\n```\n\nKey points:\n- X-Accel-Buffering: no prevents Nginx buffering SSE responses\n- SSE format: data: {json}\\n\\n (double newline terminates each event)\n- Always yield [DONE] sentinel so clients know stream ended cleanly\n- Handle errors inside the generator -- StreamingResponse cannot send HTTP errors after headers sent",
    "tags": [
      "python",
      "openai",
      "fastapi",
      "streaming"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Stripe subscription checkout session creation",
    "context": "I need to integrate Stripe Checkout for subscription payments. Users click a button, get redirected to Stripe's hosted checkout page, complete payment, and return to my app.",
    "solution": "Create Stripe Checkout sessions:\n\n```python\nimport stripe\nfrom fastapi import APIRouter\n\nstripe.api_key = settings.stripe_secret_key\nrouter = APIRouter()\n\n@router.post('/billing/checkout')\nasync def create_checkout(user: CurrentUser, db: DbSession):\n    db_user = await get_user(db, user.id)\n    \n    # Create/get Stripe customer:\n    if not db_user.stripe_customer_id:\n        customer = stripe.Customer.create(\n            email=db_user.email,\n            metadata={'user_id': str(user.id)},\n        )\n        db_user.stripe_customer_id = customer['id']\n        await db.commit()\n    \n    session = stripe.checkout.Session.create(\n        customer=db_user.stripe_customer_id,\n        mode='subscription',\n        line_items=[{'price': settings.stripe_pro_price_id, 'quantity': 1}],\n        success_url=f'{settings.frontend_url}/billing/success?session_id={{CHECKOUT_SESSION_ID}}',\n        cancel_url=f'{settings.frontend_url}/billing/cancel',\n        metadata={'user_id': str(user.id)},\n        allow_promotion_codes=True,\n    )\n    return {'checkout_url': session['url']}\n\n# Confirm via webhook (not success_url):\nasync def activate_subscription(user_id: str, subscription_id: str):\n    await db.execute(\n        update(User).where(User.id == user_id)\n        .values(subscription_status='active', stripe_subscription_id=subscription_id)\n    )\n```\n\nKey points:\n- {CHECKOUT_SESSION_ID} is a Stripe template variable, filled on redirect\n- Confirm payment via webhook checkout.session.completed, not the success URL\n- Always create a Stripe Customer and link to your user -- needed for portal/subscriptions\n- Use mode='payment' for one-time payments, 'subscription' for recurring",
    "tags": [
      "python",
      "stripe",
      "payments",
      "fastapi"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React useMemo and useCallback performance patterns",
    "context": "My React app re-renders excessively. I want practical guidance on when useMemo and useCallback actually help vs when they are premature optimization that adds overhead without benefit.",
    "solution": "Strategic memoization patterns:\n\n```typescript\nimport { useMemo, useCallback, memo } from 'react';\n\n// useCallback: stabilize function refs for memoized children\nfunction SearchPage() {\n  // Without useCallback: new function every render -> SearchResults re-renders\n  const handleSearch = useCallback((q: string) => {\n    setQuery(q);\n    analytics.track('search', { query: q });\n  }, []);  // Stable: function never needs to change\n  \n  return <SearchResults onSearch={handleSearch} />; // memo-wrapped child\n}\n\nconst SearchResults = memo(({ traces, onSearch }: Props) => {\n  return <div>{traces.map(t => <TraceCard key={t.id} trace={t} />)}</div>;\n});\n\n// useMemo: expensive computation or stable object for useEffect\nfunction TraceList({ traces, filters }: Props) {\n  // Filter+sort is expensive -- only recompute when inputs change:\n  const sorted = useMemo(\n    () => traces.filter(t => t.status === filters.status)\n                .sort((a, b) => b.trust_score - a.trust_score),\n    [traces, filters.status]\n  );\n  return <>{sorted.map(t => <TraceCard key={t.id} trace={t} />)}</>;\n}\n```\n\nWhen NOT to memoize:\n- Simple computations (string concatenation, boolean check)\n- Values that change on every render anyway\n- Components without expensive children\n\nKey points:\n- Profile first with React DevTools Profiler -- memoization has overhead\n- memo + useCallback must be used together for callbacks to be effective\n- useMemo for computations taking more than 1ms or for stable object references\n- Objects/arrays in deps cause infinite re-renders -- stabilize with useMemo first",
    "tags": [
      "react",
      "typescript",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python Pydantic v2 field validators and model validators",
    "context": "I am migrating from Pydantic v1 to v2 and need custom validation: field-level validators that transform input (normalize strings, coerce types), cross-field validators comparing multiple fields, and custom error messages.",
    "solution": "Pydantic v2 validators:\n\n```python\nfrom pydantic import BaseModel, field_validator, model_validator, Field\nfrom typing import Self\n\nclass TraceCreate(BaseModel):\n    title: str = Field(min_length=1, max_length=500)\n    context_text: str = Field(min_length=10)\n    solution_text: str = Field(min_length=10)\n    tags: list[str] = Field(default_factory=list, max_length=10)\n\n    @field_validator('title', 'context_text', 'solution_text', mode='before')\n    @classmethod\n    def strip_whitespace(cls, v: str) -> str:\n        return v.strip() if isinstance(v, str) else v\n\n    @field_validator('tags', mode='before')\n    @classmethod\n    def normalize_tags(cls, v: list[str]) -> list[str]:\n        seen, result = set(), []\n        for tag in v:\n            normalized = tag.strip().lower()[:50]\n            if normalized and normalized not in seen:\n                seen.add(normalized)\n                result.append(normalized)\n        return result\n\n    @model_validator(mode='after')\n    def check_solution_length(self) -> Self:\n        if len(self.solution_text) < len(self.context_text) * 0.5:\n            raise ValueError('Solution seems too brief relative to context')\n        return self\n```\n\nKey v1 -> v2 changes:\n- @validator -> @field_validator with explicit mode='before' or 'after'\n- @root_validator -> @model_validator(mode='after') returning Self\n- Validators must be @classmethod\n- model_dump() replaces .dict(), model_validate() replaces .parse_obj()",
    "tags": [
      "python",
      "pydantic",
      "validation"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI middleware for request timing and Prometheus metrics",
    "context": "I want to measure API request duration and expose Prometheus metrics. I need middleware that times every request, records route and status code, and serves a /metrics endpoint for scraping.",
    "solution": "Prometheus middleware with request duration histogram:\n\n```python\n# pip install prometheus-client\nfrom prometheus_client import Histogram, generate_latest, CONTENT_TYPE_LATEST\nfrom fastapi import FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\nimport time\n\nREQUEST_DURATION = Histogram(\n    'http_request_duration_seconds',\n    'HTTP request duration',\n    ['method', 'route', 'status_code'],\n    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0],\n)\n\n@app.middleware('http')\nasync def metrics_middleware(request: Request, call_next):\n    start = time.perf_counter()\n    response = await call_next(request)\n    duration = time.perf_counter() - start\n    \n    # Route template prevents high cardinality:\n    route = request.scope.get('route')\n    path = route.path if isinstance(route, APIRoute) else request.url.path\n    \n    REQUEST_DURATION.labels(\n        method=request.method,\n        route=path,\n        status_code=str(response.status_code),\n    ).observe(duration)\n    return response\n\n@app.get('/metrics', include_in_schema=False)\nasync def metrics():\n    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n```\n\nKey points:\n- Use route template (/traces/{id}) not actual URL to avoid high cardinality\n- time.perf_counter() is more accurate than time.time() for short durations\n- Define metrics at module level -- never inside request handlers\n- Exclude /metrics and /health from your own metrics to reduce noise",
    "tags": [
      "python",
      "fastapi",
      "prometheus",
      "monitoring"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy 2.0 select with filters and efficient pagination",
    "context": "I need a paginated list endpoint in FastAPI with optional filters (status, tag). I want efficient pagination with total count and no N+1 queries.",
    "solution": "Filtered pagination with COUNT subquery:\n\n```python\nfrom sqlalchemy import select, func\nfrom sqlalchemy.orm import selectinload\nimport math\n\nasync def list_traces(\n    session: AsyncSession,\n    status: str | None = None,\n    tag: str | None = None,\n    page: int = 1,\n    page_size: int = 20,\n) -> tuple[list[Trace], int]:\n    base_q = select(Trace).options(selectinload(Trace.tags))\n    \n    if status:\n        base_q = base_q.where(Trace.status == status)\n    if tag:\n        base_q = base_q.where(Trace.tags.any(Tag.name == normalize_tag(tag)))\n    \n    # Count without limit:\n    total = (await session.execute(\n        select(func.count()).select_from(base_q.subquery())\n    )).scalar_one()\n    \n    # Paginated results:\n    offset = (page - 1) * page_size\n    result = await session.execute(\n        base_q.order_by(Trace.created_at.desc())\n              .offset(offset).limit(page_size)\n    )\n    return result.scalars().unique().all(), total\n\n@router.get('/traces')\nasync def list_endpoint(page: int = 1, page_size: int = Query(20, le=100)):\n    traces, total = await list_traces(db, page=page, page_size=page_size)\n    return {\n        'items': traces,\n        'total': total,\n        'page': page,\n        'pages': math.ceil(total / page_size),\n    }\n```\n\nKey points:\n- .scalars().unique().all() deduplicates rows from JOIN-based selectinload\n- Query(20, le=100) caps page size to prevent abuse\n- Build filters dynamically by chaining .where() calls\n- For >10k rows, use keyset pagination (cursor-based) instead of OFFSET",
    "tags": [
      "python",
      "sqlalchemy",
      "fastapi",
      "pagination"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Node.js async patterns: promises, async/await, and streams",
    "context": "I need to understand best practices for async JavaScript/TypeScript: when to use Promise.all vs Promise.allSettled, how to handle errors in async patterns, and how to avoid common pitfalls like unhandled rejections.",
    "solution": "Modern async/await patterns in Node.js:\n\n```typescript\n// Promise.all vs Promise.allSettled:\nasync function fetchTraceData(traceId: string) {\n  // Promise.all: fails fast if ANY promise rejects\n  const [trace, tags, votes] = await Promise.all([\n    getTrace(traceId),\n    getTags(traceId),\n    getVotes(traceId),\n  ]);\n  return { trace, tags, votes };\n}\n\n// Promise.allSettled: get all results even if some fail\nasync function bulkFetch(ids: string[]) {\n  const results = await Promise.allSettled(ids.map(id => getTrace(id)));\n  return results.map((r, i) => ({\n    id: ids[i],\n    trace: r.status === 'fulfilled' ? r.value : null,\n    error: r.status === 'rejected' ? r.reason.message : null,\n  }));\n}\n\n// Sequential with await in loop (only when order matters):\nfor (const trace of traces) {\n  await processTrace(trace); // Sequential -- waits for each\n}\n\n// Concurrent with limit using p-limit:\nimport pLimit from 'p-limit';\nconst limit = pLimit(5);\nconst results = await Promise.all(\n  traces.map(t => limit(() => processTrace(t)))\n);\n\n// Unhandled rejection prevention:\nprocess.on('unhandledRejection', (reason, promise) => {\n  console.error('Unhandled rejection:', reason);\n  process.exit(1);\n});\n```\n\nKey points:\n- Promise.all fails fast -- use when ALL results are needed\n- Promise.allSettled for independent operations where partial failure is ok\n- Avoid await inside forEach -- forEach is not async-aware\n- p-limit for concurrency limiting -- equivalent to asyncio.Semaphore in Python\n- Always attach .catch() or use try/catch to prevent unhandled rejections",
    "tags": [
      "typescript",
      "nodejs",
      "async"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI custom exception hierarchy for clean error responses",
    "context": "My FastAPI application raises various exceptions in different layers and I want all errors to return a consistent JSON structure. I also want to map internal exceptions to appropriate HTTP status codes.",
    "solution": "Custom exception hierarchy with FastAPI handlers:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi.exceptions import RequestValidationError\n\n# Custom exception hierarchy:\nclass AppError(Exception):\n    status_code: int = 500\n    code: str = 'internal_error'\n    def __init__(self, message: str):\n        self.message = message\n\nclass NotFoundError(AppError):\n    status_code, code = 404, 'not_found'\n\nclass ConflictError(AppError):\n    status_code, code = 409, 'conflict'\n\nclass ForbiddenError(AppError):\n    status_code, code = 403, 'forbidden'\n\nclass UnauthorizedError(AppError):\n    status_code, code = 401, 'unauthorized'\n\n# Register handlers:\ndef add_exception_handlers(app: FastAPI) -> None:\n    @app.exception_handler(AppError)\n    async def app_error_handler(request: Request, exc: AppError):\n        return JSONResponse(\n            status_code=exc.status_code,\n            content={'error': exc.code, 'message': exc.message},\n        )\n    \n    @app.exception_handler(RequestValidationError)\n    async def validation_handler(request: Request, exc: RequestValidationError):\n        return JSONResponse(\n            status_code=422,\n            content={'error': 'validation_error', 'detail': exc.errors()},\n        )\n\n# Usage in service layer:\nasync def get_trace_or_404(session, trace_id):\n    trace = await session.get(Trace, trace_id)\n    if not trace:\n        raise NotFoundError(f'Trace {trace_id} not found')\n    return trace\n```\n\nKey points:\n- Custom hierarchy lets you except AppError to catch any app error\n- Never expose internal error details (stack traces, DB errors) in production\n- FastAPI's built-in 422 handler can be overridden for custom format\n- Use specific subclasses in route handlers for clear intent",
    "tags": [
      "python",
      "fastapi",
      "error-handling"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL partial indexes for low-cardinality filter columns",
    "context": "I have PostgreSQL queries that always filter on status='validated' and most queries only target this subset. A full index wastes space indexing pending rows I never query. I want partial indexes for filtered queries.",
    "solution": "Partial indexes with WHERE clause:\n\n```sql\n-- Partial index: only validated traces (much smaller if 90% are validated)\nCREATE INDEX CONCURRENTLY ix_traces_validated_created\nON traces (created_at DESC)\nWHERE status = 'validated';\n\n-- For embedding queue (null embeddings only):\nCREATE INDEX CONCURRENTLY ix_traces_pending_embed\nON traces (created_at ASC)\nWHERE embedding IS NULL AND status = 'validated';\n\n-- Seed trace lookup:\nCREATE INDEX CONCURRENTLY ix_traces_seed\nON traces (title)\nWHERE is_seed = TRUE;\n\n-- Verify index is being used:\nEXPLAIN SELECT * FROM traces\nWHERE status = 'validated' AND embedding IS NOT NULL\nORDER BY created_at DESC LIMIT 10;\n-- Should show: Index Scan using ix_traces_validated_created\n```\n\nAlembic migration:\n```python\ndef upgrade():\n    op.create_index(\n        'ix_traces_pending_embed', 'traces', ['created_at'],\n        postgresql_where=\"embedding IS NULL AND status = 'validated'\",\n    )\n    op.create_index(\n        'ix_traces_validated_created', 'traces', [sa.text('created_at DESC')],\n        postgresql_where=\"status = 'validated'\",\n    )\n```\n\nKey points:\n- Partial index size = fraction of matching rows -- much smaller, faster updates\n- Query WHERE must match index WHERE for planner to use it\n- CONCURRENTLY avoids ACCESS EXCLUSIVE lock -- required for production tables\n- After large data changes, run ANALYZE to update planner statistics",
    "tags": [
      "postgresql",
      "indexing",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python FastAPI pagination with cursor-based keyset navigation",
    "context": "My API returns large datasets and offset-based pagination becomes slow at high page numbers. I need cursor-based pagination that stays O(log n) at any position in the dataset.",
    "solution": "Keyset pagination with opaque cursors:\n\n```python\nimport base64\nimport json\nfrom datetime import datetime\nfrom pydantic import BaseModel\n\ndef encode_cursor(created_at: datetime, id: str) -> str:\n    return base64.urlsafe_b64encode(\n        json.dumps({'ts': created_at.isoformat(), 'id': id}).encode()\n    ).decode()\n\ndef decode_cursor(cursor: str) -> tuple[datetime, str]:\n    data = json.loads(base64.urlsafe_b64decode(cursor))\n    return datetime.fromisoformat(data['ts']), data['id']\n\nclass PagedResponse(BaseModel):\n    items: list\n    next_cursor: str | None\n    has_more: bool\n\nasync def list_keyset(\n    session: AsyncSession, limit: int = 20, cursor: str | None = None\n) -> PagedResponse:\n    query = select(Trace).order_by(Trace.created_at.desc(), Trace.id.desc())\n    \n    if cursor:\n        ts, last_id = decode_cursor(cursor)\n        query = query.where(\n            (Trace.created_at < ts) |\n            ((Trace.created_at == ts) & (Trace.id < last_id))\n        )\n    \n    result = await session.execute(query.limit(limit + 1))  # Fetch one extra\n    items = result.scalars().all()\n    \n    has_more = len(items) > limit\n    items = items[:limit]\n    next_cursor = encode_cursor(items[-1].created_at, str(items[-1].id)) if has_more else None\n    return PagedResponse(items=items, next_cursor=next_cursor, has_more=has_more)\n```\n\nKey points:\n- Keyset pagination is O(log n) regardless of position -- offsets are O(n)\n- Sort by (timestamp, id) -- pure timestamp sort is non-deterministic for same-second rows\n- Encode cursor as opaque base64 -- clients should not parse it\n- Fetch limit+1 to detect has_more without a COUNT query",
    "tags": [
      "python",
      "fastapi",
      "postgresql",
      "pagination"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python dataclass vs Pydantic model vs TypedDict comparison",
    "context": "I need to decide when to use Python dataclasses, Pydantic models, TypedDict, or NamedTuple for different data structures in my application. Each has tradeoffs in validation, serialization, and overhead.",
    "solution": "Choose the right data container for the use case:\n\n```python\n# Pydantic BaseModel -- API boundaries: validation + serialization\nfrom pydantic import BaseModel\nclass TraceCreate(BaseModel):  # API request body\n    title: str\n    context_text: str\n    tags: list[str] = []\n    # Validates on creation, serializes with model_dump()\n\n# dataclass -- internal DTOs: fast, simple, no validation overhead\nfrom dataclasses import dataclass, field\n@dataclass\nclass EmbeddingResult:  # Internal data between worker layers\n    trace_id: str\n    embedding: list[float]\n    model: str\n    tokens_used: int = 0\n\n# TypedDict -- dict-compatible type hints for JSON-like structures\nfrom typing import TypedDict\nclass SearchFilters(TypedDict, total=False):\n    status: str\n    tag: str\n    min_trust: float\n\n# NamedTuple -- small immutable records\nfrom typing import NamedTuple\nclass PaginationMeta(NamedTuple):\n    page: int\n    page_size: int\n    total: int\n    pages: int\n\n# Pydantic Settings -- configuration\nfrom pydantic_settings import BaseSettings\nclass Settings(BaseSettings):\n    database_url: str\n    redis_url: str = 'redis://localhost:6379'\n```\n\nDecision guide:\n- API boundary (in/out): Pydantic -- validation + OpenAPI schema\n- Configuration: Pydantic Settings -- env var support\n- Internal DTOs: dataclass -- fast, no overhead, stdlib\n- Dict-like JSON: TypedDict -- type hints without instantiation cost\n- Small immutable: NamedTuple -- readable, hashable",
    "tags": [
      "python",
      "pydantic",
      "dataclasses",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy async bulk insert with RETURNING for batch operations",
    "context": "I need to bulk insert thousands of rows into PostgreSQL efficiently. Individual inserts in a loop are too slow. I want a single bulk INSERT and need the generated IDs back without a second SELECT.",
    "solution": "Bulk insert with RETURNING for generated IDs:\n\n```python\nfrom sqlalchemy import insert\n\nasync def bulk_insert_traces(\n    session: AsyncSession, trace_dicts: list[dict]\n) -> list[uuid.UUID]:\n    if not trace_dicts:\n        return []\n    \n    stmt = (\n        insert(Trace)\n        .values(trace_dicts)\n        .returning(Trace.id)\n    )\n    result = await session.execute(stmt)\n    ids = result.scalars().all()\n    await session.commit()\n    return ids\n\n# Usage:\nrows = [\n    {\n        'title': f'Trace {i}',\n        'context_text': 'Context...',\n        'solution_text': 'Solution...',\n        'contributor_id': user_id,\n        'status': 'validated',\n        'is_seed': True,\n        'trust_score': 1.0,\n    }\n    for i in range(1000)\n]\nids = await bulk_insert_traces(session, rows)\nprint(f'Inserted {len(ids)} traces')\n\n# For upsert (INSERT ... ON CONFLICT DO NOTHING):\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\n\nstmt = pg_insert(Tag).values(name='python')\nstmt = stmt.on_conflict_do_nothing(index_elements=['name'])\nawait session.execute(stmt)\n```\n\nKey points:\n- Single bulk INSERT is 10-100x faster than loop of individual inserts\n- RETURNING avoids second SELECT for generated IDs/values\n- Batch by 500-1000 rows to avoid PostgreSQL parameter limits\n- PostgreSQL pg_insert for upsert (on_conflict_do_update / on_conflict_do_nothing)",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql",
      "async"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python asyncio TaskGroup for concurrent structured operations",
    "context": "I need to run multiple async operations concurrently and collect all results. I want to handle cases where some may fail independently and need Python 3.11+ structured concurrency approach.",
    "solution": "asyncio.TaskGroup for structured concurrency:\n\n```python\nimport asyncio\n\n# TaskGroup -- structured concurrency (Python 3.11+)\nasync def fetch_trace_details(trace_id: str) -> dict:\n    async with asyncio.TaskGroup() as tg:\n        trace_task = tg.create_task(get_trace(trace_id))\n        tags_task = tg.create_task(get_tags(trace_id))\n        votes_task = tg.create_task(get_vote_count(trace_id))\n    # All done -- any exception propagates as ExceptionGroup\n    return {\n        'trace': trace_task.result(),\n        'tags': tags_task.result(),\n        'votes': votes_task.result(),\n    }\n\n# For Python 3.10 or when tasks are independent:\nresults = await asyncio.gather(\n    get_trace(trace_id),\n    get_tags(trace_id),\n    get_vote_count(trace_id),\n    return_exceptions=True,\n)\nfor result in results:\n    if isinstance(result, Exception):\n        log.error('Task failed', exc_info=result)\n\n# Handle ExceptionGroup (Python 3.11+):\ntry:\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_traces())\n        task2 = tg.create_task(fetch_users())\nexcept* ValueError as eg:\n    for exc in eg.exceptions:\n        log.error('Validation error', exc_info=exc)\n```\n\nKey points:\n- TaskGroup cancels all tasks if any raises -- use gather(return_exceptions=True) for independent failures\n- ExceptionGroup wraps multiple failures -- catch with except* syntax\n- asyncio.timeout() inside tasks prevents indefinite hangs\n- TaskGroup preferred over gather when all tasks must succeed together",
    "tags": [
      "python",
      "async",
      "asyncio",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest parametrize for data-driven tests",
    "context": "I have a function handling many edge cases and want to test all of them without a separate test function per case. I want pytest parametrize with multiple inputs and expected outputs including error cases.",
    "solution": "Data-driven testing with parametrize:\n\n```python\nimport pytest\nfrom app.services.tags import normalize_tag, validate_tag\n\n@pytest.mark.parametrize('raw,expected', [\n    ('Python',      'python'),\n    ('  React  ',   'react'),\n    ('Node.js',     'node.js'),\n    ('type-script', 'type-script'),\n    ('A' * 60,      'a' * 50),  # Truncated to 50\n])\ndef test_normalize_tag(raw: str, expected: str):\n    assert normalize_tag(raw) == expected\n\n@pytest.mark.parametrize('tag,valid', [\n    ('python',    True),\n    ('node.js',   True),\n    ('my-tag',    True),\n    ('',          False),\n    ('has space', False),\n    ('hello!',    False),\n])\ndef test_validate_tag(tag: str, valid: bool):\n    assert validate_tag(tag) == valid\n\n# Testing exceptions:\n@pytest.mark.parametrize('status,next_status,allowed', [\n    ('pending', 'validated', True),\n    ('validated', 'pending', False),\n])\ndef test_status_transition(status, next_status, allowed):\n    result = is_valid_transition(status, next_status)\n    assert result == allowed\n\n# Readable IDs:\n@pytest.mark.parametrize('n,expected', [\n    pytest.param(0, 0.0, id='zero-votes'),\n    pytest.param(1, 0.206, id='one-vote-low-confidence'),\n    pytest.param(100, 0.963, id='many-votes-high-confidence'),\n])\ndef test_wilson_score(n, expected):\n    assert abs(wilson_score(n, n) - expected) < 0.01\n```\n\nKey points:\n- Each parametrize tuple becomes a separate test case in the report\n- Use ids= or pytest.param(..., id=...) for human-readable test names\n- Combine multiple @parametrize decorators for combinatorial testing\n- pytest.param(..., marks=pytest.mark.xfail) marks expected failures",
    "tags": [
      "python",
      "pytest",
      "testing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI API versioning with router prefixes and shared middleware",
    "context": "I need to version my FastAPI API (v1, v2 eventually) without breaking existing clients. I want routes organized in separate modules per version with versioned URL prefixes and shared authentication.",
    "solution": "Router includes with version prefixes:\n\n```python\n# app/routers/v1/__init__.py\nfrom fastapi import APIRouter, Depends\nfrom app.auth import require_auth\nfrom .traces import router as traces_router\nfrom .search import router as search_router\n\nv1_router = APIRouter(\n    prefix='/v1',\n    dependencies=[Depends(require_auth)],  # Auth for all v1 routes\n)\nv1_router.include_router(traces_router, prefix='/traces', tags=['traces'])\nv1_router.include_router(search_router, prefix='/traces', tags=['search'])\n\n# app/main.py\nfrom app.routers.v1 import v1_router\napp = FastAPI()\napp.include_router(v1_router, prefix='/api')\n# Routes: /api/v1/traces, /api/v1/traces/search, etc.\n\n# Per-router auth scope:\nfrom fastapi import APIRouter\ntraces_router = APIRouter()\n\n@traces_router.post('')  # POST /api/v1/traces\nasync def create_trace(body: TraceCreate, user: CurrentUser, db: DbSession):\n    ...\n\n@traces_router.get('/{trace_id}')  # GET /api/v1/traces/{id}\nasync def get_trace(trace_id: uuid.UUID, db: DbSession):\n    ...\n```\n\nKey points:\n- prefix='/api' on the app include so all routes are under /api\n- Separate router modules per resource keep files small\n- Per-router tags group endpoints in Swagger UI\n- Apply auth at router level with dependencies=[] to avoid repetition\n- When adding v2, include a v2_router alongside v1_router",
    "tags": [
      "python",
      "fastapi",
      "api-design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy ORM relationship loading and N+1 prevention",
    "context": "I keep getting MissingGreenlet errors or N+1 queries when accessing related objects on SQLAlchemy models in an async context. I need to understand which loading strategies work with async and how to eagerly load.",
    "solution": "Eager loading strategies for async SQLAlchemy:\n\n```python\nfrom sqlalchemy.orm import selectinload, joinedload\nfrom sqlalchemy import select\n\n# selectinload -- separate SELECT IN query (best for to-many)\nresult = await session.execute(\n    select(Trace)\n    .options(selectinload(Trace.tags))  # Separate: SELECT * FROM tags WHERE id IN (...)\n    .where(Trace.id == trace_id)\n)\ntrace = result.scalar_one_or_none()\n# trace.tags is loaded -- safe to access\n\n# joinedload -- LEFT OUTER JOIN (best for to-one)\nresult = await session.execute(\n    select(Trace)\n    .options(joinedload(Trace.contributor))  # JOIN users ON ...\n    .where(Trace.id == trace_id)\n)\n# Use scalar_one() not scalar_one_or_none() with joinedload (avoids unique row issues)\ntrace = result.unique().scalar_one()\n\n# Nested loading:\nresult = await session.execute(\n    select(User)\n    .options(selectinload(User.traces).selectinload(Trace.tags))\n)\n\n# Prevent accidental lazy loads:\nclass Trace(Base):\n    contributor: Mapped['User'] = relationship('User', lazy='raise')\n```\n\nKey points:\n- Never use default lazy='select' in async -- raises MissingGreenlet\n- selectinload for one-to-many and many-to-many (separate query, efficient)\n- joinedload for many-to-one (JOIN, efficient for single item)\n- lazy='raise' in model definition catches accidental lazy access in tests\n- scalars().unique().all() deduplicates rows when using joins",
    "tags": [
      "python",
      "sqlalchemy",
      "async",
      "orm"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React Suspense and lazy loading for route-level code splitting",
    "context": "My React app has a large JavaScript bundle. I want to split the code so users only download JS for the current page using React.lazy for component-level code splitting.",
    "solution": "React.lazy with Suspense for route splitting:\n\n```typescript\nimport { lazy, Suspense } from 'react';\nimport { BrowserRouter, Routes, Route } from 'react-router-dom';\n\n// Each becomes a separate JS chunk downloaded on demand:\nconst Dashboard = lazy(() => import('./pages/Dashboard'));\nconst TraceDetail = lazy(() => import('./pages/TraceDetail'));\nconst Settings = lazy(() => import('./pages/Settings'));\n\nfunction PageLoader() {\n  return (\n    <div className=\"flex items-center justify-center h-screen\">\n      <div className=\"animate-spin h-8 w-8 border-b-2 border-blue-600 rounded-full\" />\n    </div>\n  );\n}\n\nfunction App() {\n  return (\n    <BrowserRouter>\n      <Suspense fallback={<PageLoader />}>\n        <Routes>\n          <Route path=\"/\" element={<Dashboard />} />\n          <Route path=\"/traces/:id\" element={<TraceDetail />} />\n          <Route path=\"/settings\" element={<Settings />} />\n        </Routes>\n      </Suspense>\n    </BrowserRouter>\n  );\n}\n\n// Preload on hover to reduce perceived latency:\nconst preloadDetail = () => import('./pages/TraceDetail');\n\nfunction TraceCard({ id }: { id: string }) {\n  return (\n    <a href={`/traces/${id}`} onMouseEnter={preloadDetail}>\n      View Trace\n    </a>\n  );\n}\n```\n\nKey points:\n- React.lazy works only with default exports\n- Suspense fallback shows while the chunk is loading\n- Route-level splitting has the highest ROI -- different pages rarely needed together\n- Preload on hover reduces perceived latency for likely navigation\n- Vite/webpack split at import() boundaries automatically",
    "tags": [
      "react",
      "typescript",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI WebSocket for real-time live updates",
    "context": "I need to push real-time updates (trace validated, vote cast) to browser clients. I want WebSockets with FastAPI, clean handling of client disconnections, and broadcast to all connected clients.",
    "solution": "WebSocket manager for broadcast:\n\n```python\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom typing import Any\nimport json\n\nclass ConnectionManager:\n    def __init__(self):\n        self.connections: list[WebSocket] = []\n    \n    async def connect(self, ws: WebSocket) -> None:\n        await ws.accept()\n        self.connections.append(ws)\n    \n    def disconnect(self, ws: WebSocket) -> None:\n        if ws in self.connections:\n            self.connections.remove(ws)\n    \n    async def broadcast(self, data: Any) -> None:\n        dead = []\n        for ws in self.connections:\n            try:\n                await ws.send_text(json.dumps(data))\n            except Exception:\n                dead.append(ws)\n        for ws in dead:\n            self.connections.remove(ws)\n\nmanager = ConnectionManager()\n\n@app.websocket('/ws/updates')\nasync def ws_endpoint(ws: WebSocket):\n    await manager.connect(ws)\n    try:\n        while True:\n            await ws.receive_text()  # Keep connection alive\n    except WebSocketDisconnect:\n        manager.disconnect(ws)\n\n# Broadcast from any route:\n@router.post('/traces/{trace_id}/validate')\nasync def validate_trace(trace_id: str):\n    await do_validate(trace_id)\n    await manager.broadcast({'type': 'trace_validated', 'id': trace_id})\n```\n\nKey points:\n- Always handle WebSocketDisconnect -- clients disconnect at any time\n- Track disconnected sockets during broadcast -- collect and remove after iteration\n- For multi-instance scale, use Redis pub/sub as the broadcast backend\n- Authenticate WebSocket connections via query param or first message",
    "tags": [
      "python",
      "fastapi",
      "websocket",
      "real-time"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python SQLAlchemy event listeners for audit logging",
    "context": "I need to automatically track when rows are inserted or updated in PostgreSQL for audit purposes. I want this without modifying every query in my codebase.",
    "solution": "SQLAlchemy ORM event listeners:\n\n```python\nfrom sqlalchemy import event, inspect\nimport structlog\n\nlog = structlog.get_logger()\n\n@event.listens_for(Trace, 'after_insert')\ndef after_trace_insert(mapper, connection, target):\n    log.info(\n        'trace_created',\n        trace_id=str(target.id),\n        is_seed=target.is_seed,\n        contributor_id=str(target.contributor_id),\n    )\n\n@event.listens_for(Trace, 'after_update')\ndef after_trace_update(mapper, connection, target):\n    state = inspect(target)\n    history = state.attrs.status.history\n    if history.has_changes():\n        log.info(\n            'trace_status_changed',\n            trace_id=str(target.id),\n            old=history.deleted[0] if history.deleted else None,\n            new=target.status,\n        )\n\n# Mixin for all models:\nclass AuditMixin:\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        event.listen(cls, 'after_insert', AuditMixin._log_insert)\n    \n    @staticmethod\n    def _log_insert(mapper, connection, target):\n        log.info('inserted', model=type(target).__name__, id=str(target.id))\n\nclass Trace(AuditMixin, Base):\n    ...\n```\n\nKey points:\n- ORM events fire AFTER SQL executes but BEFORE commit\n- inspect(target).attrs.field.history gives before/after values for update events\n- Keep event listeners lightweight -- they run synchronously in the DB transaction\n- For async sessions, use async_object_session(target) to get the session",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql",
      "logging"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI background tasks for post-response processing",
    "context": "After handling an API request (creating a trace), I want to trigger background processing (generate embeddings) without blocking the response. The work should run after the response is sent to the client.",
    "solution": "FastAPI BackgroundTasks:\n\n```python\nfrom fastapi import BackgroundTasks\n\nasync def generate_embedding_bg(trace_id: str) -> None:\n    embedding = await call_openai(trace_id)\n    await store_embedding(trace_id, embedding)\n\n@router.post('/traces', status_code=201)\nasync def create_trace(\n    body: TraceCreate,\n    background_tasks: BackgroundTasks,\n    db: DbSession,\n):\n    trace = Trace(**body.model_dump())\n    db.add(trace)\n    await db.commit()\n    await db.refresh(trace)\n    \n    # Schedule AFTER commit -- background task reads from DB\n    background_tasks.add_task(generate_embedding_bg, str(trace.id))\n    return trace\n```\n\nFor heavier workloads, use arq:\n```python\n# arq task:\nasync def process_embedding(ctx, trace_id: str):\n    await generate_and_store(trace_id)\n\n# Enqueue from route:\nawait redis.enqueue_job('process_embedding', str(trace.id))\n```\n\nKey points:\n- BackgroundTasks run in the same process after response is sent\n- Not suitable for tasks >30s or needing retry logic -- use arq/Celery\n- Always commit to DB before scheduling background tasks that read that data\n- BackgroundTasks error handling: exceptions are logged but not propagated",
    "tags": [
      "python",
      "fastapi",
      "async",
      "background-tasks"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pydantic v2 Settings for type-safe configuration management",
    "context": "I need to manage configuration for a FastAPI app across local dev, CI, and production environments using environment variables and .env files. I want type-safe settings with validation and defaults.",
    "solution": "pydantic-settings for type-safe configuration:\n\n```python\nfrom pydantic import field_validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file='.env',\n        env_file_encoding='utf-8',\n        case_sensitive=False,\n    )\n    \n    database_url: str\n    redis_url: str = 'redis://localhost:6379'\n    secret_key: str\n    debug: bool = False\n    allowed_hosts: list[str] = ['localhost']\n    openai_api_key: str = ''\n    \n    @field_validator('database_url')\n    @classmethod\n    def validate_db_url(cls, v: str) -> str:\n        if not v.startswith('postgresql+asyncpg://'):\n            raise ValueError('database_url must use asyncpg driver')\n        return v\n\n# Singleton -- import from here, fail fast at startup if misconfigured:\nsettings = Settings()\n```\n\nKey points:\n- BaseSettings reads env vars first, then .env file, then defaults\n- field_validators run after env var parsing\n- Use SecretStr for sensitive values: secret_key: SecretStr (prevents logging)\n- settings = Settings() at module level fails fast at import time if config invalid\n- case_sensitive=False allows both DATABASE_URL and database_url",
    "tags": [
      "python",
      "pydantic",
      "configuration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI response model with computed fields and ORM mode",
    "context": "I have a SQLAlchemy ORM model with sensitive fields I don't want in API responses (api_key_hash, internal flags). I also want computed fields in the response. I need Pydantic v2 ORM mode.",
    "solution": "Pydantic v2 response models with from_attributes:\n\n```python\nfrom pydantic import BaseModel, ConfigDict, computed_field\nfrom datetime import datetime\nimport uuid\n\nclass TagResponse(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n    id: uuid.UUID\n    name: str\n\nclass TraceResponse(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n    \n    id: uuid.UUID\n    title: str\n    context_text: str\n    solution_text: str\n    status: str\n    trust_score: float\n    created_at: datetime\n    tags: list[TagResponse] = []\n    # api_key_hash, is_flagged NOT included -- excluded by default\n    \n    @computed_field\n    @property\n    def is_validated(self) -> bool:\n        return self.status == 'validated'\n    \n    @computed_field\n    @property\n    def trust_label(self) -> str:\n        if self.trust_score >= 0.8: return 'high'\n        if self.trust_score >= 0.5: return 'medium'\n        return 'low'\n\n@router.get('/traces/{trace_id}', response_model=TraceResponse)\nasync def get_trace(trace_id: uuid.UUID, db: DbSession):\n    trace = await get_with_tags(db, trace_id)\n    return trace  # FastAPI serializes via response_model\n```\n\nKey points:\n- from_attributes=True (Pydantic v2) replaces orm_mode=True (Pydantic v1)\n- Only declared fields are included -- sensitive fields excluded by default\n- @computed_field for dynamic fields computed at serialization time\n- response_model=TraceResponse on the route triggers automatic serialization",
    "tags": [
      "python",
      "pydantic",
      "fastapi",
      "api-design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python enum usage with SQLAlchemy and JSON serialization",
    "context": "I am using Python Enum classes for status fields in my application and need them to work with SQLAlchemy (stored as strings), with Pydantic validation, and with JSON serialization without a custom encoder.",
    "solution": "str-based Enums for maximum compatibility:\n\n```python\nimport enum, json\nfrom sqlalchemy import String\nfrom sqlalchemy.orm import mapped_column, Mapped\nfrom pydantic import BaseModel\n\n# str enum -- value IS the string\nclass TraceStatus(str, enum.Enum):\n    pending = 'pending'\n    validated = 'validated'\n\nclass VoteType(str, enum.Enum):\n    confirmed = 'confirmed'\n    disputed = 'disputed'\n\n# SQLAlchemy -- store as String:\nclass Trace(Base):\n    status: Mapped[str] = mapped_column(String(20), default=TraceStatus.pending)\n\n# Works in queries with both enum and string:\nstmt = select(Trace).where(Trace.status == TraceStatus.validated)\nstmt = select(Trace).where(Trace.status == 'validated')  # Also works\n\n# Pydantic -- validates and serializes as string:\nclass TraceResponse(BaseModel):\n    status: TraceStatus  # Accepts 'validated' or TraceStatus.validated\n\n# JSON -- serializes as value (no custom encoder needed):\nstatus = TraceStatus.validated\njson.dumps({'status': status})  # -> '{\"status\": \"validated\"}'\n\n# State transition validation:\nALLOWED_TRANSITIONS = {\n    TraceStatus.pending: {TraceStatus.validated},\n}\n\ndef is_valid_transition(current: TraceStatus, new: TraceStatus) -> bool:\n    return new in ALLOWED_TRANSITIONS.get(current, set())\n```\n\nKey points:\n- str + enum.Enum means instances ARE strings -- no .value access needed\n- Store as String column not Enum -- avoids PostgreSQL ENUM migration complexity\n- Pydantic coerces string input to enum value automatically\n- json.dumps works without a custom encoder for str enums",
    "tags": [
      "python",
      "sqlalchemy",
      "pydantic",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python mocking with unittest.mock for unit tests",
    "context": "I need to unit test code that depends on external services (OpenAI, database, Redis). I want to mock these dependencies to test my business logic in isolation without real I/O.",
    "solution": "unittest.mock patterns for isolation:\n\n```python\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock, patch, call\n\n# Patch at the usage location (not the definition):\n@pytest.mark.asyncio\nasync def test_generate_embedding_success():\n    mock_response = MagicMock()\n    mock_response.data = [MagicMock(embedding=[0.1] * 1536, index=0)]\n    \n    with patch('app.services.embeddings.openai_client.embeddings.create',\n               new=AsyncMock(return_value=mock_response)):\n        result = await generate_embedding('test text')\n        assert len(result) == 1536\n\n# Mock for database calls:\n@pytest.mark.asyncio\nasync def test_create_trace_calls_db():\n    mock_session = AsyncMock()\n    mock_session.add = MagicMock()  # sync method\n    mock_session.commit = AsyncMock()\n    mock_session.refresh = AsyncMock()\n    \n    result = await create_trace(mock_session, TraceCreate(title='Test', ...))\n    \n    mock_session.add.assert_called_once()\n    mock_session.commit.assert_awaited_once()\n\n# Multiple return values:\nmock_func = AsyncMock(side_effect=[first_result, second_result, Exception('fail')])\n\n# Spy (call real function but track calls):\nwith patch('app.services.tags.normalize_tag', wraps=normalize_tag) as spy:\n    result = process_tags(['Python', 'FastAPI'])\n    spy.assert_called()  # Was called\n    assert spy.call_count == 2\n```\n\nKey points:\n- Patch at the import location where it's used, not where it's defined\n- AsyncMock for async functions; MagicMock for sync\n- side_effect for raising exceptions or returning different values per call\n- wraps= for spies -- calls real function but tracks calls",
    "tags": [
      "python",
      "pytest",
      "testing",
      "mocking"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL transaction isolation levels for concurrent operations",
    "context": "I have concurrent reads and writes and am seeing unexpected behavior: dirty reads, non-repeatable reads, or phantom rows. I need to understand PostgreSQL isolation levels and when to use each.",
    "solution": "PostgreSQL isolation levels and when to use them:\n\n```sql\n-- Default: READ COMMITTED (most operations)\n-- Each statement sees data committed BEFORE that statement started\nBEGIN;\nSELECT trust_score FROM traces WHERE id = $1;  -- Reads committed data\nUPDATE traces SET trust_score = $2 WHERE id = $1;\nCOMMIT;\n\n-- REPEATABLE READ (for reports and analytics)\n-- All reads in transaction see same snapshot from transaction start\nBEGIN ISOLATION LEVEL REPEATABLE READ;\nSELECT count(*) FROM traces WHERE status = 'validated';  -- Snapshot taken here\n-- ... other reads see same snapshot\nCOMMIT;\n\n-- SERIALIZABLE (for financial transactions, audit-critical operations)\n-- Transactions execute as if they ran serially, one at a time\nBEGIN ISOLATION LEVEL SERIALIZABLE;\nSELECT sum(amount) FROM accounts WHERE user_id = $1;\nUPDATE accounts SET amount = amount - $2 WHERE user_id = $1;\nCOMMIT;  -- May fail with serialization failure -- retry required\n```\n\nIn SQLAlchemy:\n```python\n# Set isolation for specific operations:\nasync with session.begin():\n    await session.execute(text('SET TRANSACTION ISOLATION LEVEL REPEATABLE READ'))\n    result = await session.execute(complex_analytics_query)\n\n# Per-engine isolation level:\nengine = create_async_engine(url, isolation_level='REPEATABLE READ')\n```\n\nKey points:\n- READ COMMITTED is correct for 99% of operations -- no phantom reads in typical OLTP\n- REPEATABLE READ for reports where consistent snapshot matters\n- SERIALIZABLE has overhead and retry requirement -- avoid unless truly needed\n- PostgreSQL does NOT have dirty reads even at READ UNCOMMITTED\n- Retry serialization failures with exponential backoff",
    "tags": [
      "postgresql",
      "transactions",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL JSONB for flexible metadata storage",
    "context": "I have trace metadata that varies by domain (language version, framework name, OS). I don't want to add columns for every possible field. I need JSONB storage with indexing for specific keys.",
    "solution": "JSONB for flexible schema with indexing:\n\n```sql\n-- Column definition:\nALTER TABLE traces ADD COLUMN metadata_json JSONB DEFAULT '{}';\n\n-- GIN index for any-key queries:\nCREATE INDEX ix_traces_metadata ON traces USING gin(metadata_json);\n\n-- Specific key index (faster for targeted queries):\nCREATE INDEX ix_traces_language ON traces ((metadata_json->>'language'));\n\n-- Queries:\nSELECT * FROM traces WHERE metadata_json @> '{\"language\": \"python\"}';  -- Contains\nSELECT * FROM traces WHERE metadata_json->>'language' = 'python';      -- Key value\nSELECT * FROM traces WHERE metadata_json ? 'language';                  -- Key exists\n\n-- Update specific key:\nUPDATE traces SET metadata_json = metadata_json || '{\"verified\": true}' WHERE id = $1;\n\n-- Extract with default:\nSELECT COALESCE(metadata_json->>'language', 'unknown') FROM traces;\n```\n\nIn SQLAlchemy:\n```python\nfrom sqlalchemy.dialects.postgresql import JSONB\nfrom sqlalchemy import cast\n\nclass Trace(Base):\n    metadata_json: Mapped[dict] = mapped_column(JSONB, default=dict, nullable=False)\n\n# Query by JSONB key:\nstmt = select(Trace).where(\n    Trace.metadata_json['language'].as_string() == 'python'\n)\n\n# Contains operator:\nstmt = select(Trace).where(\n    Trace.metadata_json.op('@>')({\"language\": \"python\"})\n)\n```\n\nKey points:\n- JSONB stores parsed binary -- faster queries than JSON (text)\n- GIN index enables @> (contains) queries\n- Specific key indexes for frequently filtered keys\n- @> (contains) is indexable; ->> (extract text) uses functional index\n- Use JSONB for metadata; use columns for fields you filter/sort on frequently",
    "tags": [
      "postgresql",
      "jsonb",
      "indexing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis caching with cache-aside pattern and TTL management",
    "context": "I have expensive database queries for search results I want to cache in Redis. I need a clean cache-aside pattern that serializes Pydantic models, handles Redis unavailability gracefully, and avoids stale data.",
    "solution": "Cache-aside pattern with Pydantic serialization:\n\n```python\nimport json\nfrom typing import Optional, TypeVar, Type\nfrom pydantic import BaseModel\nimport redis.asyncio as redis\n\nT = TypeVar('T', bound=BaseModel)\n\nasync def cache_get(redis_client: redis.Redis, key: str, model: Type[T]) -> Optional[T]:\n    \"\"\"Cache read -- returns None on miss or Redis error.\"\"\"\n    try:\n        data = await redis_client.get(key)\n        if data is None:\n            return None\n        return model.model_validate_json(data)\n    except Exception:\n        return None  # Fail open on Redis errors\n\nasync def cache_set(\n    redis_client: redis.Redis, key: str, value: BaseModel, ttl: int = 300\n) -> None:\n    \"\"\"Cache write -- silently fails if Redis unavailable.\"\"\"\n    try:\n        await redis_client.setex(key, ttl, value.model_dump_json())\n    except Exception:\n        pass\n\nasync def cache_invalidate(redis_client: redis.Redis, pattern: str) -> None:\n    \"\"\"Delete all keys matching pattern.\"\"\"\n    try:\n        keys = await redis_client.keys(pattern)\n        if keys:\n            await redis_client.delete(*keys)\n    except Exception:\n        pass\n\n# Usage:\n@router.get('/search')\nasync def search(q: str, db: DbSession, redis=Depends(get_redis)):\n    cache_key = f'search:{hash(q.lower().strip())}'\n    \n    cached = await cache_get(redis, cache_key, SearchResponse)\n    if cached:\n        return cached\n    \n    results = await do_search(db, q)\n    response = SearchResponse(results=results)\n    await cache_set(redis, cache_key, response, ttl=300)\n    return response\n```\n\nKey points:\n- Always fail open on Redis errors -- cache is an optimization, not a requirement\n- model_validate_json/model_dump_json for efficient Pydantic serialization\n- Invalidate by pattern when underlying data changes\n- Include all query parameters in cache key for correct scoping\n- setex (SET + EXPIRY) is atomic -- prevents keys without TTL",
    "tags": [
      "python",
      "redis",
      "caching",
      "fastapi"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL EXPLAIN ANALYZE interpretation and query optimization",
    "context": "I have a slow PostgreSQL query and ran EXPLAIN ANALYZE but don't know how to read the output. I need to identify why the query is slow (wrong index, bad join order, row estimate mismatch) and know what to fix.",
    "solution": "Reading EXPLAIN ANALYZE output:\n\n```sql\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT t.id, t.title, array_agg(tg.name) as tags\nFROM traces t\nJOIN trace_tags tt ON tt.trace_id = t.id\nJOIN tags tg ON tg.id = tt.tag_id\nWHERE t.status = 'validated'\nGROUP BY t.id\nORDER BY t.created_at DESC LIMIT 20;\n```\n\nKey nodes to look for:\n```\n-- Good:\nIndex Scan using ix_traces_status_created on traces  (actual time=0.1..5.2 rows=20)\nBitmap Heap Scan on traces  (actual time=12..45 rows=1000)\n\n-- Bad:\nSeq Scan on trace_tags  <- Missing index!\n    (cost=500..1000 rows=100000)  (actual rows=50)  <- 2000x over-estimate -> stale stats\nSort Method: external merge  Disk: 2048kB  <- Sort spilling to disk\nBuffers: shared hit=100 read=5000  <- 5000 disk reads, poor cache hit\n```\n\nDiagnosis and fixes:\n```sql\n-- Seq Scan on join column:\nCREATE INDEX CONCURRENTLY ON trace_tags(trace_id);\n\n-- Stale statistics:\nANALYZE traces;\n\n-- Sort spill:\nSET work_mem = '64MB';  -- Session-level\n\n-- Cost estimate vs actual row mismatch:\n-- Check pg_stats for the column -- may need better statistics target\nALTER TABLE traces ALTER COLUMN status SET STATISTICS 500;\nANALYZE traces;\n```\n\nKey points:\n- Seq Scan on large tables indicates missing index\n- Estimated rows >> actual rows indicates stale statistics -- run ANALYZE\n- external merge in Sort means increasing work_mem will help\n- Buffers: read=N high means data not in shared_buffers -- consider index\n- CONCURRENTLY avoids table lock when adding indexes to production tables",
    "tags": [
      "postgresql",
      "performance",
      "sql"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose override files for environment-specific configuration",
    "context": "I want to use a base docker-compose.yml for common configuration and override files for environment-specific settings (development with hot reload, production with resource limits, CI with test setup).",
    "solution": "Docker Compose override pattern:\n\n```yaml\n# docker-compose.yml -- base config (committed)\nservices:\n  api:\n    build: ./api\n    environment:\n      DATABASE_URL: ${DATABASE_URL}\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n# docker-compose.dev.yml -- development overrides\nservices:\n  api:\n    volumes:\n      - ./api:/app  # Hot reload with bind mount\n    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\n    environment:\n      DEBUG: 'true'\n\n# docker-compose.prod.yml -- production overrides\nservices:\n  api:\n    deploy:\n      resources:\n        limits:\n          memory: 512m\n          cpus: '1.0'\n    restart: unless-stopped\n    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4\n\n# docker-compose.ci.yml -- CI overrides\nservices:\n  api:\n    environment:\n      DATABASE_URL: postgresql+asyncpg://test:test@postgres:5432/test\n```\n\n```bash\n# Usage:\ndocker compose -f docker-compose.yml -f docker-compose.dev.yml up\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Or set COMPOSE_FILE env var:\nexport COMPOSE_FILE=docker-compose.yml:docker-compose.dev.yml\ndocker compose up\n```\n\nKey points:\n- Override files merge with base -- array values are replaced, not extended\n- Bind mount in dev enables hot reload without rebuilding image\n- In production: set restart policy, resource limits, multiple workers\n- COMPOSE_FILE env var avoids specifying -f on every command\n- Keep secrets out of both files -- use .env or secrets management",
    "tags": [
      "docker",
      "docker-compose",
      "deployment"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Dockerfile layer caching optimization",
    "context": "My Docker builds are slow because every code change invalidates the package installation layer. I need to structure my Dockerfile so that dependency installation is cached and only code changes trigger re-execution.",
    "solution": "Optimize layer order for maximum cache hits:\n\n```dockerfile\n# BAD: Code copied first -- any change invalidates pip install\nFROM python:3.12-slim\nCOPY . /app  # Every code change invalidates everything below\nRUN pip install -r requirements.txt\n\n# GOOD: Dependencies before code\nFROM python:3.12-slim\nWORKDIR /app\n\n# 1. System deps (changes rarely)\nRUN apt-get update && apt-get install -y libpq-dev && rm -rf /var/lib/apt/lists/*\n\n# 2. Dependency files only (changes when you add packages)\nCOPY pyproject.toml uv.lock ./\n\n# 3. Install deps (cached if pyproject.toml/uv.lock unchanged)\nCOPY --from=ghcr.io/astral-sh/uv:0.5 /uv /usr/local/bin/uv\nRUN uv sync --frozen --no-install-project --no-dev\n\n# 4. Application code (changes frequently -- last)\nCOPY ./app ./app\nCOPY ./migrations ./migrations\n\n# Layer ordering rule: least-frequently-changed first\n```\n\n.dockerignore (critical for cache validity):\n```\n**/__pycache__\n*.pyc\n.git/\ntests/\n*.md\n.env*\n.venv/\n```\n\nKey points:\n- Docker caches layers -- a changed layer invalidates all subsequent layers\n- Copy package files (requirements.txt, uv.lock) before copying application code\n- System packages should be installed before Python packages\n- .dockerignore prevents irrelevant files from invalidating cache\n- Use RUN --mount=type=cache for pip cache between builds (BuildKit)",
    "tags": [
      "docker",
      "dockerfile",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript React custom hooks for data fetching with SWR",
    "context": "I need reusable data fetching hooks for my React application that handle loading, error, and success states, cache responses, revalidate on focus, and support optimistic updates.",
    "solution": "Custom hooks with SWR for intelligent caching:\n\n```typescript\n// npm install swr\nimport useSWR, { mutate } from 'swr';\nimport useSWRMutation from 'swr/mutation';\n\nconst fetcher = (url: string) => api.fetch<unknown>(url);\n\n// Basic data fetching hook:\nexport function useTrace(traceId: string | undefined) {\n  const { data, error, isLoading } = useSWR<Trace>(\n    traceId ? `/api/v1/traces/${traceId}` : null,\n    fetcher,\n    {\n      revalidateOnFocus: true,  // Refresh when user returns to tab\n      refreshInterval: 30000,   // Poll every 30 seconds\n      onError: (err) => console.error('Failed to load trace:', err),\n    }\n  );\n  return { trace: data, error, isLoading };\n}\n\n// Search hook with debouncing:\nexport function useSearch(query: string) {\n  const debouncedQuery = useDebounce(query, 300);\n  return useSWR<SearchResult>(\n    debouncedQuery ? `/api/v1/traces/search?q=${encodeURIComponent(debouncedQuery)}` : null,\n    fetcher,\n  );\n}\n\n// Mutation with optimistic update:\nexport function useVote(traceId: string) {\n  const { trigger, isMutating } = useSWRMutation(\n    `/api/v1/traces/${traceId}/vote`,\n    async (url, { arg }: { arg: { type: string } }) => {\n      return api.post(url, arg);\n    }\n  );\n  \n  const vote = async (type: string) => {\n    // Optimistic update:\n    await mutate(`/api/v1/traces/${traceId}`,\n      (trace: Trace | undefined) => trace ? { ...trace, confirmation_count: trace.confirmation_count + 1 } : trace,\n      { revalidate: false }\n    );\n    await trigger({ type });\n  };\n  return { vote, isMutating };\n}\n```\n\nKey points:\n- SWR caches by URL key -- same URL = same cache entry across components\n- null key disables fetching (conditional fetching)\n- revalidateOnFocus automatically refreshes stale data when user returns\n- useSWRMutation for POST/PATCH/DELETE operations\n- mutate() for optimistic updates -- update cache before server confirms",
    "tags": [
      "typescript",
      "react",
      "api",
      "caching"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript strict mode configuration and common fixes",
    "context": "I enabled TypeScript strict mode and now have many type errors. I need to understand what strict mode enables, how to fix common errors (possibly undefined, implicit any), and how to migrate existing code incrementally.",
    "solution": "TypeScript strict mode and fixes:\n\n```json\n// tsconfig.json\n{\n  \"compilerOptions\": {\n    \"strict\": true,  // Enables all strict checks\n    // Equivalent to:\n    // \"strictNullChecks\": true,     -- no implicit null/undefined\n    // \"noImplicitAny\": true,        -- no implicit any type\n    // \"strictFunctionTypes\": true,  -- stricter function typing\n    // \"strictPropertyInitialization\": true\n  }\n}\n```\n\nCommon fixes:\n\n```typescript\n// 1. strictNullChecks: T | undefined issues\nfunction getTrace(id: string): Trace | null { ... }\n\nconst trace = getTrace('123');\n// Error: Object is possibly null\ntrace.title; // WRONG\ntrace?.title; // OK -- optional chaining\nif (trace) trace.title; // OK -- type guard\ntrace!.title; // OK -- non-null assertion (use sparingly)\n\n// 2. noImplicitAny: parameter types required\n// WRONG: Parameter 'item' implicitly has an 'any' type\nfunction process(item) { ... }\n// RIGHT:\nfunction process(item: Trace) { ... }\nfunction process(item: unknown) { ... }  // For truly unknown input\n\n// 3. Array access possibly undefined:\nconst items: Trace[] = [];\nconst first = items[0];  // Type: Trace | undefined in strict mode\nif (first) first.title; // Safe\n\n// 4. Incremental migration with @ts-ignore:\n// @ts-ignore -- TODO: fix in next sprint\nlegacyFunction(untypedArg);\n\n// 5. Type assertions for known types:\nconst el = document.getElementById('root') as HTMLDivElement;\n```\n\nKey points:\n- strictNullChecks is most impactful -- enables optional chaining patterns\n- Optional chaining (?.) and nullish coalescing (??) are the primary tools\n- Non-null assertion (!) is an escape hatch -- document why it's safe\n- Migrate file by file using @ts-ignore for unresolved issues\n- unknown is safer than any -- forces explicit type narrowing",
    "tags": [
      "typescript",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions workflow for automatic semantic versioning",
    "context": "I want GitHub Actions to automatically create version tags and GitHub Releases when I push to main. I use conventional commits (feat:, fix:, chore:) and want semantic version bumps based on commit types.",
    "solution": "Semantic versioning with release-please:\n\n```yaml\n# .github/workflows/release.yml\nname: Release\n\non:\n  push:\n    branches: [main]\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    outputs:\n      release_created: ${{ steps.release.outputs.release_created }}\n      tag_name: ${{ steps.release.outputs.tag_name }}\n    \n    steps:\n      - uses: googleapis/release-please-action@v4\n        id: release\n        with:\n          release-type: python\n          # For Node.js: release-type: node\n  \n  docker-publish:\n    needs: release\n    if: ${{ needs.release.outputs.release_created }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ needs.release.outputs.tag_name }}\n      - name: Build and push\n        # ... docker build and push to registry\n```\n\nConventional commit format:\n```\nfeat: add trace search endpoint    -> minor version bump (0.1.0 -> 0.2.0)\nfix: correct pagination offset      -> patch version bump (0.2.0 -> 0.2.1)\nfeat!: redesign API response format -> major version bump (0.2.1 -> 1.0.0)\nchore: update dependencies          -> no version bump\n```\n\nKey points:\n- release-please creates a PR with changelog and version bump\n- Merging that PR creates the GitHub Release and tag\n- Conventional commits (feat/fix/chore/docs) drive semantic version decisions\n- feat! or BREAKING CHANGE in footer triggers major version bump\n- Use release outputs to trigger downstream workflows (Docker publish) only on release",
    "tags": [
      "ci",
      "github-actions",
      "deployment"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions caching for faster builds",
    "context": "My GitHub Actions workflows take 10+ minutes mainly due to installing dependencies on every run. I need to cache dependencies, understand what to cache for Python and Node.js, and handle cache invalidation.",
    "solution": "Dependency caching strategies:\n\n```yaml\n# Python with uv (preferred):\nsteps:\n  - uses: astral-sh/setup-uv@v4\n    with:\n      enable-cache: true\n      cache-dependency-glob: \"**/uv.lock\"  # Invalidate on lockfile change\n\n# Python with pip (manual cache):\n- uses: actions/cache@v4\n  with:\n    path: ~/.cache/pip\n    key: pip-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}\n    restore-keys: pip-${{ runner.os }}-  # Fallback to older cache\n\n# Node.js with npm:\n- uses: actions/setup-node@v4\n  with:\n    node-version: '20'\n    cache: 'npm'  # Built-in caching\n\n# Docker layer caching:\n- uses: docker/build-push-action@v5\n  with:\n    cache-from: type=gha\n    cache-to: type=gha,mode=max\n    # Or registry-based:\n    # cache-from: type=registry,ref=ghcr.io/org/app:cache\n    # cache-to: type=registry,ref=ghcr.io/org/app:cache,mode=max\n\n# General file caching:\n- uses: actions/cache@v4\n  with:\n    path: |\n      ~/.cache/pre-commit\n      .mypy_cache\n    key: tools-${{ hashFiles('.pre-commit-config.yaml', 'mypy.ini') }}\n```\n\nKey points:\n- Cache key with hashFiles() auto-invalidates when dependency files change\n- restore-keys provides fallback to partial cache (saves some time)\n- setup-node with cache: 'npm' handles npm caching automatically\n- Docker GHA cache shares layers between workflow runs on same branch\n- Cache size limit: 10GB per repo in GitHub Actions",
    "tags": [
      "ci",
      "github-actions",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions concurrency to cancel outdated runs",
    "context": "My GitHub Actions workflows queue up multiple runs when I push rapidly. I want to cancel old runs when a new commit is pushed to the same branch, while still running all checks on main.",
    "solution": "Concurrency groups for smart cancellation:\n\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: ['**']\n  pull_request:\n\n# Cancel previous runs on same branch (not on main):\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: uv run pytest\n\n# For PRs: cancel if new commit pushed to the PR branch\n# concurrency:\n#   group: pr-${{ github.event.pull_request.number }}\n#   cancel-in-progress: true\n\n# For deployments: queue instead of cancel\n# concurrency:\n#   group: deploy-${{ github.ref }}\n#   cancel-in-progress: false  # Queue, don't cancel deploys\n```\n\nEnvironment-level concurrency (from GitHub Environments):\n```yaml\njobs:\n  deploy:\n    environment: production\n    # GitHub Environments have built-in concurrency via protection rules\n    # Can require manual approval before deployment\n```\n\nKey points:\n- group key scopes the concurrency -- same group = cancel previous\n- cancel-in-progress: false queues rather than cancels (good for deployments)\n- github.ref includes branch name -- prevents cross-branch cancellation\n- Different groups for test vs deploy -- don't cancel running deployments\n- PR concurrency group by PR number not branch (multiple PRs from same branch)",
    "tags": [
      "ci",
      "github-actions"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "OpenAI API error handling with rate limit backoff",
    "context": "I am hitting OpenAI rate limits and getting RateLimitError exceptions. I need robust retry logic with exponential backoff specifically for OpenAI API calls, and I need to handle different error types differently.",
    "solution": "OpenAI error handling with tenacity:\n\n```python\n# pip install tenacity\nfrom openai import AsyncOpenAI, RateLimitError, APITimeoutError, APIConnectionError\nfrom tenacity import (\n    retry, stop_after_attempt, wait_exponential,\n    retry_if_exception_type, before_sleep_log\n)\nimport logging\n\nclient = AsyncOpenAI()\nlog = logging.getLogger(__name__)\n\n@retry(\n    retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIConnectionError)),\n    wait=wait_exponential(multiplier=1, min=2, max=60),\n    stop=stop_after_attempt(5),\n    before_sleep=before_sleep_log(log, logging.WARNING),\n)\nasync def generate_embedding_with_retry(text: str) -> list[float]:\n    response = await client.embeddings.create(\n        model='text-embedding-3-small',\n        input=text,\n    )\n    return response.data[0].embedding\n\n# Handle non-retryable errors separately:\nasync def safe_embed(text: str) -> list[float] | None:\n    try:\n        return await generate_embedding_with_retry(text)\n    except RateLimitError as e:\n        log.error('Rate limit exhausted after retries', error=str(e))\n        return None\n    except Exception as e:\n        log.error('Unexpected embedding error', error=str(e))\n        return None\n```\n\nRate limit tiers (as of 2024):\n- Tier 1: 500 RPM, 200K TPM for embeddings\n- Each batch of 100 texts uses ~1 request\n\nKey points:\n- Retry on RateLimitError, TimeoutError, ConnectionError (transient)\n- Do NOT retry on AuthenticationError, InvalidRequestError (permanent failures)\n- tenacity wait_exponential: starts at 2s, doubles, caps at 60s\n- Log before_sleep to monitor retry frequency in production\n- Track token usage to stay within tier limits proactively",
    "tags": [
      "python",
      "openai",
      "error-handling",
      "api"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Webhook handling with signature verification and idempotency",
    "context": "I need to handle webhooks from multiple providers (Stripe, GitHub, custom services). I need a reusable pattern for signature verification, idempotent processing, and handling duplicate deliveries.",
    "solution": "Generic webhook handler with verification:\n\n```python\nimport hashlib\nimport hmac\nfrom fastapi import APIRouter, Request, HTTPException\n\nrouter = APIRouter()\n\ndef verify_hmac_signature(\n    payload: bytes,\n    signature: str,\n    secret: str,\n    algorithm: str = 'sha256',\n    prefix: str = 'sha256=',\n) -> bool:\n    \"\"\"Verify HMAC signature (GitHub/Stripe style).\"\"\"\n    expected = hmac.new(\n        secret.encode(),\n        payload,\n        getattr(hashlib, algorithm),\n    ).hexdigest()\n    provided = signature.removeprefix(prefix)\n    return hmac.compare_digest(expected, provided)  # Timing-safe comparison\n\n@router.post('/webhooks/github')\nasync def github_webhook(\n    request: Request,\n    db: DbSession,\n    x_hub_signature_256: str = Header(None),\n):\n    payload = await request.body()\n    \n    if not verify_hmac_signature(\n        payload,\n        x_hub_signature_256 or '',\n        settings.github_webhook_secret,\n        prefix='sha256=',\n    ):\n        raise HTTPException(401, 'Invalid signature')\n    \n    event = request.headers.get('X-GitHub-Event')\n    delivery_id = request.headers.get('X-GitHub-Delivery')\n    \n    # Idempotency:\n    if await is_processed(db, delivery_id):\n        return {'status': 'duplicate'}\n    \n    body = await request.json()\n    match event:\n        case 'push': await handle_push(body)\n        case 'pull_request': await handle_pr(body)\n    \n    await mark_processed(db, delivery_id)\n    return {'status': 'ok'}\n```\n\nKey points:\n- hmac.compare_digest prevents timing attacks (constant-time comparison)\n- Read raw bytes BEFORE parsing JSON -- signature is over raw bytes\n- Store delivery_id for idempotency -- webhooks are delivered at-least-once\n- Return 200 for unknown event types -- provider retries on non-2xx\n- Read body() once and cache -- Request.body() can only be read once in some frameworks",
    "tags": [
      "python",
      "fastapi",
      "webhooks",
      "security"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest fixtures for factory-based test data generation",
    "context": "My tests need realistic data (traces with tags, users with votes). Creating data manually in each test is verbose and fragile. I want factory fixtures that generate realistic test data with sensible defaults and allow overrides.",
    "solution": "Factory fixtures for flexible test data:\n\n```python\n# tests/fixtures/factories.py\nimport pytest\nimport pytest_asyncio\nfrom dataclasses import dataclass\n\n@pytest_asyncio.fixture\nasync def make_user(session):\n    \"\"\"Factory fixture for users.\"\"\"\n    created = []\n    \n    async def factory(\n        email: str = None,\n        display_name: str = 'Test User',\n        is_seed: bool = False,\n    ) -> User:\n        user = User(\n            email=email or f'user_{len(created)}@test.com',\n            display_name=display_name,\n            is_seed=is_seed,\n        )\n        session.add(user)\n        await session.flush()\n        created.append(user)\n        return user\n    \n    return factory\n\n@pytest_asyncio.fixture\nasync def make_trace(session, make_user):\n    \"\"\"Factory for traces with auto-created contributor.\"\"\"\n    async def factory(\n        title: str = 'Test Trace',\n        status: str = 'validated',\n        trust_score: float = 0.8,\n        is_seed: bool = False,\n        contributor: User = None,\n        tags: list[str] = None,\n    ) -> Trace:\n        if contributor is None:\n            contributor = await make_user()\n        \n        trace = Trace(\n            title=title,\n            context_text='Test context',\n            solution_text='Test solution',\n            status=status,\n            trust_score=trust_score,\n            is_seed=is_seed,\n            contributor_id=contributor.id,\n        )\n        session.add(trace)\n        await session.flush()\n        \n        for tag_name in (tags or []):\n            tag = await get_or_create_tag(session, tag_name)\n            await session.execute(trace_tags.insert().values(\n                trace_id=trace.id, tag_id=tag.id\n            ))\n        return trace\n    \n    return factory\n\n# Usage:\nasync def test_search_returns_validated_only(client, make_trace):\n    validated = await make_trace(status='validated', title='React hooks')\n    pending = await make_trace(status='pending', title='React hooks pending')\n    \n    response = await client.get('/api/v1/traces/search?q=react+hooks')\n    ids = [t['id'] for t in response.json()['results']]\n    assert str(validated.id) in ids\n    assert str(pending.id) not in ids\n```\n\nKey points:\n- Factory fixtures return callable functions -- not data directly\n- Sensible defaults let tests override only what they care about\n- auto-created dependencies (contributor) reduce boilerplate in tests\n- Factory tracks created objects if cleanup is needed\n- Combine factories: make_trace uses make_user internally",
    "tags": [
      "python",
      "pytest",
      "testing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Property-based testing with Hypothesis for edge case discovery",
    "context": "I want to go beyond example-based tests to automatically discover edge cases in my code. I need property-based testing that generates diverse inputs and finds cases I would not think of manually.",
    "solution": "Property-based testing with Hypothesis:\n\n```python\n# pip install hypothesis\nimport pytest\nfrom hypothesis import given, settings, assume\nfrom hypothesis import strategies as st\nfrom app.services.tags import normalize_tag, validate_tag\n\n# Property: normalized tag is always lowercase\n@given(st.text(min_size=1, max_size=100))\ndef test_normalize_always_lowercase(raw: str):\n    normalized = normalize_tag(raw)\n    assert normalized == normalized.lower()\n\n# Property: normalized tag never exceeds 50 chars\n@given(st.text(min_size=1, max_size=1000))\ndef test_normalize_truncates_to_50(raw: str):\n    assert len(normalize_tag(raw)) <= 50\n\n# Property: normalizing twice gives same result (idempotent)\n@given(st.text(min_size=1))\ndef test_normalize_idempotent(raw: str):\n    once = normalize_tag(raw)\n    twice = normalize_tag(once)\n    assert once == twice\n\n# Property: valid tag always validates\n@given(st.from_regex(r'^[a-z0-9._-]{1,50}$'))\ndef test_valid_regex_always_validates(tag: str):\n    assert validate_tag(tag) is True\n\n# Property: Wilson score bounds\n@given(\n    confirmed=st.integers(min_value=0, max_value=10000),\n    total=st.integers(min_value=1, max_value=10000),\n)\ndef test_wilson_score_in_bounds(confirmed: int, total: int):\n    assume(confirmed <= total)  # Precondition\n    score = wilson_score(confirmed, total)\n    assert 0.0 <= score <= 1.0\n\n@settings(max_examples=500)  # Run more examples for complex properties\n@given(st.lists(st.text(), min_size=1, max_size=20))\ndef test_normalize_tags_deduplicates(raw_tags):\n    result = normalize_tags(raw_tags)\n    assert len(result) == len(set(result))  # No duplicates\n```\n\nKey points:\n- Hypothesis generates and shrinks -- when it finds a failure, it minimizes the input\n- @given specifies input strategies -- text(), integers(), lists(), from_regex()\n- assume() adds preconditions without affecting example count\n- Properties should be true for all inputs -- not just specific values\n- @settings(max_examples=500) runs more examples for important properties",
    "tags": [
      "python",
      "pytest",
      "testing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python logging best practices with structlog in production",
    "context": "I need consistent structured logging across my Python application. Logs need to be machine-parseable JSON in production but human-readable in development, with context (request_id, user_id) propagated automatically.",
    "solution": "structlog setup with context propagation:\n\n```python\nimport structlog\nimport logging\nimport sys\n\ndef configure_logging(json_logs: bool = False, log_level: str = 'INFO') -> None:\n    processors = [\n        structlog.contextvars.merge_contextvars,  # Inject request-scoped context\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.add_logger_name,\n        structlog.processors.TimeStamper(fmt='iso', utc=True),\n        structlog.processors.StackInfoRenderer(),\n    ]\n    \n    if json_logs:\n        processors.append(structlog.processors.JSONRenderer())\n    else:\n        processors.append(structlog.dev.ConsoleRenderer())\n    \n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(logging.getLevelName(log_level)),\n        logger_factory=structlog.PrintLoggerFactory(sys.stdout),\n        cache_logger_on_first_use=True,\n    )\n\n# Bind context for all logs in a request:\nstructlog.contextvars.bind_contextvars(\n    request_id=request_id,\n    user_id=str(current_user.id),\n)\n\n# Usage throughout the codebase:\nlog = structlog.get_logger(__name__)\nlog.info('trace.created', trace_id=str(trace.id), title=trace.title)\nlog.warning('rate_limit.exceeded', api_key_prefix=key[:8])\nlog.error('embedding.failed', trace_id=str(trace_id), error=str(e))\n\n# Clear context at end of request:\nstructlog.contextvars.clear_contextvars()\n```\n\nKey points:\n- merge_contextvars automatically injects bound vars into every log line\n- JSON output in production for log aggregation (Datadog, Elasticsearch)\n- ConsoleRenderer in development for human-readable output\n- cache_logger_on_first_use: True improves performance\n- Use log.exception('msg') to include stack trace (equivalent to log.error with exc_info=True)",
    "tags": [
      "python",
      "logging",
      "structlog"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python FastAPI API key authentication with SHA-256 hash storage",
    "context": "I need API key authentication for my FastAPI service. API keys must be stored securely (not plaintext), validated on every request, and I want to support multiple keys per user with revocation.",
    "solution": "SHA-256 hash-based API key auth:\n\n```python\nimport hashlib\nimport secrets\nfrom fastapi import HTTPException, Header, Depends\nfrom sqlalchemy import select\n\nAPI_KEY_PREFIX_LENGTH = 8  # For display/lookup (non-secret prefix)\n\ndef generate_api_key() -> tuple[str, str]:\n    \"\"\"Generate API key. Returns (raw_key, hashed_key).\"\"\"\n    raw = secrets.token_urlsafe(32)  # 256 bits of randomness\n    hashed = hashlib.sha256(raw.encode()).hexdigest()\n    return raw, hashed\n\nasync def get_current_user(\n    x_api_key: str = Header(alias='X-API-Key'),\n    db: AsyncSession = Depends(get_db),\n) -> User:\n    if not x_api_key:\n        raise HTTPException(401, 'API key required')\n    \n    # Hash the provided key and look up:\n    key_hash = hashlib.sha256(x_api_key.encode()).hexdigest()\n    result = await db.execute(\n        select(User).where(User.api_key_hash == key_hash)\n    )\n    user = result.scalar_one_or_none()\n    if user is None:\n        raise HTTPException(401, 'Invalid API key')\n    return user\n\nCurrentUser = Annotated[User, Depends(get_current_user)]\n\n# Registration endpoint:\n@router.post('/auth/register')\nasync def register(email: str, db: DbSession):\n    raw_key, hashed = generate_api_key()\n    user = User(email=email, api_key_hash=hashed)\n    db.add(user)\n    await db.commit()\n    # Return raw key ONCE -- not stored, cannot be recovered:\n    return {'api_key': raw_key, 'note': 'Save this -- it cannot be shown again'}\n```\n\nKey points:\n- Never store raw API keys -- store SHA-256 hash only\n- Generate with secrets.token_urlsafe (cryptographically secure)\n- Index api_key_hash column for fast lookup\n- Return raw key only at generation time -- if lost, user must regenerate\n- Use compare_digest if doing manual comparison to prevent timing attacks",
    "tags": [
      "python",
      "fastapi",
      "auth",
      "security"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python async generator for streaming HTTP responses",
    "context": "I need to stream large data exports from my FastAPI application. I want to generate NDJSON (newline-delimited JSON) as an async generator and stream it to the client without loading all data into memory.",
    "solution": "Async generator with StreamingResponse:\n\n```python\nimport json\nfrom fastapi.responses import StreamingResponse\nfrom sqlalchemy import select\n\nasync def trace_export_generator(\n    session: AsyncSession,\n    status: str = 'validated',\n) -> AsyncIterator[str]:\n    \"\"\"Yields NDJSON lines for all matching traces.\"\"\"\n    query = (\n        select(Trace)\n        .where(Trace.status == status)\n        .order_by(Trace.created_at.asc())\n    )\n    \n    async with session.stream(query) as result:\n        async for batch in result.partitions(100):\n            for (trace,) in batch:\n                yield json.dumps({\n                    'id': str(trace.id),\n                    'title': trace.title,\n                    'context_text': trace.context_text,\n                    'solution_text': trace.solution_text,\n                    'trust_score': trace.trust_score,\n                    'created_at': trace.created_at.isoformat(),\n                }) + '\\n'\n\n@router.get('/traces/export')\nasync def export_traces(\n    db: DbSession,\n    status: str = Query('validated'),\n):\n    return StreamingResponse(\n        trace_export_generator(db, status),\n        media_type='application/x-ndjson',\n        headers={'Content-Disposition': 'attachment; filename=traces.ndjson'},\n    )\n```\n\nClient-side parsing (Python):\n```python\nimport httpx, json\n\nwith httpx.stream('GET', '/traces/export') as response:\n    for line in response.iter_lines():\n        trace = json.loads(line)\n        process(trace)\n```\n\nKey points:\n- StreamingResponse with async generator streams HTTP response incrementally\n- session.stream() uses server-side cursor -- constant memory for large tables\n- NDJSON format: one JSON object per line -- easier to parse than one big array\n- Partitions(100) yields in chunks to balance memory vs round-trips\n- Content-Disposition header triggers browser download dialog",
    "tags": [
      "python",
      "fastapi",
      "streaming",
      "async"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL materialized views for expensive aggregation queries",
    "context": "I have expensive aggregation queries (contributor statistics, tag popularity, trust score distributions) that run on every page load. I want to precompute these and refresh them periodically.",
    "solution": "Materialized views with scheduled refresh:\n\n```sql\n-- Create materialized view:\nCREATE MATERIALIZED VIEW contributor_stats AS\nSELECT\n    u.id as contributor_id,\n    u.display_name,\n    count(t.id) as trace_count,\n    count(t.id) FILTER (WHERE t.status = 'validated') as validated_count,\n    avg(t.trust_score) as avg_trust_score,\n    max(t.created_at) as last_contribution\nFROM users u\nLEFT JOIN traces t ON t.contributor_id = u.id\nGROUP BY u.id, u.display_name;\n\n-- Index the materialized view:\nCREATE INDEX ON contributor_stats (contributor_id);\nCREATE INDEX ON contributor_stats (validated_count DESC);\n\n-- Refresh (recalculates from scratch):\nREFRESH MATERIALIZED VIEW contributor_stats;\n\n-- Concurrent refresh (doesn't lock reads):\nREFRESH MATERIALIZED VIEW CONCURRENTLY contributor_stats;\n-- Note: requires at least one unique index:\nCREATE UNIQUE INDEX ON contributor_stats (contributor_id);\n```\n\nScheduled refresh with pg_cron or application:\n```python\n# Refresh in background worker:\nasync def refresh_stats():\n    async with async_session() as session:\n        await session.execute(text(\n            'REFRESH MATERIALIZED VIEW CONCURRENTLY contributor_stats'\n        ))\n        await session.commit()\n\n# Schedule every hour:\nasync def scheduled_refresh():\n    while True:\n        await asyncio.sleep(3600)\n        await refresh_stats()\n```\n\nKey points:\n- Materialized views store query results -- reads are fast table scans\n- CONCURRENTLY refresh avoids read locks but requires unique index\n- Non-concurrent REFRESH takes ACCESS EXCLUSIVE lock -- blocks reads\n- Refresh frequency depends on staleness tolerance (seconds to hours)\n- Use for expensive aggregations that can tolerate slight staleness",
    "tags": [
      "postgresql",
      "performance",
      "sql"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL advisory locks for distributed mutex",
    "context": "I have multiple API instances and need to ensure only one runs a specific operation at a time (e.g., leaderboard rebuild, scheduled job). I need a distributed mutex without adding a separate locking service.",
    "solution": "PostgreSQL advisory locks as distributed mutex:\n\n```python\nfrom sqlalchemy import text\n\nasync def with_advisory_lock(\n    session: AsyncSession,\n    lock_id: int,\n    timeout_ms: int = 5000,\n) -> bool:\n    \"\"\"Try to acquire advisory lock. Returns True if acquired.\"\"\"\n    # Session-level lock (held until session closes or explicit unlock):\n    result = await session.execute(\n        text('SELECT pg_try_advisory_lock(:lock_id)'),\n        {'lock_id': lock_id},\n    )\n    return result.scalar_one()\n\nasync def release_advisory_lock(session: AsyncSession, lock_id: int) -> None:\n    await session.execute(\n        text('SELECT pg_advisory_unlock(:lock_id)'),\n        {'lock_id': lock_id},\n    )\n\n# Transactional advisory lock (auto-released at transaction end):\nasync def with_transactional_lock(session: AsyncSession, lock_id: int) -> None:\n    \"\"\"Blocks until lock acquired. Released automatically at transaction end.\"\"\"\n    await session.execute(\n        text('SELECT pg_advisory_xact_lock(:lock_id)'),\n        {'lock_id': lock_id},\n    )\n\n# Usage:\nLEADERBOARD_LOCK_ID = hash('leaderboard_rebuild') & 0x7FFFFFFF\n\nasync def rebuild_leaderboard_safe():\n    async with async_session() as session:\n        acquired = await with_advisory_lock(session, LEADERBOARD_LOCK_ID)\n        if not acquired:\n            log.info('Leaderboard rebuild already running, skipping')\n            return\n        try:\n            await do_rebuild(session)\n        finally:\n            await release_advisory_lock(session, LEADERBOARD_LOCK_ID)\n```\n\nKey points:\n- Advisory locks are cooperative -- all code must use them for the guarantee\n- pg_try_advisory_lock returns False immediately if lock is held (non-blocking)\n- pg_advisory_lock blocks until acquired (blocking version)\n- Transactional locks (pg_advisory_xact_lock) release automatically at COMMIT/ROLLBACK\n- Lock IDs are integers -- use consistent hashing to generate from string keys",
    "tags": [
      "postgresql",
      "concurrency",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Alembic autogenerate with custom types and constraints",
    "context": "Alembic's autogenerate misses some schema changes: custom PostgreSQL types, check constraints, and index changes. I need to configure autogenerate to detect more changes and add manual DDL for unsupported features.",
    "solution": "Alembic autogenerate configuration and limitations:\n\n```python\n# migrations/env.py -- configure comparison behavior\nfrom alembic import context\nfrom sqlalchemy import event\n\ndef run_migrations_online():\n    connectable = async_engine_from_config(...)\n    \n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=Base.metadata,\n            # Include schema objects autogenerate compares:\n            compare_type=True,           # Detect column type changes\n            compare_server_default=True,  # Detect server default changes\n            include_schemas=True,         # Multi-schema support\n            # Render custom types in migrations:\n            render_as_batch=False,\n        )\n```\n\nFor features autogenerate misses, use manual op.execute():\n```python\n# migrations/versions/0010_add_check_constraints.py\nfrom alembic import op\n\ndef upgrade():\n    # Check constraints (autogenerate misses these):\n    op.execute(\"\"\"\n        ALTER TABLE traces\n        ADD CONSTRAINT ck_trust_score_range\n        CHECK (trust_score >= 0.0 AND trust_score <= 1.0)\n    \"\"\")\n    \n    # Partial indexes (autogenerate misses these):\n    op.execute(\"\"\"\n        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_traces_validated\n        ON traces (created_at DESC)\n        WHERE status = 'validated'\n    \"\"\")\n    \n    # Enum types:\n    op.execute(\"CREATE TYPE vote_type AS ENUM ('confirmed', 'disputed')\")\n\ndef downgrade():\n    op.execute(\"ALTER TABLE traces DROP CONSTRAINT ck_trust_score_range\")\n    op.execute(\"DROP INDEX CONCURRENTLY IF EXISTS ix_traces_validated\")\n    op.execute(\"DROP TYPE IF EXISTS vote_type\")\n```\n\nKey points:\n- compare_type=True detects VARCHAR -> TEXT changes\n- Partial indexes, HNSW indexes, custom functions -- always manual\n- Run autogenerate and review the output -- don't blindly apply\n- Add IF NOT EXISTS / IF EXISTS for idempotent manual migrations\n- Always test downgrade() -- it is used in rollbacks",
    "tags": [
      "python",
      "alembic",
      "postgresql",
      "migrations"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript satisfies operator and const assertions",
    "context": "I want TypeScript to check that objects match a type while preserving the most specific (narrowest) type for inference. I also want const objects with literal types not broadened to string.",
    "solution": "satisfies and as const patterns:\n\n```typescript\n// as const: preserve literal types, not broadened types\nconst STATUS = {\n  pending: 'pending',\n  validated: 'validated',\n} as const;\n// Without as const: { pending: string, validated: string }\n// With as const: { readonly pending: 'pending', readonly validated: 'validated' }\n\ntype TraceStatus = typeof STATUS[keyof typeof STATUS];\n// TraceStatus = 'pending' | 'validated'  (literal union, not just string)\n\n// satisfies: validate against type while keeping narrow type\nconst config = {\n  endpoints: {\n    traces: '/api/v1/traces',\n    search: '/api/v1/traces/search',\n    votes: '/api/v1/votes',\n  },\n  timeout: 5000,\n} satisfies { endpoints: Record<string, string>; timeout: number };\n\n// config.endpoints.traces is still inferred as '/api/v1/traces' (not just string)\n// Without satisfies: type assertion loses the narrower type\n// With satisfies: both type checking AND narrow type preservation\n\n// Use case: route configuration\nconst routes = [\n  { path: '/', component: Dashboard, exact: true },\n  { path: '/traces/:id', component: TraceDetail, exact: false },\n] satisfies Array<{ path: string; component: React.FC; exact: boolean }>;\n\n// Combine both:\nconst PERMISSIONS = {\n  admin: ['read', 'write', 'delete'],\n  user: ['read', 'write'],\n  guest: ['read'],\n} as const satisfies Record<string, readonly string[]>;\n\ntype Role = keyof typeof PERMISSIONS;  // 'admin' | 'user' | 'guest'\n```\n\nKey points:\n- as const makes all values readonly and preserves literal types\n- satisfies validates against a type but keeps inferred narrow type\n- Combine as const satisfies for both readonly literal types and type checking\n- Use for config objects, route tables, and enum-like constants\n- typeof obj[keyof typeof obj] extracts union of values from const objects",
    "tags": [
      "typescript",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React Portal for modals and tooltips outside DOM hierarchy",
    "context": "My modal component is inside a div with overflow:hidden or z-index stacking context that clips it. I need to render the modal at the document.body level while keeping it controlled by my React component.",
    "solution": "React createPortal for out-of-tree rendering:\n\n```typescript\nimport { createPortal } from 'react-dom';\nimport { useEffect, useRef, ReactNode } from 'react';\n\nfunction Modal({\n  isOpen, onClose, children\n}: {\n  isOpen: boolean;\n  onClose: () => void;\n  children: ReactNode;\n}) {\n  const portalRef = useRef<HTMLDivElement | null>(null);\n  \n  // Create portal container on mount:\n  useEffect(() => {\n    const div = document.createElement('div');\n    document.body.appendChild(div);\n    portalRef.current = div;\n    return () => document.body.removeChild(div);\n  }, []);\n  \n  // Handle Escape key:\n  useEffect(() => {\n    const handler = (e: KeyboardEvent) => {\n      if (e.key === 'Escape') onClose();\n    };\n    if (isOpen) document.addEventListener('keydown', handler);\n    return () => document.removeEventListener('keydown', handler);\n  }, [isOpen, onClose]);\n  \n  if (!isOpen || !portalRef.current) return null;\n  \n  return createPortal(\n    <div\n      className=\"fixed inset-0 bg-black/50 flex items-center justify-center z-50\"\n      onClick={e => e.target === e.currentTarget && onClose()}\n      role=\"dialog\"\n      aria-modal=\"true\"\n    >\n      <div className=\"bg-white rounded-lg p-6 max-w-md w-full\">\n        {children}\n        <button onClick={onClose} aria-label=\"Close\">X</button>\n      </div>\n    </div>,\n    portalRef.current,\n  );\n}\n\n// Usage -- works even inside overflow:hidden containers:\nfunction App() {\n  const [showModal, setShowModal] = useState(false);\n  return (\n    <div style={{ overflow: 'hidden' }}>  {/* Portal escapes this */}\n      <button onClick={() => setShowModal(true)}>Open Modal</button>\n      <Modal isOpen={showModal} onClose={() => setShowModal(false)}>\n        <p>Modal content here</p>\n      </Modal>\n    </div>\n  );\n}\n```\n\nKey points:\n- createPortal renders children into a different DOM node than the component tree\n- Events still bubble up through React tree (not DOM tree) -- React event handling works normally\n- Close on backdrop click by checking e.target === e.currentTarget\n- Escape key handling requires explicit event listener (not handled by portal)\n- Use for modals, tooltips, dropdowns that need to escape stacking contexts",
    "tags": [
      "react",
      "typescript"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript module augmentation for extending third-party types",
    "context": "I need to extend types from a third-party library (fastify, express, next.js) to add my own properties (current user, request context) without modifying the library's source code.",
    "solution": "Module augmentation to extend existing types:\n\n```typescript\n// Extend FastAPI/Express request with custom properties:\n// types/express.d.ts\nimport 'express';\n\ndeclare module 'express' {\n  interface Request {\n    user?: { id: string; email: string; role: string; };\n    requestId: string;\n    startTime: number;\n  }\n}\n\n// types/next.d.ts -- extend Next.js App context\nimport type { NextApiRequest } from 'next';\n\ndeclare module 'next' {\n  interface NextApiRequest {\n    user?: AuthUser;\n  }\n}\n\n// Extend environment variables type:\n// types/env.d.ts\ndeclare namespace NodeJS {\n  interface ProcessEnv {\n    DATABASE_URL: string;\n    REDIS_URL: string;\n    OPENAI_API_KEY: string;\n    NODE_ENV: 'development' | 'production' | 'test';\n  }\n}\n\n// Extend window for analytics:\n// types/global.d.ts\ndeclare global {\n  interface Window {\n    analytics?: {\n      track: (event: string, props?: Record<string, unknown>) => void;\n      identify: (userId: string) => void;\n    };\n  }\n}\n\n// Usage -- TypeScript knows about your additions:\napp.use((req, res, next) => {\n  req.requestId = crypto.randomUUID();  // TypeScript knows this exists\n  next();\n});\n\nconst dbUrl: string = process.env.DATABASE_URL; // TypeScript knows it's string not string | undefined\n```\n\nKey points:\n- Module augmentation must be in a .d.ts file or a module file with import/export\n- Use declare module 'package-name' to augment existing modules\n- declare global for augmenting global types (window, process.env)\n- The file must be included in tsconfig.json include or typeRoots\n- Do not use augmentation to add methods that don't exist -- only types for existing runtime behavior",
    "tags": [
      "typescript",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions OIDC for keyless AWS authentication",
    "context": "I need my GitHub Actions workflows to access AWS services (S3, ECR, ECS) without storing long-lived AWS credentials as GitHub secrets. I want keyless authentication using OIDC.",
    "solution": "GitHub Actions OIDC with AWS IAM:\n\n```yaml\n# .github/workflows/deploy.yml\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write  # Required for OIDC\n      contents: read\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Configure AWS credentials via OIDC\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::123456789:role/GitHubActionsRole\n          aws-region: us-east-1\n          # No access key/secret needed!\n      \n      - name: Login to ECR\n        uses: aws-actions/amazon-ecr-login@v2\n      \n      - name: Deploy to ECS\n        run: |\n          aws ecs update-service --cluster prod --service api --force-new-deployment\n```\n\nAWS IAM Role trust policy (one-time setup):\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": { \"Federated\": \"arn:aws:iam::123456789:oidc-provider/token.actions.githubusercontent.com\" },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n      \"StringLike\": {\n        \"token.actions.githubusercontent.com:sub\": \"repo:myorg/myrepo:ref:refs/heads/main\"\n      }\n    }\n  }]\n}\n```\n\nKey points:\n- OIDC tokens are short-lived (15 min) -- no long-lived secrets to rotate\n- id-token: write permission required to request OIDC token\n- IAM trust policy scopes by repo AND branch for security\n- Works for AWS, GCP, Azure -- each has their own action\n- No secrets stored in GitHub -- audit trail in CloudTrail instead",
    "tags": [
      "ci",
      "github-actions",
      "deployment"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis distributed rate limiting across multiple instances",
    "context": "I have multiple FastAPI instances behind a load balancer. Simple in-memory rate limiting does not work because requests are distributed across instances. I need Redis-based rate limiting that works across all instances.",
    "solution": "Redis-backed sliding window rate limiter:\n\n```python\nimport time\nimport redis.asyncio as redis\n\nSLIDING_WINDOW_SCRIPT = \"\"\"\nlocal key = KEYS[1]\nlocal window = tonumber(ARGV[1])  -- window in seconds\nlocal limit = tonumber(ARGV[2])   -- max requests per window\nlocal now = tonumber(ARGV[3])     -- current timestamp (ms)\nlocal window_start = now - window * 1000\n\n-- Remove old entries outside the window:\nredis.call('ZREMRANGEBYSCORE', key, '-inf', window_start)\n\n-- Count remaining entries:\nlocal count = redis.call('ZCARD', key)\n\nif count < limit then\n    -- Add this request:\n    redis.call('ZADD', key, now, now)\n    redis.call('EXPIRE', key, window)\n    return 1  -- allowed\nelse\n    return 0  -- rate limited\nend\n\"\"\"\n\nasync def check_sliding_window_limit(\n    redis_client: redis.Redis,\n    identifier: str,\n    limit: int = 60,\n    window_seconds: int = 60,\n) -> bool:\n    \"\"\"Returns True if request is within rate limit.\"\"\"\n    key = f'rate:{identifier}'\n    now = int(time.time() * 1000)\n    try:\n        result = await redis_client.eval(\n            SLIDING_WINDOW_SCRIPT, 1, key, window_seconds, limit, now\n        )\n        return bool(result)\n    except Exception:\n        return True  # Fail open\n\n# Dependency:\nasync def require_rate_limit(\n    request: Request,\n    redis_client=Depends(get_redis),\n):\n    identifier = request.headers.get('X-API-Key', request.client.host)\n    allowed = await check_sliding_window_limit(redis_client, identifier)\n    if not allowed:\n        raise HTTPException(\n            429,\n            'Rate limit exceeded',\n            headers={'Retry-After': '60'},\n        )\n```\n\nKey points:\n- Sliding window counts actual requests in last N seconds (not fixed window)\n- Sorted set with timestamp as score enables window-based counting\n- ZREMRANGEBYSCORE removes old entries -- O(log n + k) where k is removed count\n- Lua script is atomic -- consistent across concurrent requests from multiple instances\n- Fail open (return True) when Redis is unavailable",
    "tags": [
      "redis",
      "python",
      "rate-limiting",
      "fastapi"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub OAuth2 integration for user authentication",
    "context": "I want to let users log in with their GitHub account. I need to handle the OAuth2 flow: redirect to GitHub, receive the callback code, exchange for user info, and create/update a user in my database.",
    "solution": "GitHub OAuth2 flow with FastAPI:\n\n```python\nimport httpx\nfrom fastapi import APIRouter, Request\nfrom fastapi.responses import RedirectResponse\n\nrouter = APIRouter()\n\nGITHUB_AUTH_URL = 'https://github.com/login/oauth/authorize'\nGITHUB_TOKEN_URL = 'https://github.com/login/oauth/access_token'\nGITHUB_USER_URL = 'https://api.github.com/user'\n\n@router.get('/auth/github')\nasync def github_login(request: Request):\n    import secrets\n    state = secrets.token_urlsafe(32)\n    request.session['oauth_state'] = state  # Store in session\n    \n    params = {\n        'client_id': settings.github_client_id,\n        'redirect_uri': f'{settings.app_url}/auth/github/callback',\n        'scope': 'read:user user:email',\n        'state': state,\n    }\n    return RedirectResponse(f'{GITHUB_AUTH_URL}?{urlencode(params)}')\n\n@router.get('/auth/github/callback')\nasync def github_callback(code: str, state: str, request: Request, db: DbSession):\n    if state != request.session.get('oauth_state'):\n        raise HTTPException(400, 'Invalid state -- possible CSRF')\n    \n    async with httpx.AsyncClient() as client:\n        # Exchange code for access token:\n        token_resp = await client.post(GITHUB_TOKEN_URL, json={\n            'client_id': settings.github_client_id,\n            'client_secret': settings.github_client_secret,\n            'code': code,\n        }, headers={'Accept': 'application/json'})\n        access_token = token_resp.json()['access_token']\n        \n        # Get user info:\n        user_resp = await client.get(GITHUB_USER_URL,\n            headers={'Authorization': f'Bearer {access_token}'}\n        )\n        github_user = user_resp.json()\n    \n    # Upsert user in database:\n    user = await upsert_github_user(db, {\n        'github_id': str(github_user['id']),\n        'email': github_user.get('email'),\n        'display_name': github_user.get('name') or github_user['login'],\n    })\n    \n    # Generate API key for the user:\n    raw_key, hashed = generate_api_key()\n    user.api_key_hash = hashed\n    await db.commit()\n    \n    return RedirectResponse(f'{settings.frontend_url}/auth/success?api_key={raw_key}')\n```\n\nKey points:\n- Validate state parameter to prevent CSRF attacks\n- Exchange code at your backend -- never expose client_secret to frontend\n- Accept: application/json required for GitHub token endpoint\n- Store github_id not just email -- email can change\n- Return API key to frontend via redirect (not ideal -- use secure cookie in production)",
    "tags": [
      "python",
      "fastapi",
      "auth",
      "oauth"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "End-to-end testing with Playwright for web applications",
    "context": "I need end-to-end tests that verify my web application works from the user's perspective. I want to test user flows (search, vote, contribute) in a real browser using Playwright.",
    "solution": "Playwright E2E tests:\n\n```typescript\n// npm install @playwright/test\nimport { test, expect, Page } from '@playwright/test';\n\n// playwright.config.ts\nexport default {\n  testDir: './e2e',\n  use: {\n    baseURL: 'http://localhost:3000',\n    headless: true,\n  },\n  webServer: {\n    command: 'npm run dev',\n    url: 'http://localhost:3000',\n    reuseExistingServer: !process.env.CI,\n  },\n};\n\n// e2e/search.spec.ts\ntest('search returns relevant traces', async ({ page }) => {\n  await page.goto('/');\n  \n  // Type in search box:\n  const searchInput = page.getByPlaceholder('Search traces...');\n  await searchInput.fill('react hooks useState');\n  await searchInput.press('Enter');\n  \n  // Wait for results:\n  await page.waitForSelector('[data-testid=\"search-result\"]');\n  \n  const results = page.locator('[data-testid=\"search-result\"]');\n  await expect(results).toHaveCountGreaterThan(0);\n  \n  // First result should be relevant:\n  const firstTitle = await results.first().locator('h3').textContent();\n  expect(firstTitle?.toLowerCase()).toContain('react');\n});\n\ntest('vote on a trace', async ({ page }) => {\n  // Authenticate first:\n  await page.goto('/login');\n  await page.fill('[name=api_key]', process.env.TEST_API_KEY!);\n  await page.click('[type=submit]');\n  \n  await page.goto('/traces/known-trace-id');\n  \n  const voteButton = page.getByRole('button', { name: 'Confirm' });\n  await voteButton.click();\n  \n  // Verify vote was recorded:\n  await expect(page.getByText('Vote recorded')).toBeVisible();\n});\n```\n\nKey points:\n- webServer auto-starts your dev server for tests\n- getByRole and getByPlaceholder prefer semantic selectors over CSS selectors\n- waitForSelector before accessing elements -- async loading\n- data-testid attributes provide stable selectors that survive CSS changes\n- Use test.describe for grouping and test.beforeEach for shared setup\n- Run in CI with playwright install --with-deps to install browser binaries",
    "tags": [
      "typescript",
      "testing",
      "playwright"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python coverage measurement and reporting in CI",
    "context": "I want to measure test coverage in my Python project, enforce minimum coverage thresholds in CI, and track coverage changes over time. I use pytest and want coverage reported in the CI run.",
    "solution": "pytest-cov with threshold enforcement:\n\n```toml\n# pyproject.toml\n[tool.pytest.ini_options]\naddopts = \"--cov=app --cov-report=term-missing --cov-report=xml --cov-fail-under=80\"\ntestpaths = [\"tests\"]\n\n[tool.coverage.run]\nsource = [\"app\"]\nomit = [\n    \"app/migrations/*\",\n    \"app/tests/*\",\n    \"*/__init__.py\",\n]\nbranch = true  # Measure branch coverage too\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"raise NotImplementedError\",\n    \"if __name__ == .__main__.:\",\n]\n```\n\nGitHub Actions with Codecov:\n```yaml\nsteps:\n  - name: Run tests with coverage\n    run: uv run pytest\n  \n  - name: Upload to Codecov\n    uses: codecov/codecov-action@v4\n    with:\n      token: ${{ secrets.CODECOV_TOKEN }}\n      files: coverage.xml\n      fail_ci_if_error: true\n\n# Add to PR comments:\n  - name: Coverage summary\n    uses: irongut/CodeCoverageSummary@v1.3.0\n    with:\n      filename: coverage.xml\n      badge: true\n      fail_below_min: true\n      thresholds: '80 90'  # Warning at 80, fail at <80\n```\n\nMark untestable code:\n```python\ndef unreachable_code():  # pragma: no cover\n    \"\"\"Defensive code that cannot be triggered in practice.\"\"\"\n    raise RuntimeError('Should never reach here')\n```\n\nKey points:\n- --cov-fail-under=80 fails the test run if coverage drops below 80%\n- branch=true catches untested code paths within functions\n- Codecov tracks coverage trends over time and comments on PRs\n- coverage.xml (machine-readable) + term-missing (human-readable) for CI\n- Exclude generated code and type stubs from coverage measurement",
    "tags": [
      "python",
      "pytest",
      "testing",
      "ci"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python pathlib for cross-platform file operations",
    "context": "I have Python code using os.path and string concatenation for file paths that breaks on Windows. I need to use pathlib for cross-platform file operations that work on Linux, macOS, and Windows.",
    "solution": "pathlib for modern Python file operations:\n\n```python\nfrom pathlib import Path\n\n# Path construction (works on all platforms):\nbase_dir = Path(__file__).parent\ndata_dir = base_dir / 'data'          # Uses / operator\nconfig_file = base_dir / 'config.toml'\n\n# vs old style (fragile):\nimport os\nbase_dir_old = os.path.dirname(__file__)\nconfig_old = os.path.join(base_dir_old, 'config.toml')\n\n# Common operations:\ndata_dir.mkdir(parents=True, exist_ok=True)  # mkdir -p equivalent\nconfig_file.exists()                          # File existence check\nconfig_file.is_file()                         # Is it a file (not dir)\nconfig_file.suffix                            # '.toml'\nconfig_file.stem                              # 'config' (without extension)\nconfig_file.name                              # 'config.toml'\nconfig_file.parent                            # Path to parent directory\n\n# Reading and writing:\ncontent = config_file.read_text(encoding='utf-8')\nbytes_data = config_file.read_bytes()\nconfig_file.write_text('new content', encoding='utf-8')\n\n# Glob patterns:\nfor py_file in data_dir.glob('**/*.py'):\n    print(py_file.name)\n\n# Resolve absolute path:\nabsolute = config_file.resolve()  # Resolves symlinks, makes absolute\n\n# In scripts: reliable __file__ reference\nFIXTURES_DIR = Path(__file__).parent.resolve()\nSAMPLE_FILE = FIXTURES_DIR / 'sample_traces.json'\n```\n\nKey points:\n- Path(a) / 'b' / 'c' is cross-platform; os.path.join(a, 'b', 'c') is too verbose\n- Path(__file__).parent for the directory containing the current script\n- .resolve() makes path absolute and resolves symlinks\n- mkdir(parents=True, exist_ok=True) is equivalent to mkdir -p\n- Path objects work with open(), json.load(), and most stdlib functions",
    "tags": [
      "python",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python click CLI tool with subcommands and options",
    "context": "I need to build a command-line interface for my application with subcommands (import, export, stats), options with validation, and help text. I want it to be testable and support both interactive and piped usage.",
    "solution": "Click CLI with subcommands:\n\n```python\n# cli.py\nimport click\nimport asyncio\nfrom pathlib import Path\n\n@click.group()\n@click.option('--database-url', envvar='DATABASE_URL', help='PostgreSQL connection string')\n@click.pass_context\ndef cli(ctx, database_url: str):\n    \"\"\"CommonTrace CLI -- manage traces and database.\"\"\"\n    ctx.ensure_object(dict)\n    ctx.obj['db_url'] = database_url\n\n@cli.command()\n@click.argument('file', type=click.Path(exists=True, path_type=Path))\n@click.option('--dry-run', is_flag=True, help='Preview without writing to database')\n@click.option('--batch-size', default=100, show_default=True, type=int)\n@click.pass_context\ndef import_seeds(ctx, file: Path, dry_run: bool, batch_size: int):\n    \"\"\"Import seed traces from JSON file.\"\"\"\n    click.echo(f'Importing from {file}...')\n    asyncio.run(_import_seeds(ctx.obj['db_url'], file, dry_run, batch_size))\n\nasync def _import_seeds(db_url: str, file: Path, dry_run: bool, batch_size: int):\n    import json\n    traces = json.loads(file.read_text())\n    click.echo(f'Found {len(traces)} traces')\n    if not dry_run:\n        await do_import(db_url, traces, batch_size)\n        click.echo(click.style('Import complete', fg='green'))\n    else:\n        click.echo(click.style('Dry run -- no changes made', fg='yellow'))\n\n@cli.command()\n@click.option('--format', type=click.Choice(['json', 'csv', 'ndjson']), default='json')\ndef export(format: str):\n    \"\"\"Export traces to stdout.\"\"\"\n    asyncio.run(_export(format))\n\nif __name__ == '__main__':\n    cli()\n```\n\nTesting Click commands:\n```python\nfrom click.testing import CliRunner\n\ndef test_import_dry_run(tmp_path):\n    sample = tmp_path / 'traces.json'\n    sample.write_text('[{\"title\": \"Test\", ...}]')\n    \n    runner = CliRunner()\n    result = runner.invoke(cli, ['import-seeds', str(sample), '--dry-run'])\n    assert result.exit_code == 0\n    assert 'Dry run' in result.output\n```\n\nKey points:\n- @click.group() for subcommand groups; @group.command() for subcommands\n- @click.pass_context passes shared state (db_url) through command hierarchy\n- type=click.Path(exists=True) validates path exists before running\n- click.style() for colored terminal output\n- CliRunner for isolated unit testing of CLI commands",
    "tags": [
      "python",
      "cli",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python dataclass field defaults and post-init processing",
    "context": "I am using Python dataclasses for internal data objects. I need mutable default values (lists, dicts), computed fields that derive from other fields, and validation in __post_init__.",
    "solution": "Dataclass patterns for complex defaults:\n\n```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom typing import Optional\nimport uuid\n\n@dataclass\nclass TraceRecord:\n    title: str\n    context_text: str\n    solution_text: str\n    \n    # Mutable defaults -- use field(default_factory=) not []\n    tags: list[str] = field(default_factory=list)\n    metadata: dict[str, str] = field(default_factory=dict)\n    \n    # Computed at creation time:\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    \n    # Not included in __init__:\n    _cache: dict = field(default_factory=dict, init=False, repr=False)\n    \n    def __post_init__(self):\n        # Validation:\n        if not self.title.strip():\n            raise ValueError('title cannot be empty')\n        if len(self.tags) > 10:\n            raise ValueError('maximum 10 tags')\n        \n        # Normalization:\n        self.title = self.title.strip()\n        self.tags = [t.lower().strip() for t in self.tags]\n\n# Usage:\ntrace = TraceRecord(\n    title='FastAPI setup',\n    context_text='Setting up FastAPI...',\n    solution_text='Here is how...',\n    tags=['python', 'FastAPI'],  # Normalized to ['python', 'fastapi']\n)\n\n# Immutable dataclass:\n@dataclass(frozen=True)\nclass TagKey:\n    name: str\n    domain: str\n    \n    def __hash__(self): return hash((self.name, self.domain))\n```\n\nKey points:\n- field(default_factory=list) not tags: list = [] -- mutable defaults shared across instances\n- field(init=False) excludes from __init__ -- computed internally\n- __post_init__ runs after __init__ -- use for validation and normalization\n- frozen=True makes instances hashable and immutable (usable as dict keys)\n- @dataclass(eq=True) (default) generates __eq__ based on all fields",
    "tags": [
      "python",
      "dataclasses",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy many-to-many relationships with join table",
    "context": "I have a many-to-many relationship between traces and tags. I need to create the join table correctly in SQLAlchemy, handle tag insertion with get-or-create semantics, and query traces by tag efficiently.",
    "solution": "Many-to-many with secondary join table:\n\n```python\nfrom sqlalchemy import Table, Column, ForeignKey, select\nfrom sqlalchemy.dialects.postgresql import UUID\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\n\n# Join table definition:\ntrace_tags = Table(\n    'trace_tags',\n    Base.metadata,\n    Column('trace_id', UUID(as_uuid=True), ForeignKey('traces.id'), primary_key=True),\n    Column('tag_id', UUID(as_uuid=True), ForeignKey('tags.id'), primary_key=True),\n)\n\nclass Trace(Base):\n    tags: Mapped[list['Tag']] = relationship(\n        'Tag', secondary='trace_tags', back_populates='traces'\n    )\n\nclass Tag(Base):\n    traces: Mapped[list['Trace']] = relationship(\n        'Trace', secondary='trace_tags', back_populates='tags'\n    )\n\n# Get-or-create tag (idempotent):\nasync def get_or_create_tag(session: AsyncSession, name: str) -> Tag:\n    normalized = normalize_tag(name)\n    result = await session.execute(select(Tag).where(Tag.name == normalized))\n    tag = result.scalar_one_or_none()\n    if tag is None:\n        tag = Tag(name=normalized)\n        session.add(tag)\n        await session.flush()  # Get ID without committing\n    return tag\n\n# Insert into join table directly (avoid lazy load issues):\nfrom sqlalchemy import insert\n\nasync def add_trace_tags(session: AsyncSession, trace: Trace, tag_names: list[str]) -> None:\n    for name in tag_names:\n        if not validate_tag(normalize_tag(name)):\n            continue\n        tag = await get_or_create_tag(session, name)\n        await session.execute(\n            trace_tags.insert().values(trace_id=trace.id, tag_id=tag.id)\n        )\n\n# Query traces by tag:\nstmt = select(Trace).where(Trace.tags.any(Tag.name == 'python'))\n```\n\nKey points:\n- Direct insert into join table avoids lazy-load issues in async context\n- flush() after adding tag gets ID without committing the transaction\n- relationship secondary= links via the join table automatically for ORM queries\n- back_populates= enables bidirectional navigation\n- For async: always use selectinload or joinedload, not lazy loading",
    "tags": [
      "python",
      "sqlalchemy",
      "postgresql"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python FastAPI dependency injection for database transactions",
    "context": "I need my FastAPI routes to participate in database transactions where multiple operations must succeed or fail together. I want a dependency that manages the transaction lifecycle.",
    "solution": "Transaction-scoped dependency:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom typing import Annotated\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.database import async_session_factory\n\nasync def get_db():\n    \"\"\"Provides a session with auto-commit on success, rollback on error.\"\"\"\n    async with async_session_factory() as session:\n        async with session.begin():\n            yield session\n        # Commits on success, rolls back on exception\n\nDbSession = Annotated[AsyncSession, Depends(get_db)]\n\n# For manual transaction control:\nasync def get_db_manual():\n    \"\"\"Provides a session without auto-commit -- caller controls transaction.\"\"\"\n    async with async_session_factory() as session:\n        yield session\n\n# Route that uses auto-commit:\n@router.post('/votes')\nasync def cast_vote(vote: VoteCreate, db: DbSession):\n    new_vote = Vote(**vote.model_dump())\n    db.add(new_vote)\n    # No commit needed -- session.begin() handles it\n    return new_vote\n\n# Route with multiple dependent operations:\n@router.post('/traces/{trace_id}/validate')\nasync def validate_trace(trace_id: str, db: DbSession):\n    trace = await db.get(Trace, trace_id)\n    if not trace:\n        raise HTTPException(404)\n    \n    trace.status = 'validated'\n    trace.trust_score = 1.0\n    \n    # Update contributor stats in same transaction:\n    await update_contributor_stats(db, trace.contributor_id)\n    \n    # Both updates commit together or both roll back\n```\n\nKey points:\n- session.begin() as context manager auto-commits on exit, rolls back on exception\n- All operations in one route handler share one transaction (one session)\n- Raise exceptions to trigger rollback -- FastAPI exception handlers still fire\n- For read-only routes, no transaction needed -- but session.begin() is low overhead\n- expire_on_commit=False prevents attribute access after commit from triggering lazy loads",
    "tags": [
      "python",
      "fastapi",
      "sqlalchemy",
      "async"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React compound components pattern for flexible UI composition",
    "context": "I have a complex UI component (a card with header, body, footer, and actions) that is hard to reuse because the structure is too rigid. I want a flexible composition pattern that lets callers customize specific parts.",
    "solution": "Compound components with React.createContext:\n\n```typescript\nimport { createContext, useContext, ReactNode } from 'react';\n\n// Context for the compound:\ninterface CardContext {\n  isSelected: boolean;\n  onSelect: () => void;\n}\nconst CardCtx = createContext<CardContext | null>(null);\n\n// Root component:\nfunction Card({ children, isSelected = false, onSelect = () => {} }: {\n  children: ReactNode;\n  isSelected?: boolean;\n  onSelect?: () => void;\n}) {\n  return (\n    <CardCtx.Provider value={{ isSelected, onSelect }}>\n      <div className={`card ${isSelected ? 'selected' : ''}`}>{children}</div>\n    </CardCtx.Provider>\n  );\n}\n\n// Sub-components:\nfunction CardHeader({ children }: { children: ReactNode }) {\n  const { isSelected, onSelect } = useContext(CardCtx)!;\n  return (\n    <div className=\"card-header\" onClick={onSelect}>\n      {isSelected && <span>\u2713</span>}\n      {children}\n    </div>\n  );\n}\n\nfunction CardBody({ children }: { children: ReactNode }) {\n  return <div className=\"card-body\">{children}</div>;\n}\n\nfunction CardActions({ children }: { children: ReactNode }) {\n  return <div className=\"card-actions\">{children}</div>;\n}\n\n// Attach sub-components:\nCard.Header = CardHeader;\nCard.Body = CardBody;\nCard.Actions = CardActions;\n\n// Usage -- callers control structure:\nfunction TraceCard({ trace }: { trace: Trace }) {\n  const [selected, setSelected] = useState(false);\n  return (\n    <Card isSelected={selected} onSelect={() => setSelected(!selected)}>\n      <Card.Header><h3>{trace.title}</h3></Card.Header>\n      <Card.Body><p>{trace.context_text}</p></Card.Body>\n      <Card.Actions>\n        <VoteButton traceId={trace.id} />\n      </Card.Actions>\n    </Card>\n  );\n}\n```\n\nKey points:\n- Context shares state between compound components without prop drilling\n- Callers control which sub-components to render and in what order\n- Sub-components attached to parent (Card.Header) for discoverable API\n- Better than render props for complex multi-part components\n- Document required sub-components in TypeScript types or JSDoc",
    "tags": [
      "react",
      "typescript",
      "design"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL row-level security for multi-tenant data isolation",
    "context": "I am building a multi-tenant SaaS where each user should only see their own data. I want PostgreSQL row-level security (RLS) policies to enforce data isolation at the database level.",
    "solution": "PostgreSQL RLS for multi-tenant isolation:\n\n```sql\n-- Enable RLS on table:\nALTER TABLE traces ENABLE ROW LEVEL SECURITY;\n\n-- Policy: users can only see their own traces\nCREATE POLICY traces_user_isolation ON traces\n    USING (contributor_id = current_setting('app.current_user_id')::uuid);\n\n-- Policy: allow all reads (seed traces visible to all):\nCREATE POLICY traces_read ON traces\n    FOR SELECT\n    USING (\n        contributor_id = current_setting('app.current_user_id')::uuid\n        OR is_seed = TRUE\n        OR status = 'validated'  -- Validated traces are public\n    );\n\n-- Policy: only owner can update/delete:\nCREATE POLICY traces_write ON traces\n    FOR UPDATE USING (contributor_id = current_setting('app.current_user_id')::uuid);\n\nCREATE POLICY traces_delete ON traces\n    FOR DELETE USING (contributor_id = current_setting('app.current_user_id')::uuid);\n\n-- Admin bypass (superuser or designated admin role):\nCREATE ROLE app_admin;\nALTER TABLE traces FORCE ROW LEVEL SECURITY;\nGRANT ALL ON traces TO app_admin;\n-- Note: BYPASSRLS privilege for admin users\n```\n\nIn SQLAlchemy -- set the user context per session:\n```python\nasync def get_db_with_rls(current_user: CurrentUser) -> AsyncSession:\n    async with async_session_factory() as session:\n        # Set per-session context variable:\n        await session.execute(\n            text(\"SELECT set_config('app.current_user_id', :uid, true)\"),\n            {'uid': str(current_user.id)},\n        )\n        async with session.begin():\n            yield session\n```\n\nKey points:\n- RLS policies are enforced at the DB level -- application bugs cannot bypass\n- current_setting() reads session-level configuration variables\n- FORCE ROW LEVEL SECURITY applies to table owner (superuser) too\n- Always test RLS with the application's database role (not superuser)\n- Performance: RLS adds WHERE clause to every query -- index the filtered column",
    "tags": [
      "postgresql",
      "security",
      "multi-tenant"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL full-text search with tsvector and tsquery",
    "context": "Need to add full-text search to a PostgreSQL table. Users type natural-language queries like 'python async error handling' and expect relevant rows returned quickly without external search infrastructure.",
    "solution": "Add a `tsvector` column, populate it with `to_tsvector`, index with GIN, and query with `plainto_tsquery`:\n\n```sql\n-- Add tsvector column and GIN index\nALTER TABLE traces ADD COLUMN search_vector tsvector;\nCREATE INDEX idx_traces_search_vector ON traces USING GIN(search_vector);\n\n-- Populate the column\nUPDATE traces\nSET search_vector = to_tsvector('english', coalesce(title, '') || ' ' || coalesce(context_text, '') || ' ' || coalesce(solution_text, ''));\n\n-- Keep it updated via trigger\nCREATE OR REPLACE FUNCTION update_search_vector()\nRETURNS trigger AS $$\nBEGIN\n  NEW.search_vector := to_tsvector('english',\n    coalesce(NEW.title, '') || ' ' || coalesce(NEW.context_text, '') || ' ' || coalesce(NEW.solution_text, ''));\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER traces_search_vector_update\nBEFORE INSERT OR UPDATE ON traces\nFOR EACH ROW EXECUTE FUNCTION update_search_vector();\n\n-- Query with ranking\nSELECT id, title, ts_rank(search_vector, query) AS rank\nFROM traces, plainto_tsquery('english', 'python async error') query\nWHERE search_vector @@ query\nORDER BY rank DESC\nLIMIT 20;\n```\n\nIn SQLAlchemy:\n```python\nfrom sqlalchemy import func, text\n\nasync def full_text_search(session: AsyncSession, q: str) -> list[Trace]:\n    query = func.plainto_tsquery('english', q)\n    result = await session.execute(\n        select(Trace)\n        .where(Trace.search_vector.op('@@')(query))\n        .order_by(func.ts_rank(Trace.search_vector, query).desc())\n        .limit(20)\n    )\n    return result.scalars().all()\n```\n\nKey decisions: GIN index for `tsvector` (not GIST \u2014 GIN is faster for search, GIST faster for updates). Use `plainto_tsquery` over `to_tsquery` for user input (handles plain text, no syntax errors). Trigger ensures the column stays current.",
    "tags": [
      "postgresql",
      "full-text-search",
      "tsvector",
      "gin-index"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL row-level security (RLS) with SQLAlchemy",
    "context": "Need to enforce data isolation so each tenant can only see their own rows. Application-level filtering is error-prone \u2014 a missing WHERE clause leaks all data. Want database-enforced isolation.",
    "solution": "Enable RLS on the table and create policies. Pass the current tenant via `SET LOCAL`:\n\n```sql\n-- Enable RLS\nALTER TABLE traces ENABLE ROW LEVEL SECURITY;\nALTER TABLE traces FORCE ROW LEVEL SECURITY;\n\n-- Policy: user sees only their own traces\nCREATE POLICY traces_tenant_isolation ON traces\n    USING (contributor_id = current_setting('app.current_user_id', true)::uuid);\n\n-- Create a limited role for the app (bypasses RLS by default for superuser)\nCREATE ROLE app_user;\nGRANT SELECT, INSERT, UPDATE, DELETE ON traces TO app_user;\n```\n\nIn SQLAlchemy, set the session variable per request:\n```python\nfrom sqlalchemy import event, text\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nasync def set_tenant_context(session: AsyncSession, user_id: str) -> None:\n    await session.execute(text(f\"SET LOCAL app.current_user_id = '{user_id}'\"))\n\n# In FastAPI dependency\nasync def get_session_with_tenant(\n    current_user: User = Depends(get_current_user),\n    session: AsyncSession = Depends(get_db),\n) -> AsyncGenerator[AsyncSession, None]:\n    async with session.begin():\n        await set_tenant_context(session, str(current_user.id))\n        yield session\n```\n\nKey consideration: `SET LOCAL` applies only to the current transaction, which is exactly what you want with connection pooling. Bypass RLS for admin operations with `SET row_security = off` (requires superuser or `BYPASSRLS` attribute).",
    "tags": [
      "postgresql",
      "row-level-security",
      "multi-tenant",
      "sqlalchemy"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL JSONB indexing and querying patterns",
    "context": "Storing metadata as JSONB columns for flexibility, but queries like `WHERE data->>'status' = 'active'` do full table scans. Need to index JSONB fields and write efficient queries.",
    "solution": "Use GIN indexes for containment queries, expression indexes for specific key access:\n\n```sql\n-- GIN index for containment @> operator\nCREATE INDEX idx_metadata_gin ON events USING GIN(metadata);\n\n-- Expression index for specific key (more efficient when querying one key)\nCREATE INDEX idx_metadata_status ON events ((metadata->>'status'));\n\n-- Containment query (uses GIN)\nSELECT * FROM events WHERE metadata @> '{\"status\": \"active\"}';\n\n-- Key access query (uses expression index)\nSELECT * FROM events WHERE metadata->>'status' = 'active';\n\n-- Nested key access\nSELECT * FROM events WHERE metadata->'user'->>'email' LIKE '%@example.com';\n\n-- JSONB array containment\nSELECT * FROM events WHERE metadata->'tags' ? 'python';\n\n-- Update specific key\nUPDATE events\nSET metadata = jsonb_set(metadata, '{status}', '\"archived\"')\nWHERE id = $1;\n```\n\nIn SQLAlchemy:\n```python\nfrom sqlalchemy import cast\nfrom sqlalchemy.dialects.postgresql import JSONB\n\n# Containment\nstmt = select(Event).where(Event.metadata.contains({'status': 'active'}))\n\n# Key access\nstmt = select(Event).where(Event.metadata['status'].astext == 'active')\n\n# Type cast for comparison\nstmt = select(Event).where(\n    Event.metadata['priority'].as_integer() > 3\n)\n```\n\nRule: GIN for `@>`, `?`, `?|`, `?&` (containment/existence). Expression index for `->>'key'` equality. Never index the entire JSONB column with btree.",
    "tags": [
      "postgresql",
      "jsonb",
      "indexing",
      "gin-index"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis pub/sub for real-time notifications in FastAPI",
    "context": "Need to push real-time updates to connected clients when events occur (e.g., trace validated, vote received). Using Server-Sent Events (SSE) for the client side, need Redis pub/sub to fan out events from any worker to all API instances.",
    "solution": "Use Redis pub/sub with asyncio to stream events via SSE:\n\n```python\nimport asyncio\nimport json\nfrom fastapi import APIRouter, Depends\nfrom fastapi.responses import StreamingResponse\nfrom redis.asyncio import Redis\nfrom app.dependencies import get_redis\n\nrouter = APIRouter()\n\n# Publisher (call from anywhere in the app)\nasync def publish_event(redis: Redis, channel: str, data: dict) -> None:\n    await redis.publish(channel, json.dumps(data))\n\n# SSE endpoint\n@router.get('/events/{user_id}')\nasync def stream_events(\n    user_id: str,\n    redis: Redis = Depends(get_redis)\n) -> StreamingResponse:\n    async def event_generator():\n        pubsub = redis.pubsub()\n        await pubsub.subscribe(f'user:{user_id}')\n        try:\n            while True:\n                message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=1.0)\n                if message is not None:\n                    data = message['data']\n                    if isinstance(data, bytes):\n                        data = data.decode()\n                    yield f'data: {data}\\n\\n'\n                else:\n                    # Keepalive ping\n                    yield ': ping\\n\\n'\n                    await asyncio.sleep(15)\n        finally:\n            await pubsub.unsubscribe(f'user:{user_id}')\n            await pubsub.close()\n\n    return StreamingResponse(\n        event_generator(),\n        media_type='text/event-stream',\n        headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n    )\n\n# Usage: publish when a trace is validated\nasync def on_trace_validated(trace: Trace, redis: Redis) -> None:\n    await publish_event(redis, f'user:{trace.contributor_id}', {\n        'type': 'trace_validated',\n        'trace_id': str(trace.id),\n        'title': trace.title,\n    })\n```\n\nThe `X-Accel-Buffering: no` header disables nginx buffering so events reach the client immediately. Always handle client disconnects by catching `asyncio.CancelledError` in the generator.",
    "tags": [
      "redis",
      "pub-sub",
      "fastapi",
      "server-sent-events",
      "asyncio"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis sorted sets for leaderboard and ranking",
    "context": "Need a real-time leaderboard that updates as scores change. Querying PostgreSQL for top-N with ORDER BY is expensive at scale. Need O(log N) updates and O(log N + K) range queries.",
    "solution": "Use Redis sorted sets (`ZADD`/`ZRANGE`) for the leaderboard, sync from PostgreSQL periodically:\n\n```python\nimport json\nfrom redis.asyncio import Redis\n\nLEADERBOARD_KEY = 'leaderboard:contributors'\n\n# Update score\nasync def update_contributor_score(redis: Redis, user_id: str, score: float) -> None:\n    await redis.zadd(LEADERBOARD_KEY, {user_id: score})\n\n# Get top N with scores\nasync def get_top_contributors(redis: Redis, n: int = 10) -> list[dict]:\n    # ZRANGE with REV=True and WITHSCORES\n    entries = await redis.zrange(\n        LEADERBOARD_KEY, 0, n - 1,\n        rev=True, withscores=True\n    )\n    return [\n        {'user_id': member.decode(), 'score': score}\n        for member, score in entries\n    ]\n\n# Get rank of a specific user (0-indexed)\nasync def get_user_rank(redis: Redis, user_id: str) -> int | None:\n    rank = await redis.zrevrank(LEADERBOARD_KEY, user_id)\n    return rank  # None if not in leaderboard\n\n# Increment score atomically\nasync def increment_score(redis: Redis, user_id: str, delta: float) -> float:\n    new_score = await redis.zincrby(LEADERBOARD_KEY, delta, user_id)\n    return new_score\n\n# Sync from PostgreSQL (run on startup and periodically)\nasync def sync_leaderboard(redis: Redis, session: AsyncSession) -> None:\n    result = await session.execute(\n        select(User.id, User.reputation_score)\n        .where(User.reputation_score > 0)\n        .order_by(User.reputation_score.desc())\n        .limit(1000)\n    )\n    users = result.all()\n    if users:\n        await redis.zadd(\n            LEADERBOARD_KEY,\n            {str(user.id): user.reputation_score for user in users}\n        )\n```\n\n`ZADD` is O(log N), `ZRANGE` is O(log N + K). Use `ZINCRBY` for atomic score increments to avoid race conditions.",
    "tags": [
      "redis",
      "sorted-sets",
      "leaderboard",
      "ranking"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose environment variable files per environment",
    "context": "Managing different configurations for development, staging, and production in Docker Compose. Hardcoding environment variables in compose files leads to secrets in version control and different configs for each environment.",
    "solution": "Use `.env` files with Docker Compose's `env_file` directive and environment-specific override files:\n\n```\n# .env.development (committed)\nDATABASE_URL=postgresql+asyncpg://dev:dev@postgres:5432/devdb\nDEBUG=true\nLOG_LEVEL=debug\n\n# .env.production (NOT committed \u2014 use secrets manager)\nDATABASE_URL=postgresql+asyncpg://user:secret@prod-host:5432/proddb\nDEBUG=false\nLOG_LEVEL=info\n```\n\n```yaml\n# docker-compose.yml (base)\nservices:\n  api:\n    build: ./api\n    env_file:\n      - .env.${ENV:-development}\n    environment:\n      # Override specific vars (takes precedence over env_file)\n      APP_NAME: CommonTrace\n\n  # docker-compose.production.yml (extends base)\n  api:\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          memory: 512M\n```\n\n```bash\n# Run with specific environment\nENV=production docker-compose -f docker-compose.yml -f docker-compose.production.yml up -d\n\n# Or set in .env file at project root (auto-loaded by Compose)\necho 'ENV=production' > .env\ndocker-compose up -d\n```\n\n```gitignore\n# .gitignore\n.env\n.env.production\n.env.staging\n.env.*.local\n# Commit only:\n# .env.development\n# .env.example\n```\n\nPrecedence order (highest to lowest): `environment:` block in compose file > `env_file:` > shell environment variables when running compose. Always provide a `.env.example` with all keys but no values.",
    "tags": [
      "docker",
      "docker-compose",
      "environment-variables",
      "configuration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker multi-stage build for Python with uv",
    "context": "Python Docker images are large (1GB+) because they include build tools, pip cache, and development packages. Need a lean production image while keeping a full dev environment. Using uv for fast dependency management.",
    "solution": "Three-stage build: deps (build), dev (for local), prod (for deployment):\n\n```dockerfile\n# Dockerfile\nFROM python:3.12-slim AS base\nWORKDIR /app\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.5 /uv /uvx /usr/local/bin/\n\n# Dependencies stage (cached unless pyproject.toml changes)\nFROM base AS deps\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-install-project --no-dev\n\n# Development stage\nFROM deps AS dev\nRUN uv sync --frozen --no-install-project  # installs dev deps too\nCOPY . .\nCMD [\"uv\", \"run\", \"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--reload\"]\n\n# Production stage (lean)\nFROM base AS prod\n# Copy only the virtual environment (no build tools, no uv)\nCOPY --from=deps /app/.venv /app/.venv\nCOPY . .\n# Activate venv\nENV PATH=\"/app/.venv/bin:$PATH\"\n# Run as non-root\nRUN adduser --disabled-password --gecos '' appuser && chown -R appuser /app\nUSER appuser\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"2\"]\n```\n\n```bash\n# Build specific stage\ndocker build --target prod -t myapp:latest .\ndocker build --target dev -t myapp:dev .\n```\n\nThe production image excludes uv, build tools, and dev dependencies. Only the `.venv` directory is copied. Result: ~200MB vs ~1GB+ naive build. Key: `--no-dev` in deps stage, then copy `.venv` directly to prod.",
    "tags": [
      "docker",
      "multi-stage-build",
      "python",
      "uv"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Nginx rate limiting and caching configuration",
    "context": "FastAPI is running behind Nginx. Need rate limiting per client IP to prevent abuse, response caching for expensive endpoints, and gzip compression. Currently serving all traffic directly without any of these.",
    "solution": "Configure Nginx with rate limiting zones, proxy cache, and gzip:\n\n```nginx\n# /etc/nginx/nginx.conf or /etc/nginx/conf.d/default.conf\n\n# Rate limiting zones (define at http level)\nlimit_req_zone $binary_remote_addr zone=api_limit:10m rate=60r/m;\nlimit_req_zone $binary_remote_addr zone=write_limit:10m rate=20r/m;\n\n# Cache zone\nproxy_cache_path /tmp/nginx_cache levels=1:2 keys_zone=api_cache:10m max_size=100m inactive=60m;\n\nserver {\n    listen 80;\n    server_name api.example.com;\n\n    gzip on;\n    gzip_types application/json text/plain;\n    gzip_min_length 1024;\n\n    # Apply rate limit to all routes\n    limit_req zone=api_limit burst=20 nodelay;\n\n    location /api/v1/traces/search {\n        # Stricter limit for expensive search endpoint\n        limit_req zone=api_limit burst=10 nodelay;\n        limit_req_status 429;\n\n        # Cache GET search responses for 30 seconds\n        proxy_cache api_cache;\n        proxy_cache_key \"$request_uri$http_x_api_key\";\n        proxy_cache_valid 200 30s;\n        proxy_cache_bypass $http_pragma;\n        add_header X-Cache-Status $upstream_cache_status;\n\n        proxy_pass http://api:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n\n    location /api/v1/traces {\n        # Stricter write limit\n        limit_req zone=write_limit burst=5 nodelay;\n        proxy_pass http://api:8000;\n    }\n\n    location / {\n        proxy_pass http://api:8000;\n        proxy_read_timeout 30s;\n    }\n}\n```\n\nKey: `burst` allows queuing up to N requests above the rate. `nodelay` processes burst immediately (no queue delay) but counts against limit. `$binary_remote_addr` uses 4 bytes vs 15 for the string form \u2014 more efficient for large zones.",
    "tags": [
      "nginx",
      "rate-limiting",
      "caching",
      "docker"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker volume management and data persistence patterns",
    "context": "Docker containers are ephemeral \u2014 data stored inside the container is lost on restart. Need to persist PostgreSQL data, handle Redis persistence, and share files between containers. Confusion between named volumes and bind mounts.",
    "solution": "Use named volumes for databases (Docker manages location), bind mounts for development code:\n\n```yaml\nversion: '3.9'\n\nservices:\n  postgres:\n    image: pgvector/pgvector:pg17\n    volumes:\n      # Named volume \u2014 Docker manages the path, persists across container recreates\n      - postgres_data:/var/lib/postgresql/data\n      # Bind mount for init scripts\n      - ./migrations/init.sql:/docker-entrypoint-initdb.d/init.sql:ro\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: myuser\n      POSTGRES_PASSWORD: mypassword\n\n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n    # Enable AOF persistence\n    command: redis-server --appendonly yes --appendfsync everysec\n\n  api:\n    build: ./api\n    volumes:\n      # Bind mount for hot reload in development\n      - ./api:/app:cached\n      # Named volume for compiled .pyc files (faster than bind mount)\n      - api_pycache:/app/__pycache__\n\nvolumes:\n  postgres_data:\n    # Optional: use external volume managed outside compose\n    # external: true\n  redis_data:\n  api_pycache:\n```\n\n```bash\n# List volumes\ndocker volume ls\n\n# Inspect volume location\ndocker volume inspect myapp_postgres_data\n\n# Remove volumes on teardown (destructive!)\ndocker-compose down -v\n\n# Backup named volume\ndocker run --rm -v myapp_postgres_data:/data -v $(pwd):/backup \\\n  alpine tar czf /backup/postgres_backup.tar.gz /data\n```\n\nNamed volumes survive `docker-compose down` but NOT `docker-compose down -v`. Bind mounts reflect host changes immediately \u2014 ideal for dev. Always use named volumes in production.",
    "tags": [
      "docker",
      "volumes",
      "docker-compose",
      "persistence"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React useReducer for complex state management",
    "context": "Component state has grown complex with multiple related fields that change together. useState is getting unwieldy with many separate setters. Need predictable state transitions and easier debugging.",
    "solution": "Replace multiple useState calls with useReducer for related state:\n\n```typescript\ntype SearchState = {\n  query: string;\n  results: Trace[];\n  status: 'idle' | 'loading' | 'success' | 'error';\n  error: string | null;\n  page: number;\n};\n\ntype SearchAction =\n  | { type: 'SET_QUERY'; payload: string }\n  | { type: 'SEARCH_START' }\n  | { type: 'SEARCH_SUCCESS'; payload: { results: Trace[]; page: number } }\n  | { type: 'SEARCH_ERROR'; payload: string }\n  | { type: 'RESET' };\n\nconst initialState: SearchState = {\n  query: '',\n  results: [],\n  status: 'idle',\n  error: null,\n  page: 1,\n};\n\nfunction searchReducer(state: SearchState, action: SearchAction): SearchState {\n  switch (action.type) {\n    case 'SET_QUERY':\n      return { ...state, query: action.payload };\n    case 'SEARCH_START':\n      return { ...state, status: 'loading', error: null };\n    case 'SEARCH_SUCCESS':\n      return { ...state, status: 'success', results: action.payload.results, page: action.payload.page };\n    case 'SEARCH_ERROR':\n      return { ...state, status: 'error', error: action.payload, results: [] };\n    case 'RESET':\n      return initialState;\n    default:\n      return state;\n  }\n}\n\nfunction SearchComponent() {\n  const [state, dispatch] = useReducer(searchReducer, initialState);\n\n  const handleSearch = async () => {\n    dispatch({ type: 'SEARCH_START' });\n    try {\n      const results = await searchTraces(state.query);\n      dispatch({ type: 'SEARCH_SUCCESS', payload: { results, page: 1 } });\n    } catch (err) {\n      dispatch({ type: 'SEARCH_ERROR', payload: err instanceof Error ? err.message : 'Search failed' });\n    }\n  };\n\n  return (\n    <div>\n      <input value={state.query} onChange={e => dispatch({ type: 'SET_QUERY', payload: e.target.value })} />\n      {state.status === 'loading' && <Spinner />}\n      {state.status === 'error' && <Error message={state.error} />}\n      {state.results.map(trace => <TraceCard key={trace.id} trace={trace} />)}\n    </div>\n  );\n}\n```\n\nUse `useReducer` when: state has multiple sub-values that change together, next state depends on previous state, or when state transitions need to be explicit and testable. Use `useState` for independent simple values.",
    "tags": [
      "react",
      "usereducer",
      "typescript",
      "state-management"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript generics for reusable API hooks",
    "context": "Duplicating data-fetching logic across components \u2014 each API call has its own loading/error/data state management. Need a generic, reusable data-fetching hook that works with any endpoint and return type.",
    "solution": "Build a generic `useApi` hook with TypeScript generics:\n\n```typescript\nimport { useState, useEffect, useCallback } from 'react';\n\ntype ApiState<T> = {\n  data: T | null;\n  loading: boolean;\n  error: Error | null;\n  refetch: () => void;\n};\n\nfunction useApi<T>(fetchFn: () => Promise<T>, deps: React.DependencyList = []): ApiState<T> {\n  const [data, setData] = useState<T | null>(null);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState<Error | null>(null);\n  const [version, setVersion] = useState(0);\n\n  useEffect(() => {\n    let cancelled = false;\n    setLoading(true);\n    setError(null);\n\n    fetchFn()\n      .then(result => { if (!cancelled) { setData(result); setLoading(false); } })\n      .catch(err => { if (!cancelled) { setError(err); setLoading(false); } });\n\n    return () => { cancelled = true; };\n  }, [version, ...deps]);\n\n  const refetch = useCallback(() => setVersion(v => v + 1), []);\n\n  return { data, loading, error, refetch };\n}\n\n// Paginated variant\nfunction usePaginatedApi<T>(fetchFn: (page: number) => Promise<{ items: T[]; total: number }>) {\n  const [page, setPage] = useState(1);\n  const { data, loading, error, refetch } = useApi(() => fetchFn(page), [page]);\n\n  return {\n    items: data?.items ?? [],\n    total: data?.total ?? 0,\n    page,\n    loading,\n    error,\n    nextPage: () => setPage(p => p + 1),\n    prevPage: () => setPage(p => Math.max(1, p - 1)),\n    refetch,\n  };\n}\n\n// Usage\nfunction TraceList() {\n  const { items: traces, loading, error, nextPage } = usePaginatedApi(\n    (page) => api.getTraces({ page, limit: 20 })\n  );\n\n  if (loading) return <Spinner />;\n  if (error) return <Error message={error.message} />;\n  return <div>{traces.map(t => <TraceCard key={t.id} trace={t} />)}</div>;\n}\n```\n\nThe cancellation flag (`let cancelled = false`) prevents state updates on unmounted components. `version` state enables manual refetching without changing deps.",
    "tags": [
      "typescript",
      "react",
      "generics",
      "hooks",
      "data-fetching"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Next.js server actions for form mutations",
    "context": "Using Next.js App Router. Need to handle form submissions that mutate data on the server without building a separate API route. Want type-safe server-side form handling with optimistic updates.",
    "solution": "Use Next.js Server Actions with `useActionState` and optimistic updates:\n\n```typescript\n// app/traces/actions.ts\n'use server'\n\nimport { revalidatePath } from 'next/cache';\nimport { redirect } from 'next/navigation';\n\nexport type ActionState = {\n  success: boolean;\n  error?: string;\n  traceId?: string;\n};\n\nexport async function createTrace(\n  prevState: ActionState,\n  formData: FormData\n): Promise<ActionState> {\n  const title = formData.get('title') as string;\n  const context = formData.get('context') as string;\n  const solution = formData.get('solution') as string;\n  const tags = (formData.get('tags') as string).split(',').map(t => t.trim());\n\n  if (!title || title.length < 10) {\n    return { success: false, error: 'Title must be at least 10 characters' };\n  }\n\n  try {\n    const response = await fetch(`${process.env.API_URL}/api/v1/traces`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json', 'X-API-Key': process.env.API_KEY! },\n      body: JSON.stringify({ title, context, solution, tags }),\n    });\n\n    if (!response.ok) {\n      const error = await response.json();\n      return { success: false, error: error.detail };\n    }\n\n    const trace = await response.json();\n    revalidatePath('/traces');\n    return { success: true, traceId: trace.id };\n  } catch (err) {\n    return { success: false, error: 'Failed to create trace' };\n  }\n}\n\n// app/traces/new/page.tsx\n'use client'\n\nimport { useActionState } from 'react';\nimport { createTrace } from '../actions';\n\nexport default function NewTracePage() {\n  const [state, formAction, isPending] = useActionState(createTrace, { success: false });\n\n  return (\n    <form action={formAction}>\n      <input name=\"title\" required />\n      <textarea name=\"context\" required />\n      <textarea name=\"solution\" required />\n      <input name=\"tags\" placeholder=\"react, hooks, typescript\" />\n      {state.error && <p className=\"error\">{state.error}</p>}\n      <button type=\"submit\" disabled={isPending}>\n        {isPending ? 'Submitting...' : 'Create Trace'}\n      </button>\n    </form>\n  );\n}\n```\n\nServer Actions run exclusively on the server \u2014 no API route needed. `revalidatePath` invalidates cached data for that route. Works without JavaScript enabled (progressive enhancement).",
    "tags": [
      "nextjs",
      "server-actions",
      "react",
      "forms"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript discriminated unions for API response typing",
    "context": "API calls return different shapes depending on success or failure. Using a generic `{ data: T | null, error: string | null }` pattern leads to redundant null checks everywhere and TypeScript can't narrow the type properly.",
    "solution": "Use discriminated unions to make success/failure states mutually exclusive:\n\n```typescript\n// Define the union\ntype ApiResult<T> =\n  | { ok: true; data: T; error?: never }\n  | { ok: false; data?: never; error: string; status: number };\n\n// Typed fetch wrapper\nasync function apiFetch<T>(url: string, options?: RequestInit): Promise<ApiResult<T>> {\n  try {\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      const body = await response.json().catch(() => ({ detail: 'Unknown error' }));\n      return { ok: false, error: body.detail ?? 'Request failed', status: response.status };\n    }\n    const data = await response.json() as T;\n    return { ok: true, data };\n  } catch (err) {\n    return { ok: false, error: err instanceof Error ? err.message : 'Network error', status: 0 };\n  }\n}\n\n// Usage \u2014 TypeScript narrows correctly\nasync function getTrace(id: string) {\n  const result = await apiFetch<Trace>(`/api/v1/traces/${id}`);\n\n  if (!result.ok) {\n    console.error(`Error ${result.status}: ${result.error}`);\n    // result.data is never here \u2014 TypeScript prevents access\n    return null;\n  }\n\n  // result.data is Trace here \u2014 no null check needed\n  return result.data;\n}\n\n// Pattern with exhaustive checking\nfunction handleResult<T>(result: ApiResult<T>): T | null {\n  if (result.ok) return result.data;\n  if (result.status === 401) redirect('/login');\n  if (result.status === 404) return null;\n  throw new Error(result.error);\n}\n\n// For loading states, add a third variant\ntype LoadingResult<T> =\n  | { status: 'loading' }\n  | { status: 'success'; data: T }\n  | { status: 'error'; error: string };\n```\n\nThe `error?: never` syntax prevents setting `error` on the success branch (and vice versa). TypeScript's control flow analysis narrows the type after `if (result.ok)` checks.",
    "tags": [
      "typescript",
      "discriminated-unions",
      "api",
      "error-handling"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React context with useReducer for global state",
    "context": "Multiple deeply nested components need access to the same state (current user, theme, feature flags). Prop drilling has become unmanageable. Want to avoid Redux for a smaller app but need predictable state management.",
    "solution": "Combine React Context with useReducer for lightweight global state:\n\n```typescript\n// contexts/AppContext.tsx\nimport { createContext, useContext, useReducer, ReactNode } from 'react';\n\ntype User = { id: string; email: string; displayName: string };\n\ntype AppState = {\n  user: User | null;\n  theme: 'light' | 'dark';\n  apiKey: string | null;\n};\n\ntype AppAction =\n  | { type: 'SET_USER'; payload: User }\n  | { type: 'CLEAR_USER' }\n  | { type: 'TOGGLE_THEME' }\n  | { type: 'SET_API_KEY'; payload: string };\n\nfunction appReducer(state: AppState, action: AppAction): AppState {\n  switch (action.type) {\n    case 'SET_USER': return { ...state, user: action.payload };\n    case 'CLEAR_USER': return { ...state, user: null, apiKey: null };\n    case 'TOGGLE_THEME': return { ...state, theme: state.theme === 'light' ? 'dark' : 'light' };\n    case 'SET_API_KEY': return { ...state, apiKey: action.payload };\n    default: return state;\n  }\n}\n\nconst AppContext = createContext<{ state: AppState; dispatch: React.Dispatch<AppAction> } | null>(null);\n\nexport function AppProvider({ children }: { children: ReactNode }) {\n  const [state, dispatch] = useReducer(appReducer, {\n    user: null,\n    theme: 'light',\n    apiKey: null,\n  });\n\n  return (\n    <AppContext.Provider value={{ state, dispatch }}>\n      {children}\n    </AppContext.Provider>\n  );\n}\n\nexport function useApp() {\n  const context = useContext(AppContext);\n  if (!context) throw new Error('useApp must be used within AppProvider');\n  return context;\n}\n\n// Usage\nfunction UserMenu() {\n  const { state, dispatch } = useApp();\n\n  if (!state.user) return <LoginButton />;\n  return (\n    <div>\n      <span>{state.user.displayName}</span>\n      <button onClick={() => dispatch({ type: 'CLEAR_USER' })}>Logout</button>\n    </div>\n  );\n}\n```\n\nSplit contexts for performance: if `theme` and `user` change at different rates, put them in separate contexts so theme changes don't re-render user-dependent components.",
    "tags": [
      "react",
      "context",
      "usereducer",
      "typescript",
      "state-management"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Node.js readable streams for large file processing",
    "context": "Processing a large CSV or JSON file (hundreds of MB) by reading the entire file into memory causes out-of-memory errors. Need to process records one at a time as they're read from disk.",
    "solution": "Use Node.js streams with the pipeline API for backpressure-safe processing:\n\n```typescript\nimport { createReadStream } from 'fs';\nimport { createInterface } from 'readline';\nimport { pipeline } from 'stream/promises';\nimport { Transform } from 'stream';\n\n// Process CSV line by line (memory-efficient)\nasync function processCsvFile(filePath: string): Promise<void> {\n  const fileStream = createReadStream(filePath, { encoding: 'utf8' });\n  const rl = createInterface({ input: fileStream, crlfDelay: Infinity });\n\n  let lineNumber = 0;\n  const errors: string[] = [];\n\n  for await (const line of rl) {\n    lineNumber++;\n    if (lineNumber === 1) continue; // skip header\n\n    try {\n      const fields = line.split(',');\n      await processRecord({ id: fields[0], name: fields[1], value: Number(fields[2]) });\n    } catch (err) {\n      errors.push(`Line ${lineNumber}: ${err instanceof Error ? err.message : 'unknown error'}`);\n    }\n  }\n\n  console.log(`Processed ${lineNumber - 1} records, ${errors.length} errors`);\n}\n\n// Transform stream for JSON Lines format (one JSON object per line)\nclass JsonLineParser extends Transform {\n  constructor() { super({ objectMode: true }); }\n\n  _transform(chunk: Buffer, _encoding: string, callback: () => void) {\n    const lines = chunk.toString().split('\\n').filter(l => l.trim());\n    for (const line of lines) {\n      try { this.push(JSON.parse(line)); } catch { /* skip invalid */ }\n    }\n    callback();\n  }\n}\n\n// Batch records before inserting into DB\nclass BatchProcessor extends Transform {\n  private batch: object[] = [];\n  private readonly batchSize = 100;\n\n  constructor() { super({ objectMode: true }); }\n\n  async _transform(record: object, _encoding: string, callback: () => void) {\n    this.batch.push(record);\n    if (this.batch.length >= this.batchSize) {\n      await this.flushBatch();\n    }\n    callback();\n  }\n\n  async _flush(callback: () => void) {\n    await this.flushBatch();\n    callback();\n  }\n\n  private async flushBatch() {\n    if (this.batch.length > 0) {\n      await db.bulkInsert(this.batch);\n      this.batch = [];\n    }\n  }\n}\n```\n\nThe `for await...of` loop on a readline interface handles backpressure automatically. Use `pipeline()` from `stream/promises` for proper error propagation and cleanup.",
    "tags": [
      "nodejs",
      "streams",
      "csv",
      "memory-efficiency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions workflow for Docker image build and push",
    "context": "Need to build a Docker image and push it to GitHub Container Registry (GHCR) on every push to main, with proper tagging (latest, sha, and version tags). Want to avoid rebuilding unchanged layers.",
    "solution": "Use the official Docker GitHub Actions with layer caching:\n\n```yaml\n# .github/workflows/docker.yml\nname: Build and Push Docker Image\n\non:\n  push:\n    branches: [main]\n    tags: ['v*.*.*']\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Log in to GHCR\n        uses: docker/login-action@v3\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Extract metadata\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n          tags: |\n            type=ref,event=branch\n            type=semver,pattern={{version}}\n            type=semver,pattern={{major}}.{{minor}}\n            type=sha,prefix=sha-,format=short\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          file: ./api/Dockerfile\n          target: prod\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n          build-args: |\n            BUILD_VERSION=${{ github.ref_name }}\n```\n\n`type=gha` caching stores Docker layers in GitHub Actions cache \u2014 each layer is reused if unchanged. `docker/metadata-action` generates tags based on git refs. `GITHUB_TOKEN` has write permission to GHCR packages automatically.",
    "tags": [
      "github-actions",
      "docker",
      "ghcr",
      "ci-cd"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions reusable workflows",
    "context": "Multiple repositories have duplicate CI/CD logic (lint, test, build). Updating the workflow in each repo separately is error-prone. Need a single source of truth for shared CI steps.",
    "solution": "Create a reusable workflow in a central repo and call it from others:\n\n```yaml\n# .github/workflows/reusable-python-ci.yml (in shared-workflows repo)\nname: Python CI\n\non:\n  workflow_call:\n    inputs:\n      python-version:\n        description: 'Python version to use'\n        type: string\n        default: '3.12'\n      working-directory:\n        type: string\n        default: '.'\n    secrets:\n      CODECOV_TOKEN:\n        required: false\n\njobs:\n  lint-and-test:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ${{ inputs.working-directory }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v3\n        with:\n          python-version: ${{ inputs.python-version }}\n\n      - name: Install dependencies\n        run: uv sync --frozen\n\n      - name: Lint\n        run: uv run ruff check . && uv run ruff format --check .\n\n      - name: Test\n        run: uv run pytest --cov --cov-report=xml\n\n      - name: Upload coverage\n        if: ${{ secrets.CODECOV_TOKEN != '' }}\n        uses: codecov/codecov-action@v4\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n```\n\n```yaml\n# .github/workflows/ci.yml (in consumer repo)\nname: CI\n\non: [push, pull_request]\n\njobs:\n  python-ci:\n    uses: my-org/shared-workflows/.github/workflows/reusable-python-ci.yml@main\n    with:\n      python-version: '3.12'\n      working-directory: 'api'\n    secrets:\n      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n```\n\nReusable workflows use `workflow_call` trigger. Inputs are typed (string, boolean, number). Secrets are passed explicitly \u2014 they're not inherited automatically. Reference with `{owner}/{repo}/.github/workflows/{file}@{ref}`.",
    "tags": [
      "github-actions",
      "reusable-workflows",
      "ci-cd"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions deployment with manual approval gate",
    "context": "CI pipeline runs tests automatically, but production deployment should require manual approval. Want automated staging deploy on merge to main, but production needs a human sign-off before deploying.",
    "solution": "Use GitHub Environments with required reviewers for the production approval gate:\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy\n\non:\n  push:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run tests\n        run: make test\n\n  deploy-staging:\n    needs: test\n    runs-on: ubuntu-latest\n    environment:\n      name: staging\n      url: https://staging.example.com\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to staging\n        run: ./scripts/deploy.sh staging\n        env:\n          DEPLOY_KEY: ${{ secrets.STAGING_DEPLOY_KEY }}\n\n  deploy-production:\n    needs: deploy-staging\n    runs-on: ubuntu-latest\n    environment:\n      name: production  # This environment has required reviewers configured\n      url: https://example.com\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to production\n        run: ./scripts/deploy.sh production\n        env:\n          DEPLOY_KEY: ${{ secrets.PROD_DEPLOY_KEY }}\n```\n\nConfigure in GitHub Settings > Environments > production:\n- Enable 'Required reviewers' and add team members\n- Set 'Wait timer' (optional delay before approval is possible)\n- Set 'Deployment branches' to `main` only\n- Add environment-specific secrets (PROD_DEPLOY_KEY)\n\nThe `production` job pauses until a reviewer approves in the GitHub UI. The URL is shown on the environment page. Secrets are scoped to the environment \u2014 staging secrets can't access production secrets.",
    "tags": [
      "github-actions",
      "deployment",
      "environments",
      "approval-gate"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "OpenAI streaming chat completions in Python",
    "context": "Using OpenAI chat completions but responses have high latency before any text appears. Need to stream the response token-by-token so users see text as it's generated, rather than waiting for the full response.",
    "solution": "Use the streaming API with async generators in FastAPI:\n\n```python\nfrom openai import AsyncOpenAI\nfrom fastapi import APIRouter\nfrom fastapi.responses import StreamingResponse\n\nrouter = APIRouter()\nclient = AsyncOpenAI()\n\n@router.post('/chat/stream')\nasync def stream_chat(request: ChatRequest) -> StreamingResponse:\n    async def generate():\n        stream = await client.chat.completions.create(\n            model='gpt-4o-mini',\n            messages=request.messages,\n            stream=True,\n        )\n        async for chunk in stream:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                # Server-Sent Events format\n                yield f'data: {json.dumps({\"content\": delta.content})}\\n\\n'\n            if chunk.choices[0].finish_reason == 'stop':\n                yield 'data: [DONE]\\n\\n'\n\n    return StreamingResponse(\n        generate(),\n        media_type='text/event-stream',\n        headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'},\n    )\n\n# Client-side consumption (Next.js)\nasync function streamChat(messages: Message[]) {\n  const response = await fetch('/chat/stream', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ messages }),\n  });\n\n  const reader = response.body!.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const text = decoder.decode(value);\n    const lines = text.split('\\n').filter(l => l.startsWith('data: '));\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') break;\n      const parsed = JSON.parse(data);\n      onChunk(parsed.content); // update UI\n    }\n  }\n}\n```\n\nKey: `stream=True` returns an async iterable. The `X-Accel-Buffering: no` header prevents nginx from buffering the stream. Always handle `[DONE]` sentinel to know when streaming is complete.",
    "tags": [
      "openai",
      "streaming",
      "fastapi",
      "asyncio"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Stripe webhook verification and idempotent event handling",
    "context": "Implementing Stripe webhooks. Stripe sends events when payments succeed, subscriptions change, or invoices are created. Need to verify webhook signatures to prevent spoofed events and handle retries idempotently.",
    "solution": "Verify the Stripe signature header and use event IDs for idempotency:\n\n```python\nimport stripe\nfrom fastapi import APIRouter, Request, HTTPException\nfrom app.models.payment import ProcessedEvent\nfrom sqlalchemy.exc import IntegrityError\n\nrouter = APIRouter()\nstripe.api_key = settings.stripe_secret_key\n\n@router.post('/webhooks/stripe')\nasync def stripe_webhook(\n    request: Request,\n    session: AsyncSession = Depends(get_db),\n) -> dict:\n    payload = await request.body()\n    sig_header = request.headers.get('stripe-signature')\n\n    try:\n        event = stripe.Webhook.construct_event(\n            payload, sig_header, settings.stripe_webhook_secret\n        )\n    except ValueError:\n        raise HTTPException(status_code=400, detail='Invalid payload')\n    except stripe.error.SignatureVerificationError:\n        raise HTTPException(status_code=400, detail='Invalid signature')\n\n    # Idempotency: skip already-processed events\n    try:\n        await session.execute(\n            insert(ProcessedEvent).values(stripe_event_id=event['id'])\n        )\n        await session.flush()\n    except IntegrityError:\n        # Already processed \u2014 return 200 so Stripe stops retrying\n        return {'status': 'already_processed'}\n\n    # Handle event types\n    match event['type']:\n        case 'checkout.session.completed':\n            await handle_checkout_completed(event['data']['object'], session)\n        case 'customer.subscription.deleted':\n            await handle_subscription_cancelled(event['data']['object'], session)\n        case 'invoice.payment_failed':\n            await handle_payment_failed(event['data']['object'], session)\n        case _:\n            pass  # Ignore unhandled event types\n\n    await session.commit()\n    return {'status': 'ok'}\n```\n\nCritical: Always return 200 for events you've already processed. Stripe retries on non-2xx responses. The `processed_events` table with a unique constraint on `stripe_event_id` prevents duplicate processing. Test locally with `stripe listen --forward-to localhost:8000/webhooks/stripe`.",
    "tags": [
      "stripe",
      "webhooks",
      "fastapi",
      "payments",
      "idempotency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Resend email API with HTML templates in Python",
    "context": "Need to send transactional emails (welcome emails, password resets, notifications) from a FastAPI application. Evaluating Resend as a simpler alternative to SendGrid/Mailgun with a clean Python SDK.",
    "solution": "Use the Resend Python SDK with Jinja2 HTML templates:\n\n```python\n# app/services/email.py\nimport resend\nfrom jinja2 import Environment, PackageLoader, select_autoescape\nfrom app.config import settings\n\nresend.api_key = settings.resend_api_key\n\njinja_env = Environment(\n    loader=PackageLoader('app', 'templates/email'),\n    autoescape=select_autoescape(['html'])\n)\n\nasync def send_welcome_email(to_email: str, display_name: str) -> None:\n    template = jinja_env.get_template('welcome.html')\n    html = template.render(display_name=display_name, app_url=settings.app_url)\n\n    resend.Emails.send({\n        'from': 'CommonTrace <noreply@commontrace.dev>',\n        'to': [to_email],\n        'subject': f'Welcome to CommonTrace, {display_name}!',\n        'html': html,\n    })\n\nasync def send_trace_validated_email(to_email: str, trace_title: str, trace_url: str) -> None:\n    resend.Emails.send({\n        'from': 'CommonTrace <noreply@commontrace.dev>',\n        'to': [to_email],\n        'subject': 'Your trace has been validated!',\n        'html': f\"\"\"\n            <h2>Great news!</h2>\n            <p>Your trace <strong>{trace_title}</strong> has been validated by the community.</p>\n            <p><a href=\"{trace_url}\">View your trace</a></p>\n        \"\"\",\n    })\n\n# app/templates/email/welcome.html\n# <!DOCTYPE html>\n# <html>\n# <body>\n#   <h1>Welcome, {{ display_name }}!</h1>\n#   <p>Start contributing: <a href=\"{{ app_url }}/traces/new\">Submit a trace</a></p>\n# </body>\n# </html>\n```\n\nResend supports batch sending (`resend.Emails.send_batch()`), email scheduling, and has a React Email integration for component-based templates. Free tier: 100 emails/day, 3,000/month.",
    "tags": [
      "resend",
      "email",
      "fastapi",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest parametrize with complex test cases",
    "context": "Writing tests for a function that should behave differently based on many input combinations. Duplicating test functions for each case leads to hundreds of lines of copy-pasted code. Need a clean way to test all edge cases.",
    "solution": "Use `@pytest.mark.parametrize` with IDs and indirect fixtures:\n\n```python\nimport pytest\nfrom app.services.tags import normalize_tag, validate_tag\nfrom app.services.scoring import wilson_score\n\n# Basic parametrize\n@pytest.mark.parametrize('raw_input,expected', [\n    ('Python', 'python'),\n    ('  react hooks  ', 'react hooks'),\n    ('Node.JS', 'node.js'),\n    ('my_tag', 'my_tag'),\n    ('a' * 60, 'a' * 50),  # truncation\n])\ndef test_normalize_tag(raw_input: str, expected: str) -> None:\n    assert normalize_tag(raw_input) == expected\n\n# With explicit IDs for readable test names\n@pytest.mark.parametrize('tag,is_valid', [\n    pytest.param('python', True, id='valid-simple'),\n    pytest.param('my-tag', True, id='valid-hyphenated'),\n    pytest.param('tag.v2', True, id='valid-dot'),\n    pytest.param('', False, id='invalid-empty'),\n    pytest.param('UPPER', False, id='invalid-uppercase'),\n    pytest.param('has space', False, id='invalid-space'),\n    pytest.param('a' * 51, False, id='invalid-too-long'),\n])\ndef test_validate_tag(tag: str, is_valid: bool) -> None:\n    assert validate_tag(tag) == is_valid\n\n# Parametrize with multiple arguments and marks\n@pytest.mark.parametrize('upvotes,total,expected_range', [\n    pytest.param(0, 0, (0.0, 0.0), id='no-votes'),\n    pytest.param(1, 1, (0.0, 1.0), id='one-vote-up'),\n    pytest.param(10, 10, (0.7, 1.0), id='all-upvotes'),\n    pytest.param(5, 10, (0.2, 0.8), id='half-upvotes'),\n    pytest.param(0, 10, (0.0, 0.3), id='all-downvotes'),\n])\ndef test_wilson_score(upvotes: int, total: int, expected_range: tuple[float, float]) -> None:\n    score = wilson_score(upvotes, total)\n    lo, hi = expected_range\n    assert lo <= score <= hi, f'Expected {lo}..{hi}, got {score}'\n\n# Parametrize class-based tests\n@pytest.mark.parametrize('status_code,expected_exception', [\n    (400, ValueError),\n    (401, PermissionError),\n    (404, LookupError),\n    (500, RuntimeError),\n])\nclass TestErrorHandling:\n    def test_raises_correct_exception(self, status_code, expected_exception):\n        with pytest.raises(expected_exception):\n            raise_for_status(status_code)\n```\n\nParametrize IDs appear in test names: `test_validate_tag[valid-simple]`. Use `pytest.param(..., marks=pytest.mark.skip)` to skip specific cases. Stack multiple `@parametrize` decorators for cartesian product.",
    "tags": [
      "pytest",
      "parametrize",
      "testing",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI integration testing with httpx.AsyncClient",
    "context": "Unit tests mock too much and don't catch integration bugs between routes, middleware, and database. Need integration tests that test the full HTTP stack (auth, rate limiting, actual DB queries) without a running server.",
    "solution": "Use `httpx.AsyncClient` with the FastAPI `app` directly and a real test database:\n\n```python\n# tests/conftest.py\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\nfrom app.main import app\nfrom app.dependencies import get_db\nfrom app.models.base import Base\n\nTEST_DATABASE_URL = 'postgresql+asyncpg://test:test@localhost:5432/test_commontrace'\n\n@pytest_asyncio.fixture(scope='session')\nasync def engine():\n    eng = create_async_engine(TEST_DATABASE_URL)\n    async with eng.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield eng\n    async with eng.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    await eng.dispose()\n\n@pytest_asyncio.fixture\nasync def session(engine):\n    factory = async_sessionmaker(engine, expire_on_commit=False)\n    async with factory() as s:\n        yield s\n        await s.rollback()  # Rollback after each test\n\n@pytest_asyncio.fixture\nasync def client(session):\n    # Override the database dependency\n    app.dependency_overrides[get_db] = lambda: session\n    async with AsyncClient(\n        transport=ASGITransport(app=app),\n        base_url='http://test',\n    ) as c:\n        yield c\n    app.dependency_overrides.clear()\n\n# tests/test_traces.py\n@pytest_asyncio.fixture\nasync def api_key_header(session):\n    user = User(email='test@example.com', api_key_hash=hash_key('test-key-123'))\n    session.add(user)\n    await session.commit()\n    return {'X-API-Key': 'test-key-123'}\n\nasync def test_create_trace(client, api_key_header):\n    response = await client.post('/api/v1/traces', headers=api_key_header, json={\n        'title': 'Test trace title here',\n        'context': 'Testing context for the trace',\n        'solution': 'The solution code here',\n        'tags': ['python', 'testing'],\n    })\n    assert response.status_code == 201\n    data = response.json()\n    assert data['status'] == 'pending'\n    assert data['trust_score'] == 0.0\n```\n\nThe rollback pattern isolates each test without dropping tables. `ASGITransport` runs the full middleware stack. Override `get_db` to inject the test session into the app.",
    "tags": [
      "fastapi",
      "testing",
      "httpx",
      "integration-tests",
      "pytest"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Mocking external HTTP calls with respx",
    "context": "Tests call external APIs (OpenAI, Stripe, GitHub). Real API calls make tests slow, expensive, and flaky. Need to intercept HTTP calls at the transport layer so the application code doesn't need to change.",
    "solution": "Use `respx` to mock `httpx` requests at the transport level:\n\n```python\nimport respx\nimport pytest\nimport httpx\nfrom unittest.mock import patch\n\n# Basic mock with respx\n@pytest.mark.asyncio\nasync def test_embedding_service():\n    with respx.mock() as mock:\n        mock.post('https://api.openai.com/v1/embeddings').mock(\n            return_value=httpx.Response(200, json={\n                'data': [{'embedding': [0.1] * 1536, 'index': 0}],\n                'model': 'text-embedding-3-small',\n                'usage': {'prompt_tokens': 8, 'total_tokens': 8},\n            })\n        )\n\n        result = await embed_text('test query')\n        assert len(result) == 1536\n        assert mock.called\n\n# Mock with pattern matching\n@pytest.mark.asyncio\nasync def test_github_api():\n    with respx.mock() as mock:\n        mock.get('https://api.github.com/user').mock(\n            return_value=httpx.Response(200, json={'login': 'testuser', 'id': 12345})\n        )\n        mock.get(respx.pattern.M('https://api.github.com/repos/**')).mock(\n            return_value=httpx.Response(200, json={'stargazers_count': 100})\n        )\n\n        user = await get_github_user(token='test-token')\n        assert user.login == 'testuser'\n\n# Fixture for reuse\n@pytest.fixture\ndef mock_openai():\n    with respx.mock() as mock:\n        mock.post('https://api.openai.com/v1/embeddings').mock(\n            return_value=httpx.Response(200, json={\n                'data': [{'embedding': [0.0] * 1536}]\n            })\n        )\n        yield mock\n\nasync def test_with_fixture(mock_openai):\n    result = await embed_text('hello')\n    assert result is not None\n    assert mock_openai.calls.last.request.url.path == '/v1/embeddings'\n```\n\n`respx.mock()` intercepts all httpx requests in the context. Use `respx.mock(assert_all_called=True)` to ensure all mocked routes were called. Works with both sync and async httpx clients.",
    "tags": [
      "testing",
      "mocking",
      "httpx",
      "respx",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest async fixtures with proper scope",
    "context": "Async pytest fixtures are slow because they recreate expensive resources (database connections, HTTP clients) for each test. Need to understand fixture scoping in async contexts and how to share resources safely.",
    "solution": "Use `pytest_asyncio.fixture` with appropriate scope levels:\n\n```python\n# conftest.py\nimport pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\n\n# Session scope: created once for entire test run\n@pytest_asyncio.fixture(scope='session')\nasync def db_engine():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    await engine.dispose()\n\n# Module scope: once per test file\n@pytest_asyncio.fixture(scope='module')\nasync def seed_data(db_engine):\n    async with async_sessionmaker(db_engine)() as session:\n        user = User(email='module@test.com', ...)\n        session.add(user)\n        await session.commit()\n        yield {'user_id': user.id}\n        await session.delete(user)\n        await session.commit()\n\n# Function scope (default): fresh for each test\n@pytest_asyncio.fixture\nasync def session(db_engine):\n    async with async_sessionmaker(db_engine)() as s:\n        yield s\n        await s.rollback()\n\n# Important: pytest.ini or pyproject.toml must set asyncio_mode\n# [tool.pytest.ini_options]\n# asyncio_mode = 'auto'  # or 'strict'\n\n# Sharing state safely across session-scoped fixtures\n@pytest_asyncio.fixture(scope='session')\nasync def http_client(db_engine):\n    # Session-scoped client shares the engine\n    factory = async_sessionmaker(db_engine)\n    async with factory() as session:\n        app.dependency_overrides[get_db] = lambda: session\n        async with AsyncClient(\n            transport=ASGITransport(app=app), base_url='http://test'\n        ) as client:\n            yield client\n    app.dependency_overrides.clear()\n```\n\n```toml\n# pyproject.toml\n[tool.pytest.ini_options]\nasyncio_mode = 'auto'\n\n[tool.pytest_asyncio]\nmode = 'auto'\n```\n\nSession scope creates the engine once (expensive) and function scope creates a fresh session per test (cheap, isolated). Never mix function-scoped fixtures as dependencies of session-scoped fixtures \u2014 pytest will error.",
    "tags": [
      "pytest",
      "asyncio",
      "fixtures",
      "testing",
      "sqlalchemy"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python dataclasses vs Pydantic for internal models",
    "context": "Using Pydantic BaseModel for everything including internal data transfer objects (DTOs) that never touch the API boundary. Pydantic validation overhead is unnecessary for internal models. Need guidance on when to use dataclasses vs Pydantic.",
    "solution": "Use Python dataclasses for internal DTOs, Pydantic only for external API boundaries:\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom datetime import datetime\n\n# Internal DTO \u2014 no validation needed, just structure\n@dataclass\nclass TraceSearchParams:\n    query: str\n    tags: list[str] = field(default_factory=list)\n    limit: int = 20\n    offset: int = 0\n    include_seed: bool = True\n    min_trust_score: float = 0.0\n\n@dataclass(frozen=True)  # Immutable\nclass SearchResult:\n    trace_id: str\n    similarity_score: float\n    combined_score: float\n    rank: int\n\n# Pydantic for API models (validation + serialization)\nfrom pydantic import BaseModel, Field, field_validator\n\nclass TraceSearchRequest(BaseModel):  # Used at API boundary\n    q: str = Field(..., min_length=2, max_length=500)\n    tags: list[str] = Field(default_factory=list, max_length=10)\n    limit: int = Field(20, ge=1, le=100)\n\n    @field_validator('tags')\n    @classmethod\n    def normalize_tags(cls, v: list[str]) -> list[str]:\n        return [t.lower().strip() for t in v]\n\n# Convert at the boundary\ndef to_search_params(request: TraceSearchRequest) -> TraceSearchParams:\n    return TraceSearchParams(\n        query=request.q,\n        tags=request.tags,\n        limit=request.limit,\n    )\n\n# Slots for memory efficiency with many instances\n@dataclass(slots=True)\nclass EmbeddingBatch:\n    trace_id: str\n    text: str\n    created_at: datetime\n```\n\nDataclasses are ~5x faster to construct than Pydantic models (no validation overhead). Use `frozen=True` for hashable/immutable value objects. Use `slots=True` (Python 3.10+) to reduce memory by ~30% when creating many instances. Reserve Pydantic for places where validation and serialization matter: API request/response models, config, external data parsing.",
    "tags": [
      "python",
      "dataclasses",
      "pydantic",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python asyncio TaskGroup for concurrent operations",
    "context": "Executing multiple independent async operations sequentially instead of concurrently. Using `asyncio.gather()` but need better error handling when one task fails \u2014 gather continues other tasks even on failure.",
    "solution": "Use `asyncio.TaskGroup` (Python 3.11+) for structured concurrency with automatic cancellation:\n\n```python\nimport asyncio\nfrom typing import Any\n\n# asyncio.gather (old pattern)\nasync def fetch_all_gather(ids: list[str]) -> list[dict]:\n    results = await asyncio.gather(\n        *[fetch_trace(id) for id in ids],\n        return_exceptions=True  # Gather swallows errors by default\n    )\n    # Have to check each result manually\n    return [r for r in results if not isinstance(r, Exception)]\n\n# TaskGroup (Python 3.11+) \u2014 preferred\nasync def fetch_all_taskgroup(ids: list[str]) -> list[dict]:\n    results = []\n    # If ANY task raises, all others are cancelled immediately\n    async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(fetch_trace(id)) for id in ids]\n    # All tasks completed successfully here\n    return [task.result() for task in tasks]\n\n# Real example: parallel database + Redis operations\nasync def get_trace_with_context(trace_id: str) -> dict:\n    async with asyncio.TaskGroup() as tg:\n        trace_task = tg.create_task(db.get_trace(trace_id))\n        votes_task = tg.create_task(db.get_votes(trace_id))\n        cached_views_task = tg.create_task(redis.get(f'views:{trace_id}'))\n\n    trace = trace_task.result()\n    votes = votes_task.result()\n    views = cached_views_task.result() or 0\n\n    return {**trace.dict(), 'votes': votes, 'view_count': int(views)}\n\n# With timeout\nasync def fetch_with_timeout(ids: list[str], timeout: float = 5.0) -> list[dict]:\n    try:\n        async with asyncio.timeout(timeout):\n            async with asyncio.TaskGroup() as tg:\n                tasks = [tg.create_task(fetch_trace(id)) for id in ids]\n        return [t.result() for t in tasks]\n    except TimeoutError:\n        return []  # Partial results discarded; all tasks cancelled\n```\n\n`TaskGroup` raises an `ExceptionGroup` if any task fails, which cancels the rest \u2014 this is structured concurrency. Use `asyncio.gather(return_exceptions=True)` when you want partial results even on failure. `asyncio.timeout()` (3.11+) replaces `asyncio.wait_for()` for nested timeout control.",
    "tags": [
      "python",
      "asyncio",
      "concurrency",
      "taskgroup"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy lazy loading configuration and N+1 query prevention",
    "context": "SQLAlchemy ORM queries are unexpectedly slow. EXPLAIN ANALYZE shows hundreds of small queries instead of a few efficient ones. The N+1 problem: loading a list of traces then accessing trace.tags triggers one query per trace.",
    "solution": "Use eager loading with `selectinload` or `joinedload` to prevent N+1 queries:\n\n```python\nfrom sqlalchemy.orm import selectinload, joinedload, contains_eager\nfrom sqlalchemy import select\n\n# BAD: N+1 queries\nasync def get_traces_bad(session: AsyncSession) -> list[Trace]:\n    result = await session.execute(select(Trace).limit(20))\n    traces = result.scalars().all()\n    for trace in traces:\n        # Each access triggers a new query!\n        print(trace.tags)  # SELECT * FROM tags WHERE trace_id = ?\n    return traces\n\n# GOOD: selectinload (best for collections/one-to-many)\nasync def get_traces_good(session: AsyncSession) -> list[Trace]:\n    result = await session.execute(\n        select(Trace)\n        .options(selectinload(Trace.tags))  # 2 queries total: traces + all tags\n        .limit(20)\n    )\n    return result.scalars().all()\n\n# joinedload (best for many-to-one/single object)\nasync def get_trace_with_contributor(trace_id: str, session: AsyncSession) -> Trace:\n    result = await session.execute(\n        select(Trace)\n        .options(joinedload(Trace.contributor))  # 1 query with JOIN\n        .where(Trace.id == trace_id)\n    )\n    return result.scalar_one()\n\n# Multiple relationships\nasync def get_trace_full(trace_id: str, session: AsyncSession) -> Trace:\n    result = await session.execute(\n        select(Trace)\n        .options(\n            selectinload(Trace.tags),\n            joinedload(Trace.contributor),\n        )\n        .where(Trace.id == trace_id)\n    )\n    return result.scalar_one()\n\n# Force error on accidental lazy loading\n# In model definition:\n# contributor: Mapped[User] = relationship('User', lazy='raise')\n# This raises sqlalchemy.exc.InvalidRequestError if accessed outside eager load\n```\n\nRule: `selectinload` for one-to-many/many-to-many (issues 2 queries: one for parent, one for all children). `joinedload` for many-to-one/one-to-one (single JOIN query). Set `lazy='raise'` on relationships you always want to control explicitly.",
    "tags": [
      "sqlalchemy",
      "orm",
      "n-plus-one",
      "performance",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python functools.cache and lru_cache for memoization",
    "context": "Expensive computations (tag normalization regex compilation, settings lookups, configuration parsing) run repeatedly with the same inputs. Need simple memoization without external caching infrastructure.",
    "solution": "Use `functools.cache` for in-process memoization of pure functions:\n\n```python\nimport functools\nimport re\nfrom typing import Callable\n\n# functools.cache (Python 3.9+) \u2014 unlimited cache, equivalent to lru_cache(maxsize=None)\n@functools.cache\ndef get_tag_pattern() -> re.Pattern:\n    # Compiled once, cached forever\n    return re.compile(r'^[a-z0-9][a-z0-9._-]{0,48}[a-z0-9]$|^[a-z0-9]$')\n\n@functools.cache\ndef normalize_tag_cached(raw: str) -> str:\n    return raw.strip().lower()[:50]\n\n# lru_cache with max size (bounded memory)\n@functools.lru_cache(maxsize=256)\ndef get_domain_for_tag(tag: str) -> str:\n    # Expensive lookup \u2014 cached for last 256 unique tags\n    return DOMAIN_MAPPING.get(tag, 'general')\n\n# Cache with typed=True (treats int and float args as different)\n@functools.lru_cache(maxsize=128, typed=True)\ndef wilson_score_cached(upvotes: int, total: int) -> float:\n    if total == 0:\n        return 0.0\n    p = upvotes / total\n    z = 1.9600\n    n = total\n    return (p + z*z/(2*n) - z * ((p*(1-p)+z*z/(4*n))/n)**0.5) / (1 + z*z/n)\n\n# Method caching with cache_info\nprint(wilson_score_cached.cache_info())  # hits, misses, maxsize, currsize\nwilson_score_cached.cache_clear()  # clear when needed\n\n# For class methods \u2014 cache_info() works differently\nclass TagValidator:\n    @functools.cached_property\n    def pattern(self) -> re.Pattern:\n        # Computed once per instance, on first access\n        return re.compile(r'^[a-z0-9._-]+$')\n\n    def validate(self, tag: str) -> bool:\n        return bool(self.pattern.match(tag))\n```\n\n`functools.cache` is simpler but unbounded \u2014 use only for functions with limited unique inputs. `lru_cache` evicts least-recently-used entries at `maxsize`. `cached_property` is for instance properties computed once. Thread-safe in CPython but not guaranteed across Python implementations.",
    "tags": [
      "python",
      "caching",
      "memoization",
      "functools",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI dependency injection with protocol interfaces",
    "context": "FastAPI services are tightly coupled to concrete implementations (PostgreSQL, Redis, specific email provider). Want to swap implementations for testing without monkey-patching or complex mocking setups.",
    "solution": "Define service protocols and inject via FastAPI's dependency system:\n\n```python\nfrom typing import Protocol, runtime_checkable\n\n# Define interfaces\n@runtime_checkable\nclass EmbeddingService(Protocol):\n    async def embed(self, text: str) -> list[float]: ...\n    async def embed_batch(self, texts: list[str]) -> list[list[float]]: ...\n\n@runtime_checkable\nclass CacheService(Protocol):\n    async def get(self, key: str) -> str | None: ...\n    async def set(self, key: str, value: str, ttl: int = 300) -> None: ...\n\n# Concrete implementations\nclass OpenAIEmbeddingService:\n    async def embed(self, text: str) -> list[float]:\n        response = await client.embeddings.create(input=text, model='text-embedding-3-small')\n        return response.data[0].embedding\n\nclass RedisCache:\n    def __init__(self, redis: Redis): self.redis = redis\n    async def get(self, key: str) -> str | None:\n        val = await self.redis.get(key)\n        return val.decode() if val else None\n    async def set(self, key: str, value: str, ttl: int = 300) -> None:\n        await self.redis.setex(key, ttl, value)\n\n# Dependencies\nasync def get_embedding_service(request: Request) -> EmbeddingService:\n    return OpenAIEmbeddingService()\n\nasync def get_cache(request: Request) -> CacheService:\n    return RedisCache(request.app.state.redis)\n\n# Router using protocols, not concrete types\n@router.post('/traces/search')\nasync def search(\n    query: SearchRequest,\n    embed: EmbeddingService = Depends(get_embedding_service),\n    cache: CacheService = Depends(get_cache),\n) -> SearchResponse:\n    cached = await cache.get(f'search:{query.q}')\n    if cached:\n        return SearchResponse.model_validate_json(cached)\n    embedding = await embed.embed(query.q)\n    results = await search_by_vector(embedding)\n    await cache.set(f'search:{query.q}', results.model_dump_json(), ttl=60)\n    return results\n\n# In tests \u2014 swap implementations\nclass FakeEmbeddingService:\n    async def embed(self, text: str) -> list[float]:\n        return [0.1] * 1536  # Deterministic\n\napp.dependency_overrides[get_embedding_service] = lambda: FakeEmbeddingService()\n```\n\nProtocols with `@runtime_checkable` enable `isinstance(service, EmbeddingService)` checks. `dependency_overrides` is the correct FastAPI pattern for injecting test doubles \u2014 no monkey-patching needed.",
    "tags": [
      "fastapi",
      "dependency-injection",
      "protocols",
      "testing",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Alembic migration with data migration alongside schema change",
    "context": "Need to split a single `name` column into `first_name` and `last_name` columns. This requires a schema change (add columns, remove old) AND a data migration (populate new columns from old). Pure DDL migrations don't handle the data step.",
    "solution": "Write an Alembic migration with upgrade() containing both DDL and DML in the correct order:\n\n```python\n# alembic/versions/0005_split_name_column.py\n\"\"\"split name into first_name and last_name\n\nRevision ID: 0005\nDowns revision: '0004'\nCreate Date: 2024-01-15\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.sql import text\n\ndef upgrade() -> None:\n    # 1. Add new columns as nullable first\n    op.add_column('users', sa.Column('first_name', sa.String(100), nullable=True))\n    op.add_column('users', sa.Column('last_name', sa.String(100), nullable=True))\n\n    # 2. Migrate data (run before making columns NOT NULL)\n    conn = op.get_bind()\n    conn.execute(text(\"\"\"\n        UPDATE users\n        SET\n            first_name = split_part(name, ' ', 1),\n            last_name = NULLIF(split_part(name, ' ', 2), '')\n        WHERE name IS NOT NULL\n    \"\"\"))\n\n    # 3. Make columns NOT NULL now that data is populated\n    op.alter_column('users', 'first_name', nullable=False)\n\n    # 4. Drop old column (last \u2014 after data is safe)\n    op.drop_column('users', 'name')\n\n\ndef downgrade() -> None:\n    # Reverse the migration\n    op.add_column('users', sa.Column('name', sa.String(200), nullable=True))\n\n    conn = op.get_bind()\n    conn.execute(text(\"\"\"\n        UPDATE users\n        SET name = TRIM(COALESCE(first_name, '') || ' ' || COALESCE(last_name, ''))\n    \"\"\"))\n\n    op.alter_column('users', 'name', nullable=False)\n    op.drop_column('users', 'first_name')\n    op.drop_column('users', 'last_name')\n```\n\nKey ordering: (1) add nullable columns \u2192 (2) migrate data \u2192 (3) add NOT NULL constraints \u2192 (4) drop old column. Never set NOT NULL before data migration. Always write a working `downgrade()`. Test on a production-size data clone before running \u2014 large tables may need batched updates.",
    "tags": [
      "alembic",
      "postgresql",
      "migrations",
      "data-migration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python structured concurrency with anyio",
    "context": "Building code that needs to work with both asyncio and Trio event loops (or library code that shouldn't dictate the event loop). Also need nursery-style task cancellation that TaskGroup provides but with trio compatibility.",
    "solution": "Use `anyio` as an abstraction layer over asyncio and trio:\n\n```python\nimport anyio\nfrom anyio import create_task_group, move_on_after, fail_after\nfrom anyio.abc import TaskGroup\n\n# Concurrent tasks with automatic cancellation\nasync def fetch_trace_data(trace_id: str) -> dict:\n    async with create_task_group() as tg:\n        results = {}\n\n        async def fetch_trace():\n            results['trace'] = await db.get_trace(trace_id)\n\n        async def fetch_votes():\n            results['votes'] = await db.get_votes(trace_id)\n\n        async def fetch_tags():\n            results['tags'] = await db.get_tags(trace_id)\n\n        tg.start_soon(fetch_trace)\n        tg.start_soon(fetch_votes)\n        tg.start_soon(fetch_tags)\n\n    return results\n\n# Timeout with move_on_after (gives up, returns None-equivalent)\nasync def try_embed_with_timeout(text: str) -> list[float] | None:\n    result = None\n    with move_on_after(5.0):  # Gives up after 5 seconds\n        result = await embed_text(text)\n    return result\n\n# Timeout that raises on expiry\nasync def embed_or_fail(text: str) -> list[float]:\n    with fail_after(10.0):\n        return await embed_text(text)\n\n# Run from sync code\nif __name__ == '__main__':\n    # Run with asyncio (default)\n    anyio.run(main)\n\n    # Run with trio\n    anyio.run(main, backend='trio')\n\n# Library code that works with both\nasync def anyio_compatible_function() -> None:\n    # Uses anyio primitives, not asyncio directly\n    await anyio.sleep(1)\n    async with anyio.open_file('data.txt') as f:\n        content = await f.read()\n```\n\n`anyio` is used by Starlette/FastAPI internally. `create_task_group()` behaves like Python 3.11's `asyncio.TaskGroup`. `move_on_after` is cleaner than try/except TimeoutError for optional operations.",
    "tags": [
      "python",
      "anyio",
      "asyncio",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI custom exception handlers and error responses",
    "context": "Application raises various exceptions (ValidationError, NotFound, PermissionDenied) but they all return inconsistent error responses. Clients can't reliably parse error details. Need a consistent error response schema across all endpoints.",
    "solution": "Define custom exception classes and register handlers on the app:\n\n```python\n# app/exceptions.py\nfrom fastapi import HTTPException\n\nclass AppError(Exception):\n    def __init__(self, message: str, status_code: int = 500, code: str = 'internal_error'):\n        self.message = message\n        self.status_code = status_code\n        self.code = code\n        super().__init__(message)\n\nclass NotFoundError(AppError):\n    def __init__(self, resource: str, id: str):\n        super().__init__(f'{resource} not found: {id}', status_code=404, code='not_found')\n\nclass PermissionError(AppError):\n    def __init__(self, reason: str = 'Forbidden'):\n        super().__init__(reason, status_code=403, code='forbidden')\n\nclass ConflictError(AppError):\n    def __init__(self, message: str):\n        super().__init__(message, status_code=409, code='conflict')\n\n# app/main.py\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi.exceptions import RequestValidationError\nfrom pydantic import ValidationError\n\napp = FastAPI()\n\n@app.exception_handler(AppError)\nasync def app_error_handler(request: Request, exc: AppError) -> JSONResponse:\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={'error': {'code': exc.code, 'message': exc.message}}\n    )\n\n@app.exception_handler(RequestValidationError)\nasync def validation_error_handler(request: Request, exc: RequestValidationError) -> JSONResponse:\n    return JSONResponse(\n        status_code=422,\n        content={\n            'error': {\n                'code': 'validation_error',\n                'message': 'Request validation failed',\n                'details': exc.errors(),\n            }\n        }\n    )\n\n# Usage in routes\n@router.get('/traces/{trace_id}')\nasync def get_trace(trace_id: str, session: AsyncSession = Depends(get_db)) -> TraceResponse:\n    trace = await session.get(Trace, trace_id)\n    if not trace:\n        raise NotFoundError('Trace', trace_id)\n    return TraceResponse.model_validate(trace)\n```\n\nAll errors return `{'error': {'code': '...', 'message': '...'}}` \u2014 clients only need to handle one shape. Subclass `AppError` for domain-specific errors. Override the default `RequestValidationError` handler for consistent Pydantic error formatting.",
    "tags": [
      "fastapi",
      "error-handling",
      "exceptions",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python asyncio event loop patterns and common pitfalls",
    "context": "Getting errors like 'There is no current event loop' or 'coroutine was never awaited' in async Python code. Also confusion about when to use asyncio.run() vs await, and how to call async code from sync context.",
    "solution": "Understand the event loop lifecycle and correct patterns for mixed sync/async code:\n\n```python\nimport asyncio\n\n# PATTERN 1: Entry point (only call asyncio.run() at the top level)\nasync def main() -> None:\n    result = await do_something()\n    return result\n\nif __name__ == '__main__':\n    asyncio.run(main())  # Creates a new event loop, runs until complete, closes it\n\n# PATTERN 2: Call async from sync (when you have no event loop)\ndef sync_function() -> str:\n    # When there's no running loop\n    return asyncio.run(async_function())\n\n# PATTERN 3: Call async from sync (when event loop IS running \u2014 e.g., in Jupyter)\n# asyncio.run() raises RuntimeError: 'This event loop is already running'\nimport nest_asyncio\nnest_asyncio.apply()  # Allows nested event loops\n\n# PATTERN 4: Run sync from async (blocking I/O in async context)\nimport time\n\nasync def async_with_blocking() -> None:\n    loop = asyncio.get_event_loop()\n    # Run blocking I/O in thread pool (doesn't block event loop)\n    result = await loop.run_in_executor(None, time.sleep, 1)\n    # For CPU-bound: use ProcessPoolExecutor\n    from concurrent.futures import ProcessPoolExecutor\n    with ProcessPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, cpu_intensive_function, data)\n\n# COMMON MISTAKE: forgetting await\nasync def bad_example() -> None:\n    result = fetch_data()  # Returns coroutine object, NOT the result!\n    print(result)  # <coroutine object fetch_data at 0x...>\n\nasync def good_example() -> None:\n    result = await fetch_data()  # Runs the coroutine\n    print(result)\n\n# COMMON MISTAKE: mixing asyncio with threading incorrectly\nasync def thread_safe_async(loop: asyncio.AbstractEventLoop) -> None:\n    # From a thread, schedule in the event loop\n    future = asyncio.run_coroutine_threadsafe(async_operation(), loop)\n    result = future.result(timeout=5)  # Blocks the thread, not the event loop\n```\n\nRule: `asyncio.run()` is for the outermost entry point only \u2014 one per program. Inside async functions, always `await`. For sync code that needs async, use `asyncio.run()`. For blocking I/O inside async, use `run_in_executor`.",
    "tags": [
      "python",
      "asyncio",
      "event-loop",
      "async-await"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL composite indexes for multi-column query patterns",
    "context": "Queries filter on multiple columns: `WHERE status = 'validated' AND trust_score > 0.5 ORDER BY created_at DESC`. Single-column indexes on each column aren't used effectively. Query planner does a full table scan.",
    "solution": "Design composite indexes to match the exact query pattern (column order matters):\n\n```sql\n-- BAD: Three single-column indexes \u2014 planner may not combine them efficiently\nCREATE INDEX idx_traces_status ON traces(status);\nCREATE INDEX idx_traces_trust_score ON traces(trust_score);\nCREATE INDEX idx_traces_created_at ON traces(created_at);\n\n-- GOOD: Composite index ordered: equality filter \u2192 range filter \u2192 sort\nCREATE INDEX idx_traces_status_trust_created ON traces(status, trust_score DESC, created_at DESC);\n\n-- Partial index \u2014 only indexes rows that match the WHERE (smaller, faster)\nCREATE INDEX idx_validated_traces ON traces(trust_score DESC, created_at DESC)\nWHERE status = 'validated';\n\n-- Partial index for non-null embeddings (used by embedding worker polling)\nCREATE INDEX idx_traces_needs_embedding ON traces(id)\nWHERE embedding IS NULL;\n\n-- Verify index is used\nEXPLAIN ANALYZE\nSELECT id, title, trust_score\nFROM traces\nWHERE status = 'validated'\n  AND trust_score > 0.5\nORDER BY created_at DESC\nLIMIT 20;\n-- Should show: Index Scan using idx_traces_status_trust_created\n-- NOT: Seq Scan\n\n-- Check index sizes and usage\nSELECT\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS size,\n    idx_scan AS scans,\n    idx_tup_read AS rows_read\nFROM pg_stat_user_indexes\nWHERE tablename = 'traces'\nORDER BY idx_scan DESC;\n```\n\nRule for column order: equality columns first (`=`), then range columns (`>`, `<`, `BETWEEN`), then ORDER BY columns last. Partial indexes are dramatically smaller and faster when the filter covers most queries. `INCLUDE` clause adds columns to the index leaf without sorting them (useful for covering indexes).",
    "tags": [
      "postgresql",
      "indexing",
      "performance",
      "composite-index"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pydantic v2 computed fields and model serialization",
    "context": "Pydantic models need computed/derived fields that are calculated from other fields (e.g., full_name from first/last, formatted dates, masked API keys). Also need custom serialization (snake_case to camelCase, excluding None fields).",
    "solution": "Use `@computed_field`, `model_serializer`, and `model_config` for advanced Pydantic v2 patterns:\n\n```python\nfrom pydantic import BaseModel, computed_field, model_serializer, Field\nfrom pydantic import field_serializer\nfrom typing import Optional\nfrom datetime import datetime\n\nclass TraceResponse(BaseModel):\n    model_config = {\n        'populate_by_name': True,  # Allow both alias and field name\n        'from_attributes': True,   # Enable ORM mode (replaces orm_mode)\n    }\n\n    id: str\n    title: str\n    context_text: str = Field(alias='context')  # JSON uses 'context'\n    solution_text: str = Field(alias='solution')\n    trust_score: float\n    created_at: datetime\n    tags: list[str] = []\n\n    # Computed field (included in serialization)\n    @computed_field\n    @property\n    def is_highly_trusted(self) -> bool:\n        return self.trust_score >= 0.8\n\n    @computed_field\n    @property\n    def age_days(self) -> int:\n        return (datetime.utcnow() - self.created_at).days\n\n    # Custom field serializer\n    @field_serializer('created_at')\n    def serialize_created_at(self, dt: datetime) -> str:\n        return dt.isoformat() + 'Z'\n\n    @field_serializer('trust_score')\n    def serialize_score(self, score: float) -> float:\n        return round(score, 4)\n\nclass UserResponse(BaseModel):\n    model_config = {'populate_by_name': True}\n\n    id: str\n    email: str\n    api_key_hash: str  # Internal field\n\n    # Mask sensitive data in serialization\n    @field_serializer('api_key_hash')\n    def mask_api_key(self, value: str) -> str:\n        return f'***{value[-4:]}'\n\n# Exclude None fields globally\nclass BaseResponse(BaseModel):\n    model_config = {'populate_by_name': True}\n\n    def model_dump(self, **kwargs):\n        kwargs.setdefault('exclude_none', True)  # Default to excluding Nones\n        return super().model_dump(**kwargs)\n\n# Usage\ntrace = TraceResponse(id='123', title='Test', context='ctx', solution='sol', ...)\nprint(trace.model_dump(by_alias=True))  # Uses aliases: 'context', 'solution'\nprint(trace.model_dump_json())  # JSON string with computed fields included\n```\n\n`@computed_field` requires a `@property` decorator and is included in `model_dump()` and JSON serialization. `from_attributes=True` replaces Pydantic v1's `orm_mode=True`. `populate_by_name=True` allows both the field name and alias to be used.",
    "tags": [
      "pydantic",
      "python",
      "serialization",
      "computed-fields"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker healthcheck with curl vs wget and retry logic",
    "context": "Docker Compose healthchecks fail because the container doesn't have curl or wget installed in a slim image. Need a working healthcheck that works in minimal Alpine and Debian-slim images, and handles startup time correctly.",
    "solution": "Use Python's built-in HTTP or `nc` for healthchecks in minimal images:\n\n```yaml\nservices:\n  api:\n    image: myapp:latest\n    healthcheck:\n      # Works if Python is installed (guaranteed in Python images)\n      test: [\"CMD\", \"python3\", \"-c\",\n             \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=2)\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n      start_period: 30s  # Grace period before health failures count\n\n  # For Alpine-based images (has wget, not curl)\n  nginx:\n    image: nginx:alpine\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--no-verbose\", \"--tries=1\", \"--spider\",\n             \"http://localhost:80/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 10s\n\n  # For postgres \u2014 use pg_isready (built-in)\n  postgres:\n    image: pgvector/pgvector:pg17\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\n      interval: 5s\n      timeout: 3s\n      retries: 5\n      start_period: 15s\n\n  # Redis \u2014 use redis-cli\n  redis:\n    image: redis:7-alpine\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 2s\n      retries: 3\n```\n\n```python\n# FastAPI health endpoint\n@app.get('/health')\nasync def health() -> dict:\n    return {'status': 'ok', 'version': settings.app_version}\n```\n\n`start_period` gives the container time to initialize \u2014 failures during this period don't count toward `retries`. Use `CMD-SHELL` when you need shell features (environment variable expansion). `CMD` (array form) is preferred \u2014 no shell injection risk.",
    "tags": [
      "docker",
      "healthcheck",
      "docker-compose"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React Query (TanStack Query) for server state management",
    "context": "Manually managing server state with useState/useEffect is complex: tracking loading/error/stale states, deduplicating requests, invalidating cache after mutations. Looking for a dedicated server state library.",
    "solution": "Use TanStack Query v5 for declarative server state management:\n\n```typescript\n// main.tsx \u2014 wrap app with QueryClientProvider\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      staleTime: 60 * 1000,  // 1 minute before refetch\n      retry: 2,\n      refetchOnWindowFocus: false,\n    },\n  },\n});\n\nexport default function App() {\n  return (\n    <QueryClientProvider client={queryClient}>\n      <Router />\n    </QueryClientProvider>\n  );\n}\n\n// hooks/useTraces.ts\nimport { useQuery, useMutation, useQueryClient, useInfiniteQuery } from '@tanstack/react-query';\n\nexport function useTrace(id: string) {\n  return useQuery({\n    queryKey: ['traces', id],\n    queryFn: () => api.getTrace(id),\n    enabled: !!id,  // Don't run if id is empty\n  });\n}\n\nexport function useSearchTraces(query: string) {\n  return useQuery({\n    queryKey: ['traces', 'search', query],\n    queryFn: () => api.searchTraces(query),\n    enabled: query.length >= 2,\n    placeholderData: previousData => previousData,  // Keep previous results while loading\n  });\n}\n\nexport function useCreateTrace() {\n  const queryClient = useQueryClient();\n  return useMutation({\n    mutationFn: (data: CreateTraceRequest) => api.createTrace(data),\n    onSuccess: (newTrace) => {\n      // Invalidate and refetch the traces list\n      queryClient.invalidateQueries({ queryKey: ['traces'] });\n      // Optimistically add to cache\n      queryClient.setQueryData(['traces', newTrace.id], newTrace);\n    },\n  });\n}\n\n// Component usage\nfunction TraceDetail({ id }: { id: string }) {\n  const { data: trace, isLoading, error } = useTrace(id);\n  const { mutate: createTrace, isPending } = useCreateTrace();\n\n  if (isLoading) return <Skeleton />;\n  if (error) return <ErrorBoundary error={error} />;\n  return <TraceCard trace={trace} />;\n}\n```\n\nQuery keys are the cache key \u2014 same key = deduplicated request. `staleTime` controls when data is considered stale. `invalidateQueries` triggers a background refetch. `placeholderData` prevents loading flash between searches.",
    "tags": [
      "react",
      "react-query",
      "typescript",
      "data-fetching"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "OpenAI function calling for structured outputs",
    "context": "Using OpenAI chat completions to extract structured data from user input (classify intent, extract entities, fill forms). Parsing JSON from unstructured LLM output is fragile and requires complex prompt engineering.",
    "solution": "Use OpenAI function calling (tool_choice) to guarantee structured JSON output:\n\n```python\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom openai.lib._pydantic import to_strict_json_schema\n\nclient = AsyncOpenAI()\n\n# Define the output structure with Pydantic\nclass TraceClassification(BaseModel):\n    category: str  # 'python', 'javascript', 'database', 'docker', 'ci-cd', 'other'\n    primary_tags: list[str]  # 2-5 normalized tags\n    difficulty: str  # 'beginner', 'intermediate', 'advanced'\n    is_code_heavy: bool\n    confidence: float  # 0.0 to 1.0\n\nasync def classify_trace(title: str, context: str) -> TraceClassification:\n    # Method 1: JSON mode (any JSON)\n    response = await client.chat.completions.create(\n        model='gpt-4o-mini',\n        response_format={'type': 'json_object'},\n        messages=[\n            {'role': 'system', 'content': 'Classify the coding trace. Respond with JSON only.'},\n            {'role': 'user', 'content': f'Title: {title}\\nContext: {context}'},\n        ]\n    )\n\n    # Method 2: Structured outputs (enforced schema \u2014 preferred)\n    response = await client.beta.chat.completions.parse(\n        model='gpt-4o-mini',\n        response_format=TraceClassification,\n        messages=[\n            {'role': 'system', 'content': 'Classify the coding trace.'},\n            {'role': 'user', 'content': f'Title: {title}\\nContext: {context}'},\n        ]\n    )\n    return response.choices[0].message.parsed  # Already a TraceClassification instance\n\n# Method 3: Tool calling (for function invocation)\ntools = [{\n    'type': 'function',\n    'function': {\n        'name': 'classify_trace',\n        'description': 'Classify a coding trace',\n        'parameters': TraceClassification.model_json_schema(),\n        'strict': True,\n    }\n}]\n\nresponse = await client.chat.completions.create(\n    model='gpt-4o-mini',\n    tools=tools,\n    tool_choice={'type': 'function', 'function': {'name': 'classify_trace'}},\n    messages=[...]\n)\ntool_call = response.choices[0].message.tool_calls[0]\nresult = TraceClassification.model_validate_json(tool_call.function.arguments)\n```\n\nUse `client.beta.chat.completions.parse()` with a Pydantic model as `response_format` for the simplest structured output \u2014 available in gpt-4o models. `strict: True` in tool definitions enables strict schema adherence (no extra fields).",
    "tags": [
      "openai",
      "function-calling",
      "structured-output",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL EXPLAIN ANALYZE output interpretation",
    "context": "Query is slow but unsure why. Running EXPLAIN ANALYZE produces verbose output with nodes like 'Seq Scan', 'Hash Join', 'Bitmap Heap Scan' and numbers for cost, rows, and buffers. Need to understand what to look for to identify the bottleneck.",
    "solution": "Read EXPLAIN ANALYZE output from bottom up, focus on actual vs estimated rows:\n\n```sql\n-- Add BUFFERS for cache hit information\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT t.id, t.title, u.email\nFROM traces t\nJOIN users u ON t.contributor_id = u.id\nWHERE t.status = 'validated'\n  AND t.trust_score > 0.5\nORDER BY t.created_at DESC\nLIMIT 20;\n\n-- Sample output:\n-- Limit  (cost=0.29..1500.30 rows=20 width=120) (actual time=0.100..45.200 rows=20 loops=1)\n--   ->  Index Scan Backward using idx_traces_created_at on traces t\n--         (cost=0.29..75000.50 rows=1000 width=120) (actual time=0.090..45.100 rows=20 loops=1)\n--         Filter: ((status = 'validated') AND (trust_score > 0.5))\n--         Rows Removed by Filter: 50000        <-- BAD: scanning 50k to return 20\n--         Buffers: shared hit=500 read=4500    <-- 4500 disk reads is bad\n```\n\nKey indicators of problems:\n```\n-- 1. Row estimate mismatch: estimated 1000, actual 50000\n--    Fix: ANALYZE the table to update statistics\nANALYZE traces;\n\n-- 2. Seq Scan on large table\n--    Fix: Add appropriate index\nCREATE INDEX idx_traces_status_trust ON traces(status, trust_score DESC)\nWHERE status = 'validated';\n\n-- 3. 'Rows Removed by Filter' is large relative to output\n--    Fix: Add the filter column to the index\n\n-- 4. High 'Buffers: read' (disk I/O) vs 'hit' (cache)\n--    Fix: Increase shared_buffers or add an index to reduce scanned rows\n\n-- 5. Nested Loop with many loops\n--    loops=5000 means inner plan ran 5000 times \u2014 might need Hash Join\n--    Fix: SET enable_nestloop = off; to test if Hash Join is faster\n```\n\nCost units are arbitrary \u2014 compare relative costs, not absolute. Total cost is bottom number in root node. `actual time` is in milliseconds. Focus on nodes where `actual rows` >> `estimated rows` \u2014 that's where statistics are stale.",
    "tags": [
      "postgresql",
      "explain-analyze",
      "query-optimization",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis distributed lock with asyncio and Lua script",
    "context": "Multiple workers process tasks concurrently and need to ensure only one worker handles a given resource at a time. Need a distributed lock that works across multiple API instances with automatic expiry to prevent deadlocks.",
    "solution": "Implement a distributed lock using Redis SET NX EX and atomic Lua for release:\n\n```python\nimport asyncio\nimport uuid\nfrom contextlib import asynccontextmanager\nfrom redis.asyncio import Redis\n\n# Lua script for atomic lock release (check-and-delete)\nRELEASE_LOCK_SCRIPT = \"\"\"\nif redis.call('GET', KEYS[1]) == ARGV[1] then\n    return redis.call('DEL', KEYS[1])\nelse\n    return 0\nend\n\"\"\"\n\nclass DistributedLock:\n    def __init__(self, redis: Redis, name: str, ttl: int = 30):\n        self.redis = redis\n        self.key = f'lock:{name}'\n        self.ttl = ttl\n        self.token = str(uuid.uuid4())  # Unique token prevents releasing others' locks\n\n    async def acquire(self, timeout: float = 10.0) -> bool:\n        deadline = asyncio.get_event_loop().time() + timeout\n        while asyncio.get_event_loop().time() < deadline:\n            # SET NX EX: set if not exists, with TTL\n            acquired = await self.redis.set(\n                self.key, self.token,\n                nx=True,  # Only set if key doesn't exist\n                ex=self.ttl,  # Auto-expire after TTL seconds\n            )\n            if acquired:\n                return True\n            await asyncio.sleep(0.1)  # Poll interval\n        return False\n\n    async def release(self) -> bool:\n        result = await self.redis.eval(RELEASE_LOCK_SCRIPT, 1, self.key, self.token)\n        return bool(result)\n\n    async def extend(self, additional_ttl: int) -> bool:\n        # Extend TTL atomically \u2014 only if we still hold the lock\n        script = \"\"\"\n        if redis.call('GET', KEYS[1]) == ARGV[1] then\n            return redis.call('EXPIRE', KEYS[1], ARGV[2])\n        else\n            return 0\n        end\n        \"\"\"\n        result = await self.redis.eval(script, 1, self.key, self.token, additional_ttl)\n        return bool(result)\n\n@asynccontextmanager\nasync def distributed_lock(redis: Redis, name: str, ttl: int = 30, timeout: float = 10.0):\n    lock = DistributedLock(redis, name, ttl)\n    acquired = await lock.acquire(timeout=timeout)\n    if not acquired:\n        raise TimeoutError(f'Could not acquire lock: {name}')\n    try:\n        yield lock\n    finally:\n        await lock.release()\n\n# Usage\nasync def process_trace(trace_id: str, redis: Redis) -> None:\n    async with distributed_lock(redis, f'trace:{trace_id}', ttl=60) as lock:\n        # Only one worker runs this block per trace_id\n        trace = await fetch_trace(trace_id)\n        embedding = await generate_embedding(trace.text)\n        # Extend lock if processing takes longer\n        if embedding_took_long:\n            await lock.extend(30)\n        await save_embedding(trace_id, embedding)\n```\n\nThe Lua script is atomic \u2014 no race condition between GET and DEL. The unique `token` prevents Worker A from releasing Worker B's lock (important if TTL expires while still processing).",
    "tags": [
      "redis",
      "distributed-lock",
      "python",
      "asyncio",
      "concurrency"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy bulk insert with returning for IDs",
    "context": "Inserting thousands of rows one at a time with individual session.add() calls is too slow. Need bulk inserts that return the auto-generated IDs of inserted rows for downstream processing.",
    "solution": "Use `insert().returning()` with `execute()` for efficient bulk inserts:\n\n```python\nfrom sqlalchemy import insert\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\n# Method 1: executemany (fastest, no returning)\nasync def bulk_insert_traces(session: AsyncSession, traces: list[dict]) -> None:\n    await session.execute(\n        insert(Trace),\n        traces  # List of dicts matching column names\n    )\n    await session.commit()\n\n# Method 2: insert with RETURNING (get IDs back)\nasync def bulk_insert_with_ids(\n    session: AsyncSession,\n    traces: list[dict]\n) -> list[str]:\n    result = await session.execute(\n        insert(Trace).returning(Trace.id),\n        traces\n    )\n    ids = result.scalars().all()\n    await session.commit()\n    return ids\n\n# Method 3: chunked bulk insert (for very large datasets)\nasync def chunked_bulk_insert(\n    session: AsyncSession,\n    records: list[dict],\n    chunk_size: int = 500\n) -> int:\n    total_inserted = 0\n    for i in range(0, len(records), chunk_size):\n        chunk = records[i:i + chunk_size]\n        await session.execute(insert(Trace), chunk)\n        await session.commit()\n        total_inserted += len(chunk)\n        print(f'Inserted {total_inserted}/{len(records)}')\n    return total_inserted\n\n# Method 4: Bulk insert with conflict handling\nasync def upsert_tags(session: AsyncSession, tag_names: list[str]) -> list[str]:\n    from sqlalchemy.dialects.postgresql import insert as pg_insert\n\n    result = await session.execute(\n        pg_insert(Tag)\n        .values([{'name': name} for name in tag_names])\n        .on_conflict_do_nothing(index_elements=['name'])\n        .returning(Tag.id, Tag.name)\n    )\n    return result.fetchall()\n\n# Usage: import 1000 seed traces\nasync def import_seed_batch(session: AsyncSession, seed_data: list[dict]) -> list[str]:\n    trace_rows = [\n        {\n            'title': s['title'],\n            'context_text': s['context'],\n            'solution_text': s['solution'],\n            'status': 'validated',\n            'is_seed': True,\n            'trust_score': 1.0,\n            'contributor_id': seed_user_id,\n        }\n        for s in seed_data\n    ]\n    return await bulk_insert_with_ids(session, trace_rows)\n```\n\nBulk insert with `execute(stmt, list_of_dicts)` uses a single round trip. `RETURNING` adds minimal overhead vs separate SELECT. Chunk at 100-1000 rows to avoid parameter limits and reduce transaction size for large datasets.",
    "tags": [
      "sqlalchemy",
      "postgresql",
      "bulk-insert",
      "performance",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose profiles for optional services",
    "context": "Docker Compose file has many services but not all are needed all the time. Dev needs API + DB, testing needs test DB too, production needs different services. Maintaining separate compose files leads to drift.",
    "solution": "Use Docker Compose profiles to group services by use case:\n\n```yaml\n# docker-compose.yml\nversion: '3.9'\n\nservices:\n  # Core services (always started \u2014 no profile)\n  postgres:\n    image: pgvector/pgvector:pg17\n    environment:\n      POSTGRES_DB: commontrace\n      POSTGRES_USER: commontrace\n      POSTGRES_PASSWORD: commontrace\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U commontrace\"]\n      interval: 5s\n      retries: 5\n\n  redis:\n    image: redis:7-alpine\n\n  api:\n    build: ./api\n    depends_on:\n      postgres: { condition: service_healthy }\n      redis: { condition: service_started }\n\n  # Dev-only services (start with --profile dev)\n  worker:\n    build: ./api\n    command: python -m app.worker\n    profiles: [dev, production]\n    depends_on: [postgres, redis]\n\n  # Testing services (start with --profile test)\n  test-postgres:\n    image: pgvector/pgvector:pg17\n    profiles: [test]\n    environment:\n      POSTGRES_DB: test_commontrace\n      POSTGRES_USER: test\n      POSTGRES_PASSWORD: test\n\n  # Monitoring (start with --profile monitoring)\n  prometheus:\n    image: prom/prometheus\n    profiles: [monitoring]\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n\n  grafana:\n    image: grafana/grafana\n    profiles: [monitoring]\n    ports: ['3000:3000']\n```\n\n```bash\n# Start only core services\ndocker-compose up -d\n\n# Start with dev tools\ndocker-compose --profile dev up -d\n\n# Start with monitoring\ndocker-compose --profile monitoring up -d\n\n# Multiple profiles\ndocker-compose --profile dev --profile monitoring up -d\n\n# Or use COMPOSE_PROFILES env var\nexport COMPOSE_PROFILES=dev,monitoring\ndocker-compose up -d\n```\n\nServices without a `profiles:` key always start. Services with profiles only start when that profile is active. Profiles are additive \u2014 multiple `--profile` flags combine their services.",
    "tags": [
      "docker",
      "docker-compose",
      "profiles",
      "configuration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Dockerfile ARG and ENV for build-time configuration",
    "context": "Docker images need different configuration for different environments (staging vs production). Hardcoding config in the Dockerfile or passing everything at runtime isn't sufficient \u2014 some values must be baked in at build time (like the app version).",
    "solution": "Use `ARG` for build-time variables and `ENV` for runtime variables:\n\n```dockerfile\n# Dockerfile\nFROM python:3.12-slim\n\n# ARG: only available during build, not at runtime\nARG BUILD_VERSION=dev\nARG COMMIT_SHA=unknown\nARG BUILD_DATE\n\n# Convert build-time ARG to runtime ENV where needed\nENV APP_VERSION=${BUILD_VERSION}\nENV COMMIT_SHA=${COMMIT_SHA}\n\n# ENV: available at runtime\nENV PYTHONUNBUFFERED=1\nENV PYTHONDONTWRITEBYTECODE=1\nENV PORT=8000\n\n# Embed build metadata\nLABEL org.opencontainers.image.version=${BUILD_VERSION}\nLABEL org.opencontainers.image.revision=${COMMIT_SHA}\nLABEL org.opencontainers.image.created=${BUILD_DATE}\n\nWORKDIR /app\nCOPY . .\nRUN pip install -e .\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```bash\n# Pass ARGs at build time\ndocker build \\\n  --build-arg BUILD_VERSION=$(git describe --tags) \\\n  --build-arg COMMIT_SHA=$(git rev-parse --short HEAD) \\\n  --build-arg BUILD_DATE=$(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  -t myapp:$(git describe --tags) .\n\n# Override ENV at runtime\ndocker run \\\n  -e DATABASE_URL=postgresql://... \\\n  -e REDIS_URL=redis://... \\\n  myapp:latest\n```\n\n```yaml\n# GitHub Actions integration\n- name: Build\n  uses: docker/build-push-action@v5\n  with:\n    build-args: |\n      BUILD_VERSION=${{ github.ref_name }}\n      COMMIT_SHA=${{ github.sha }}\n      BUILD_DATE=${{ steps.date.outputs.date }}\n```\n\nKey: `ARG` values don't persist past the build stage they're defined in (use them before `FROM` for global or after `FROM` for stage-specific). Never put secrets in `ARG` \u2014 they appear in `docker history`. Use runtime `ENV` or Docker secrets for sensitive values.",
    "tags": [
      "docker",
      "dockerfile",
      "arg",
      "env",
      "ci-cd"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Next.js middleware for authentication and redirects",
    "context": "Need to protect certain routes in Next.js App Router \u2014 redirect unauthenticated users to /login, redirect logged-in users away from /login to dashboard. Doing this in each page component leads to flash of protected content.",
    "solution": "Use Next.js Middleware to intercept requests before they reach page components:\n\n```typescript\n// middleware.ts (at project root, next to app/)\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\n// Routes that require authentication\nconst PROTECTED_ROUTES = ['/dashboard', '/traces/new', '/settings'];\n// Routes that logged-in users shouldn't see\nconst AUTH_ROUTES = ['/login', '/signup'];\n\nexport function middleware(request: NextRequest) {\n  const token = request.cookies.get('session-token')?.value;\n  const pathname = request.nextUrl.pathname;\n\n  const isProtected = PROTECTED_ROUTES.some(route => pathname.startsWith(route));\n  const isAuthRoute = AUTH_ROUTES.some(route => pathname.startsWith(route));\n\n  // Redirect unauthenticated users from protected routes\n  if (isProtected && !token) {\n    const loginUrl = new URL('/login', request.url);\n    loginUrl.searchParams.set('redirect', pathname);\n    return NextResponse.redirect(loginUrl);\n  }\n\n  // Redirect authenticated users away from login/signup\n  if (isAuthRoute && token) {\n    const redirect = request.nextUrl.searchParams.get('redirect') ?? '/dashboard';\n    return NextResponse.redirect(new URL(redirect, request.url));\n  }\n\n  // Add auth header for API routes that need user context\n  if (pathname.startsWith('/api/') && token) {\n    const response = NextResponse.next();\n    response.headers.set('X-User-Token', token);\n    return response;\n  }\n\n  return NextResponse.next();\n}\n\n// Config: which paths middleware runs on (avoid static files)\nexport const config = {\n  matcher: [\n    '/((?!_next/static|_next/image|favicon.ico|public).*)',\n  ],\n};\n```\n\nMiddleware runs on the Edge runtime \u2014 no Node.js APIs, no database access. Use it only for routing decisions based on cookies/headers. For JWT validation, verify the token signature (edge-compatible JWT library like `jose`). Heavy auth logic (database lookups) goes in route handlers, not middleware.",
    "tags": [
      "nextjs",
      "middleware",
      "authentication",
      "routing"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript satisfies operator for type-safe object literals",
    "context": "Defining configuration objects or lookup maps with TypeScript. Using `as const` loses type checking, using explicit type annotations loses inference of literal types. Need both: type checking AND inference of exact literal values.",
    "solution": "Use the `satisfies` operator (TypeScript 4.9+) to get both type checking and literal type inference:\n\n```typescript\n// Problem: 'as const' loses type checking\nconst config = {\n  endpoint: 'https://api.example.com',\n  timeout: 'not-a-number',  // BUG: should be number, but no error\n} as const;\n\n// Problem: explicit type annotation loses literal types\ntype Config = { endpoint: string; timeout: number };\nconst config2: Config = {\n  endpoint: 'https://api.example.com',\n  timeout: 5000,\n};\n// config2.endpoint is typed as 'string', not 'https://api.example.com'\n\n// SOLUTION: satisfies\nconst config3 = {\n  endpoint: 'https://api.example.com',\n  timeout: 5000,\n} satisfies Config;\n// config3.endpoint is typed as 'https://api.example.com' (literal!)\n// config3.timeout is typed as 5000 (literal!)\n// AND: type checking is enforced at definition\n\n// Real use case: route configuration\ntype Route = {\n  path: string;\n  handler: string;\n  methods: ('GET' | 'POST' | 'PUT' | 'DELETE')[];\n  requiresAuth: boolean;\n};\n\nconst ROUTES = {\n  traces: {\n    path: '/api/v1/traces',\n    handler: 'tracesRouter',\n    methods: ['GET', 'POST'],\n    requiresAuth: true,\n  },\n  search: {\n    path: '/api/v1/traces/search',\n    handler: 'searchRouter',\n    methods: ['POST'],\n    requiresAuth: false,\n  },\n} satisfies Record<string, Route>;\n\n// TypeScript knows exact types:\n// ROUTES.traces.methods is ('GET' | 'POST')[], not ('GET'|'POST'|'PUT'|'DELETE')[]\n// ROUTES.search.requiresAuth is false, not boolean\n\n// Useful for CSS-in-JS / theme objects\ntype ThemeColors = Record<string, `#${string}` | `rgb(${string})` | 'transparent'>;\nconst colors = {\n  primary: '#3B82F6',\n  secondary: '#6B7280',\n  danger: '#EF4444',\n} satisfies ThemeColors;\n// colors.primary is typed as '#3B82F6', enables autocomplete and refactoring\n```\n\n`satisfies` validates the type without widening. Use it when you want the inferred literal type (for autocomplete, refactoring, conditional types) while still enforcing the structural type constraint.",
    "tags": [
      "typescript",
      "satisfies",
      "type-safety",
      "generics"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React Suspense and lazy loading for code splitting",
    "context": "Bundle size is large because all components load upfront. Some pages are rarely visited. Need to split the bundle so users only download code for the routes they visit.",
    "solution": "Use React.lazy with Suspense for route-based code splitting:\n\n```typescript\nimport { Suspense, lazy } from 'react';\nimport { Routes, Route } from 'react-router-dom';\n\n// Lazy load page components (each becomes a separate chunk)\nconst Dashboard = lazy(() => import('./pages/Dashboard'));\nconst TraceList = lazy(() => import('./pages/TraceList'));\nconst TraceDetail = lazy(() => import('./pages/TraceDetail'));\nconst Settings = lazy(() => import('./pages/Settings'));\n\n// Loading skeleton that matches the layout\nfunction PageSkeleton() {\n  return (\n    <div className=\"animate-pulse\">\n      <div className=\"h-8 bg-gray-200 rounded w-1/3 mb-4\" />\n      <div className=\"h-4 bg-gray-200 rounded w-2/3 mb-2\" />\n      <div className=\"h-4 bg-gray-200 rounded w-1/2\" />\n    </div>\n  );\n}\n\nfunction App() {\n  return (\n    <Suspense fallback={<PageSkeleton />}>\n      <Routes>\n        <Route path=\"/\" element={<Dashboard />} />\n        <Route path=\"/traces\" element={<TraceList />} />\n        <Route path=\"/traces/:id\" element={<TraceDetail />} />\n        <Route path=\"/settings\" element={<Settings />} />\n      </Routes>\n    </Suspense>\n  );\n}\n\n// Nested Suspense for granular loading states\nfunction TraceDetailPage({ id }: { id: string }) {\n  const LazyComments = lazy(() => import('./TraceComments'));\n\n  return (\n    <div>\n      <TraceHeader id={id} />\n      <Suspense fallback={<div>Loading comments...</div>}>\n        <LazyComments traceId={id} />\n      </Suspense>\n    </div>\n  );\n}\n\n// Preload on hover (prevents loading spinner on click)\nconst preloadDashboard = () => import('./pages/Dashboard');\n\nfunction NavLink({ to, label }: { to: string; label: string }) {\n  return (\n    <Link to={to} onMouseEnter={preloadDashboard}>\n      {label}\n    </Link>\n  );\n}\n```\n\n`React.lazy` only works with default exports. Each `lazy()` import creates a separate bundle chunk. `Suspense` must be an ancestor of lazy components. Place `Suspense` at the route level for page-level loading, and closer to the component for more granular boundaries.",
    "tags": [
      "react",
      "suspense",
      "code-splitting",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions secrets management with environments",
    "context": "CI/CD pipeline needs secrets (API keys, deployment credentials) for different environments. Using repository-level secrets means production keys are accessible in all workflows including untrusted PRs. Need secret scoping by environment.",
    "solution": "Use GitHub Environment secrets and Protection Rules to scope access:\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy-staging:\n    runs-on: ubuntu-latest\n    # Environment secrets: only available to this job\n    environment: staging\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to staging\n        run: ./deploy.sh\n        env:\n          # These only exist in the 'staging' environment\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          DEPLOY_KEY: ${{ secrets.STAGING_DEPLOY_KEY }}\n\n  deploy-production:\n    runs-on: ubuntu-latest\n    needs: deploy-staging\n    environment: production  # Has required reviewer + branch protection\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to production\n        env:\n          # Different secrets from 'production' environment\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}  # Different value\n          DEPLOY_KEY: ${{ secrets.PROD_DEPLOY_KEY }}\n        run: ./deploy.sh production\n\n  # Pull request jobs should NOT use environment secrets\n  test:\n    runs-on: ubuntu-latest\n    # No 'environment:' \u2014 only has repository-level secrets\n    steps:\n      - run: echo \"No production secrets available here\"\n```\n\n```bash\n# Set environment secrets via gh CLI\ngh secret set DATABASE_URL \\\n  --env staging \\\n  --body \"postgresql://user:pass@staging-host/db\"\n\ngh secret set DATABASE_URL \\\n  --env production \\\n  --body \"postgresql://user:prod-pass@prod-host/db\"\n\n# List secrets per environment\ngh secret list --env staging\ngh secret list --env production\n```\n\nEnvironment Protection Rules (configure in GitHub UI Settings > Environments):\n- Required reviewers: who must approve before the job runs\n- Deployment branches: only `main` can deploy to production\n- Wait timer: minimum delay before deployment (useful for production)\n\nPR workflows without `environment:` can only access repository-level secrets. Never put production database URLs in repository secrets.",
    "tags": [
      "github-actions",
      "secrets",
      "environments",
      "security"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Fly.io deployment configuration and scaling",
    "context": "Deploying a FastAPI application to Fly.io. Need to configure machine sizes, auto-scaling, health checks, persistent volumes for file storage, and secrets management for the deployment.",
    "solution": "Configure `fly.toml` for a FastAPI deployment with auto-scaling:\n\n```toml\n# fly.toml\napp = 'my-fastapi-app'\nprimary_region = 'ord'  # Chicago \u2014 pick closest to users\n\n[build]\n  dockerfile = 'Dockerfile'\n\n[env]\n  APP_ENV = 'production'\n  PORT = '8000'\n  # Non-secret config here\n\n[http_service]\n  internal_port = 8000\n  force_https = true\n  auto_stop_machines = true    # Stop when no traffic\n  auto_start_machines = true   # Start on request\n  min_machines_running = 0     # Can scale to zero (saves money)\n  processes = ['app']\n\n  [http_service.concurrency]\n    type = 'requests'\n    hard_limit = 100   # Max concurrent requests per machine\n    soft_limit = 80    # Start new machine when this is hit\n\n[[http_service.checks]]\n  grace_period = '10s'\n  interval = '15s'\n  method = 'GET'\n  path = '/health'\n  timeout = '5s'\n\n[mounts]\n  # Persistent storage for file uploads\n  source = 'uploads'\n  destination = '/app/uploads'\n\n[[vm]]\n  size = 'shared-cpu-1x'  # 256MB RAM \u2014 good for APIs\n  memory = '512mb'\n  cpu_kind = 'shared'\n  cpus = 1\n```\n\n```bash\n# Deploy\nfly deploy\n\n# Set secrets (encrypted at rest, injected as env vars)\nfly secrets set \\\n  DATABASE_URL=\"postgresql+asyncpg://user:pass@host/db\" \\\n  OPENAI_API_KEY=\"sk-...\" \\\n  REDIS_URL=\"redis://...\"\n\n# Scale machines manually\nfly scale count 2 --region ord\nfly scale vm performance-2x\n\n# View logs\nfly logs --app my-fastapi-app\n\n# Open postgres console\nfly postgres connect -a my-postgres-app\n\n# Create persistent volume\nfly volumes create uploads --region ord --size 10  # 10GB\n```\n\n`auto_stop_machines = true` with `min_machines_running = 0` enables scale-to-zero (free tier friendly). `soft_limit` triggers scale-out before hard limit is hit. Fly uses Anycast routing \u2014 your machines are globally distributed automatically.",
    "tags": [
      "fly-io",
      "deployment",
      "docker",
      "python",
      "ci-cd"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub API authentication and rate limiting in Python",
    "context": "Calling the GitHub REST API to fetch repository data, user information, and pull request details. Hitting rate limits (60 req/hour unauthenticated, 5000/hour with token). Need to handle pagination and rate limit headers.",
    "solution": "Use `httpx` with authentication headers and rate limit tracking:\n\n```python\nimport httpx\nimport asyncio\nfrom datetime import datetime\n\nclass GitHubClient:\n    BASE_URL = 'https://api.github.com'\n\n    def __init__(self, token: str):\n        self.client = httpx.AsyncClient(\n            base_url=self.BASE_URL,\n            headers={\n                'Authorization': f'Bearer {token}',\n                'Accept': 'application/vnd.github.v3+json',\n                'X-GitHub-Api-Version': '2022-11-28',\n            },\n            timeout=10.0,\n        )\n\n    async def _request(self, method: str, path: str, **kwargs) -> dict:\n        response = await self.client.request(method, path, **kwargs)\n\n        # Check rate limit\n        remaining = int(response.headers.get('X-RateLimit-Remaining', 1))\n        if remaining < 10:\n            reset_at = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait = max(0, reset_at - datetime.utcnow().timestamp())\n            print(f'Rate limit low ({remaining} remaining), waiting {wait:.0f}s')\n            await asyncio.sleep(wait + 1)\n\n        response.raise_for_status()\n        return response.json()\n\n    async def get_repo(self, owner: str, repo: str) -> dict:\n        return await self._request('GET', f'/repos/{owner}/{repo}')\n\n    async def list_prs(self, owner: str, repo: str, state: str = 'open') -> list[dict]:\n        all_prs = []\n        page = 1\n        while True:\n            data = await self._request(\n                'GET', f'/repos/{owner}/{repo}/pulls',\n                params={'state': state, 'per_page': 100, 'page': page}\n            )\n            if not data:\n                break\n            all_prs.extend(data)\n            page += 1\n        return all_prs\n\n    async def close(self):\n        await self.client.aclose()\n\n# Usage\nasync def main():\n    client = GitHubClient(token='ghp_...')\n    try:\n        repo = await client.get_repo('anthropics', 'anthropic-sdk-python')\n        print(f\"Stars: {repo['stargazers_count']}\")\n        prs = await client.list_prs('anthropics', 'anthropic-sdk-python')\n        print(f\"Open PRs: {len(prs)}\")\n    finally:\n        await client.close()\n```\n\nGitHub returns pagination links in the `Link` header (not in the body). The `X-RateLimit-Remaining` header is present on every response. Use GitHub Apps (not personal tokens) for production \u2014 5000+ req/hour per installation.",
    "tags": [
      "github",
      "api",
      "python",
      "httpx",
      "rate-limiting"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "S3-compatible object storage with boto3 in Python",
    "context": "Need to store and serve user-uploaded files (images, PDFs). Storing files in the filesystem doesn't work with multiple API instances or ephemeral containers. Need object storage that works with AWS S3, Cloudflare R2, or MinIO.",
    "solution": "Use boto3 with a class that works with any S3-compatible storage:\n\n```python\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom botocore.config import Config\nfrom pathlib import Path\nimport uuid\n\nclass ObjectStorage:\n    def __init__(\n        self,\n        bucket: str,\n        endpoint_url: str | None = None,  # None = AWS, set for R2/MinIO\n        access_key: str = '',\n        secret_key: str = '',\n        region: str = 'us-east-1',\n    ):\n        self.bucket = bucket\n        self.client = boto3.client(\n            's3',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=access_key,\n            aws_secret_access_key=secret_key,\n            region_name=region,\n            config=Config(\n                signature_version='s3v4',\n                retries={'max_attempts': 3, 'mode': 'adaptive'},\n            ),\n        )\n\n    def upload_file(self, file_data: bytes, content_type: str, prefix: str = '') -> str:\n        key = f'{prefix}/{uuid.uuid4()}.{content_type.split(\"/\")[1]}'\n        self.client.put_object(\n            Bucket=self.bucket,\n            Key=key,\n            Body=file_data,\n            ContentType=content_type,\n            # CacheControl='public, max-age=31536000',  # 1 year for immutable files\n        )\n        return key\n\n    def get_presigned_url(self, key: str, expires_in: int = 3600) -> str:\n        return self.client.generate_presigned_url(\n            'get_object',\n            Params={'Bucket': self.bucket, 'Key': key},\n            ExpiresIn=expires_in,\n        )\n\n    def delete_file(self, key: str) -> None:\n        self.client.delete_object(Bucket=self.bucket, Key=key)\n\n    def file_exists(self, key: str) -> bool:\n        try:\n            self.client.head_object(Bucket=self.bucket, Key=key)\n            return True\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                return False\n            raise\n\n# Config for different providers\n# AWS S3\nstorage = ObjectStorage(bucket='my-bucket', region='us-east-1')\n\n# Cloudflare R2 (S3-compatible, no egress fees)\nstorage = ObjectStorage(\n    bucket='my-bucket',\n    endpoint_url=f'https://{account_id}.r2.cloudflarestorage.com',\n    access_key=R2_ACCESS_KEY,\n    secret_key=R2_SECRET_KEY,\n)\n\n# MinIO (self-hosted)\nstorage = ObjectStorage(\n    bucket='my-bucket',\n    endpoint_url='http://minio:9000',\n    access_key='minioadmin',\n    secret_key='minioadmin',\n)\n```\n\nPresigned URLs let clients download directly from storage without proxying through your API. Cloudflare R2 has zero egress fees \u2014 ideal for high-bandwidth use cases. Use `multipart_upload` for files > 100MB.",
    "tags": [
      "s3",
      "storage",
      "python",
      "boto3",
      "file-upload"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Testcontainers pattern for PostgreSQL integration tests",
    "context": "Integration tests need a real PostgreSQL database but spinning up a persistent test database causes state pollution between test runs, requires manual setup, and breaks in CI without a running Postgres instance.",
    "solution": "Use testcontainers-python to spin up a real PostgreSQL container per test session:\n\n```python\n# tests/conftest.py\nimport pytest\nimport pytest_asyncio\nfrom testcontainers.postgres import PostgresContainer\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\nfrom app.models.base import Base\n\n@pytest.fixture(scope='session')\ndef postgres_container():\n    # Starts a real PostgreSQL container\n    with PostgresContainer('pgvector/pgvector:pg17') as container:\n        # Wait for container to be ready (handled by testcontainers)\n        yield container\n\n@pytest.fixture(scope='session')\ndef db_url(postgres_container):\n    # Get the connection URL (uses random ephemeral port)\n    return postgres_container.get_connection_url().replace(\n        'postgresql://', 'postgresql+asyncpg://'\n    )\n\n@pytest_asyncio.fixture(scope='session')\nasync def engine(db_url):\n    eng = create_async_engine(db_url, echo=False)\n    async with eng.begin() as conn:\n        # Create all tables including pgvector extension\n        await conn.execute(text('CREATE EXTENSION IF NOT EXISTS vector'))\n        await conn.run_sync(Base.metadata.create_all)\n    yield eng\n    await eng.dispose()\n\n@pytest_asyncio.fixture\nasync def session(engine):\n    # Each test gets a transaction that rolls back\n    async with engine.begin() as conn:\n        session_factory = async_sessionmaker(\n            bind=conn, expire_on_commit=False\n        )\n        async with session_factory() as session:\n            yield session\n            await conn.rollback()  # Rollback after each test\n\n# Usage\nasync def test_create_trace(session):\n    trace = Trace(\n        title='Test trace',\n        context_text='Context',\n        solution_text='Solution',\n        status='pending',\n    )\n    session.add(trace)\n    await session.flush()\n    assert trace.id is not None\n    # Rolled back automatically\n```\n\n```bash\n# Install\nuv add testcontainers --dev\n\n# Run tests (Docker must be running)\npytest tests/integration/\n```\n\nContainer starts once per session (`scope='session'`), tables are created once, each test rolls back. Requires Docker running locally and in CI. Add to GitHub Actions: `services:` with `docker` or use the `docker-in-docker` approach.",
    "tags": [
      "testing",
      "testcontainers",
      "postgresql",
      "pytest",
      "integration-tests"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python type narrowing with TypeGuard and isinstance",
    "context": "Working with union types and need TypeScript-style type guards in Python. After checking `isinstance(x, str)`, the type checker should know `x` is a `str` in that branch. Also need custom narrowing functions for complex types.",
    "solution": "Use `isinstance` for built-in types and `TypeGuard` for custom narrowing:\n\n```python\nfrom typing import TypeGuard, Union, Any\nfrom dataclasses import dataclass\n\n# Basic isinstance narrowing \u2014 mypy/pyright understand this automatically\ndef process_value(value: str | int | None) -> str:\n    if value is None:\n        return ''\n    if isinstance(value, int):\n        return str(value)  # value is int here\n    return value.upper()  # value is str here\n\n# TypeGuard for custom type predicates\n@dataclass\nclass ValidTrace:\n    id: str\n    title: str\n    trust_score: float\n\ndef is_valid_trace(obj: Any) -> TypeGuard[ValidTrace]:\n    return (\n        isinstance(obj, dict) and\n        isinstance(obj.get('id'), str) and\n        isinstance(obj.get('title'), str) and\n        isinstance(obj.get('trust_score'), float)\n    )\n\ndef process_api_response(data: Any) -> ValidTrace | None:\n    if is_valid_trace(data):\n        return data  # Type is narrowed to ValidTrace here\n    return None\n\n# Narrowing with literal types\nfrom typing import Literal\n\nStatus = Literal['pending', 'validated', 'rejected']\n\ndef is_status(value: str) -> TypeGuard[Status]:\n    return value in ('pending', 'validated', 'rejected')\n\ndef handle_status(raw: str) -> None:\n    if is_status(raw):\n        handle_trace_status(raw)  # raw is Status here\n\n# assert_never for exhaustive matching\nfrom typing import Never\n\ndef handle_event(event: Literal['created', 'updated', 'deleted']) -> str:\n    match event:\n        case 'created': return 'New item'\n        case 'updated': return 'Item changed'\n        case 'deleted': return 'Item removed'\n        case _ as unreachable:\n            # Type checker errors if any case is missed\n            assert_never(unreachable)\n\ndef assert_never(value: Never) -> Never:\n    raise AssertionError(f'Unhandled case: {value}')\n```\n\n`TypeGuard[T]` tells the type checker that when the function returns `True`, the argument is of type `T`. Without it, custom predicate functions don't narrow types. Works with mypy, pyright, and pyright-based editors.",
    "tags": [
      "python",
      "typing",
      "typeguard",
      "mypy",
      "type-safety"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pytest conftest.py patterns for shared fixtures",
    "context": "Test fixtures are duplicated across multiple test files. Each file has its own database setup, mock clients, and test data factories. Need a way to share fixtures across an entire test suite without importing from test files.",
    "solution": "Organize conftest.py files hierarchically \u2014 pytest discovers them automatically:\n\n```\ntests/\n  conftest.py           # Session-wide fixtures (DB engine, app client)\n  factories.py          # Test data factories\n  unit/\n    conftest.py         # Unit test specific fixtures (no DB)\n    test_tags.py\n  integration/\n    conftest.py         # Integration fixtures (with DB)\n    test_traces.py\n```\n\n```python\n# tests/factories.py \u2014 factory functions for test data\nimport uuid\nfrom datetime import datetime\n\ndef make_trace(**overrides) -> dict:\n    return {\n        'id': str(uuid.uuid4()),\n        'title': 'Test trace title',\n        'context_text': 'Test context text',\n        'solution_text': 'Test solution text',\n        'status': 'pending',\n        'trust_score': 0.0,\n        'is_seed': False,\n        'created_at': datetime.utcnow(),\n        **overrides\n    }\n\n# tests/conftest.py \u2014 top-level, available everywhere\nimport pytest\nimport pytest_asyncio\nfrom tests.factories import make_trace\n\n@pytest.fixture(scope='session')\ndef event_loop_policy():\n    # Ensure same event loop policy across session\n    import asyncio\n    return asyncio.DefaultEventLoopPolicy()\n\n@pytest_asyncio.fixture(scope='session')\nasync def app():\n    from app.main import app as fastapi_app\n    return fastapi_app\n\n# tests/integration/conftest.py \u2014 integration-specific\n@pytest_asyncio.fixture\nasync def trace_in_db(session, seed_user):\n    data = make_trace(contributor_id=str(seed_user.id))\n    trace = Trace(**{k: v for k, v in data.items() if k != 'id'})\n    session.add(trace)\n    await session.flush()\n    return trace\n\n@pytest_asyncio.fixture\nasync def validated_trace(session, seed_user):\n    trace = Trace(**make_trace(\n        contributor_id=str(seed_user.id),\n        status='validated',\n        trust_score=0.8,\n        confirmation_count=3,\n    ))\n    session.add(trace)\n    await session.flush()\n    return trace\n\n# tests/unit/conftest.py \u2014 no DB needed\n@pytest.fixture\ndef mock_redis(mocker):\n    return mocker.AsyncMock()\n```\n\nFixtures in `tests/conftest.py` are available to all tests. Fixtures in `tests/integration/conftest.py` only to `tests/integration/` tests. Use `pytest-factoryboy` or hand-rolled factories for test data. Never import from test files \u2014 put shared code in `conftest.py` or utility modules.",
    "tags": [
      "pytest",
      "conftest",
      "fixtures",
      "testing",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python context managers for resource cleanup",
    "context": "Resources like database connections, file handles, locks, and HTTP clients need cleanup even when exceptions occur. Using try/finally everywhere is verbose. Need the context manager pattern for both classes and simple functions.",
    "solution": "Implement context managers with `__enter__/__exit__` or `@contextmanager`:\n\n```python\nfrom contextlib import contextmanager, asynccontextmanager\nfrom typing import Generator, AsyncGenerator\n\n# Class-based context manager\nclass DatabaseTransaction:\n    def __init__(self, session):\n        self.session = session\n\n    async def __aenter__(self):\n        await self.session.begin()\n        return self.session\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is not None:\n            await self.session.rollback()\n        else:\n            await self.session.commit()\n        return False  # Don't suppress exceptions\n\n# Generator-based (simpler)\n@contextmanager\ndef timer(name: str) -> Generator[None, None, None]:\n    import time\n    start = time.perf_counter()\n    try:\n        yield\n    finally:\n        elapsed = time.perf_counter() - start\n        print(f'{name}: {elapsed:.3f}s')\n\n@asynccontextmanager\nasync def managed_http_client(base_url: str) -> AsyncGenerator[httpx.AsyncClient, None]:\n    client = httpx.AsyncClient(base_url=base_url, timeout=30.0)\n    try:\n        yield client\n    finally:\n        await client.aclose()\n\n# Nesting context managers\nasync def process_with_resources(data: dict) -> dict:\n    async with managed_http_client('https://api.example.com') as client:\n        async with DatabaseTransaction(session) as tx:\n            with timer('processing'):\n                result = await client.post('/process', json=data)\n                await tx.execute(insert(Log).values(data=data))\n                return result.json()\n\n# contextlib.ExitStack for dynamic context managers\nfrom contextlib import AsyncExitStack\n\nasync def open_multiple_clients(urls: list[str]):\n    async with AsyncExitStack() as stack:\n        clients = [\n            await stack.enter_async_context(managed_http_client(url))\n            for url in urls\n        ]\n        # All clients are open here\n        results = await asyncio.gather(*[c.get('/status') for c in clients])\n    # All clients closed here\n    return results\n```\n\n`__aexit__` returning `True` suppresses the exception; `False` or `None` re-raises it. `AsyncExitStack` is invaluable for dynamic resource management where the number of resources isn't known at compile time.",
    "tags": [
      "python",
      "context-managers",
      "asyncio",
      "resource-management"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI WebSocket implementation with connection management",
    "context": "Need real-time bidirectional communication between browser clients and the FastAPI server. REST/SSE only support server-to-client. Building a live collaboration feature where multiple users see updates simultaneously.",
    "solution": "Implement WebSocket endpoints with a connection manager for broadcast:\n\n```python\nfrom fastapi import APIRouter, WebSocket, WebSocketDisconnect\nfrom typing import set\nimport json\n\nrouter = APIRouter()\n\nclass ConnectionManager:\n    def __init__(self):\n        # Map room_id -> set of WebSocket connections\n        self.rooms: dict[str, set[WebSocket]] = {}\n\n    async def connect(self, websocket: WebSocket, room_id: str) -> None:\n        await websocket.accept()\n        self.rooms.setdefault(room_id, set()).add(websocket)\n\n    def disconnect(self, websocket: WebSocket, room_id: str) -> None:\n        if room_id in self.rooms:\n            self.rooms[room_id].discard(websocket)\n            if not self.rooms[room_id]:\n                del self.rooms[room_id]\n\n    async def broadcast(self, room_id: str, message: dict, exclude: WebSocket | None = None) -> None:\n        connections = self.rooms.get(room_id, set()).copy()\n        dead = set()\n        for ws in connections:\n            if ws is exclude:\n                continue\n            try:\n                await ws.send_json(message)\n            except Exception:\n                dead.add(ws)\n        for ws in dead:\n            self.disconnect(ws, room_id)\n\n    async def send_personal(self, websocket: WebSocket, message: dict) -> None:\n        await websocket.send_json(message)\n\nmanager = ConnectionManager()\n\n@router.websocket('/ws/{room_id}')\nasync def websocket_endpoint(\n    websocket: WebSocket,\n    room_id: str,\n):\n    await manager.connect(websocket, room_id)\n    try:\n        while True:\n            data = await websocket.receive_json()\n            # Broadcast to all others in the room\n            await manager.broadcast(room_id, {\n                'type': 'message',\n                'data': data,\n                'from': str(id(websocket)),\n            }, exclude=websocket)\n    except WebSocketDisconnect:\n        manager.disconnect(websocket, room_id)\n        await manager.broadcast(room_id, {\n            'type': 'user_left',\n            'client_id': str(id(websocket)),\n        })\n\n# JavaScript client\n# const ws = new WebSocket('ws://localhost:8000/ws/room-123');\n# ws.onmessage = (event) => console.log(JSON.parse(event.data));\n# ws.send(JSON.stringify({ text: 'Hello!' }));\n```\n\nFor production: this in-memory manager doesn't work across multiple API instances. Use Redis pub/sub to broadcast across instances. Always handle `WebSocketDisconnect` to clean up connections. Use `asyncio.create_task()` for concurrent send+receive.",
    "tags": [
      "fastapi",
      "websocket",
      "python",
      "real-time"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python Enum patterns for status fields",
    "context": "Using string literals for status values ('pending', 'validated', 'active') spread across the codebase. Typos cause silent bugs, IDEs can't autocomplete, and it's hard to find all places a status value is used.",
    "solution": "Use Python Enum (or StrEnum) for type-safe status values:\n\n```python\nfrom enum import Enum, StrEnum\n\n# StrEnum (Python 3.11+) \u2014 inherits from str, works in string contexts\nclass TraceStatus(StrEnum):\n    pending = 'pending'\n    validated = 'validated'\n\n# Older Python \u2014 use str mixin\nclass TraceStatus(str, Enum):\n    pending = 'pending'\n    validated = 'validated'\n\n# Usage\nstatus = TraceStatus.pending\nprint(status == 'pending')  # True \u2014 compares as string\nprint(status.value)         # 'pending'\nprint(str(status))          # 'pending'\n\n# SQLAlchemy: store as string, load as enum\nfrom sqlalchemy import String\nfrom sqlalchemy.orm import mapped_column, Mapped\n\nclass Trace(Base):\n    status: Mapped[str] = mapped_column(\n        String(20), default=TraceStatus.pending\n    )\n\n# Pydantic: validate string input as enum\nfrom pydantic import BaseModel\n\nclass TraceCreate(BaseModel):\n    status: TraceStatus = TraceStatus.pending\n\nrequest = TraceCreate(status='validated')  # Works\nrequest = TraceCreate(status='invalid')   # ValidationError\n\n# Pattern matching with enum\ndef handle_trace(status: TraceStatus) -> str:\n    match status:\n        case TraceStatus.pending:\n            return 'Waiting for votes'\n        case TraceStatus.validated:\n            return 'Accepted by community'\n        case _:\n            return 'Unknown status'\n\n# Enum with additional properties\nclass Priority(Enum):\n    low = 1\n    medium = 2\n    high = 3\n    critical = 4\n\n    @property\n    def is_urgent(self) -> bool:\n        return self.value >= 3\n\n    def __lt__(self, other: 'Priority') -> bool:\n        return self.value < other.value\n```\n\n`StrEnum` values compare equal to their string representation \u2014 safe to use in f-strings, dict keys, and JSON without `.value`. Add enum values to `__all__` in the module so they're importable at the top level.",
    "tags": [
      "python",
      "enum",
      "type-safety",
      "patterns"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python logging with structlog in production",
    "context": "Using Python's standard logging module but log output is unstructured text that's hard to search in log aggregation tools (Datadog, Grafana Loki, CloudWatch). Need structured JSON logs with consistent fields like request_id, user_id, duration.",
    "solution": "Configure structlog for structured JSON output with context binding:\n\n```python\n# app/logging.py\nimport structlog\nimport logging\nimport sys\n\ndef configure_logging(debug: bool = False) -> None:\n    shared_processors = [\n        structlog.contextvars.merge_contextvars,  # Merge bound context\n        structlog.processors.add_log_level,\n        structlog.processors.TimeStamper(fmt='iso', utc=True),\n        structlog.stdlib.add_logger_name,\n    ]\n\n    if debug:\n        # Human-readable in development\n        processors = shared_processors + [\n            structlog.dev.ConsoleRenderer()\n        ]\n    else:\n        # JSON in production\n        processors = shared_processors + [\n            structlog.processors.dict_tracebacks,\n            structlog.processors.JSONRenderer(),\n        ]\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(logging.DEBUG if debug else logging.INFO),\n        context_class=dict,\n        logger_factory=structlog.PrintLoggerFactory(file=sys.stdout),\n        cache_logger_on_first_use=True,\n    )\n\n# Usage in FastAPI middleware \u2014 bind request context\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport uuid\n\nclass RequestLoggingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        request_id = str(uuid.uuid4())[:8]\n        structlog.contextvars.clear_contextvars()\n        structlog.contextvars.bind_contextvars(\n            request_id=request_id,\n            method=request.method,\n            path=request.url.path,\n        )\n\n        logger = structlog.get_logger()\n        logger.info('request_started')\n        response = await call_next(request)\n        logger.info('request_completed', status_code=response.status_code)\n        return response\n\n# In route handlers\nlogger = structlog.get_logger()\n\nasync def create_trace(trace: TraceCreate, user: User) -> Trace:\n    logger.info('creating_trace', user_id=str(user.id), title=trace.title)\n    result = await db.insert(trace)\n    logger.info('trace_created', trace_id=str(result.id))\n    return result\n```\n\n`merge_contextvars` automatically includes context bound via `bind_contextvars()` in every log line \u2014 no need to pass logger/context around. Production JSON logs: `{\"event\": \"request_completed\", \"status_code\": 200, \"request_id\": \"abc123\", \"timestamp\": \"2024-01-01T...\"}`.",
    "tags": [
      "python",
      "structlog",
      "logging",
      "fastapi",
      "observability"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pgvector cosine similarity search with SQLAlchemy",
    "context": "Storing OpenAI embedding vectors in PostgreSQL with pgvector. Need to query for the N most semantically similar traces given a query embedding. Query must filter by tags and status before vector ranking to avoid full-table scans.",
    "solution": "Use pgvector's cosine distance operator with SQLAlchemy and filter before ranking:\n\n```python\nfrom pgvector.sqlalchemy import Vector\nfrom sqlalchemy import select, func, and_, Float\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\n# Model definition\nclass Trace(Base):\n    __tablename__ = 'traces'\n    embedding: Mapped[Optional[list[float]]] = mapped_column(\n        Vector(1536), nullable=True\n    )\n\n# Search function\nasync def semantic_search(\n    session: AsyncSession,\n    query_embedding: list[float],\n    tags: list[str] | None = None,\n    limit: int = 20,\n    ann_limit: int = 100,  # Over-fetch for re-ranking\n) -> list[Trace]:\n    # Cosine distance (1 - cosine_similarity), lower is more similar\n    cosine_dist = Trace.embedding.cosine_distance(query_embedding)\n\n    stmt = (\n        select(\n            Trace,\n            cosine_dist.label('similarity_distance'),\n        )\n        .where(\n            and_(\n                Trace.status == 'validated',\n                Trace.embedding.is_not(None),  # Only embedded traces\n            )\n        )\n        .order_by(cosine_dist)  # Ascending: smaller distance = more similar\n        .limit(ann_limit)  # Over-fetch for re-ranking by trust score\n    )\n\n    # Optional tag filter\n    if tags:\n        stmt = stmt.join(Trace.tags).where(\n            Tag.name.in_(tags)\n        ).group_by(Trace.id).having(\n            func.count(Tag.id) > 0\n        )\n\n    result = await session.execute(stmt)\n    rows = result.all()\n\n    # Re-rank by combining similarity and trust score\n    def combined_score(row) -> float:\n        similarity = 1 - row.similarity_distance  # Convert distance to similarity\n        return 0.7 * similarity + 0.3 * row.Trace.trust_score\n\n    ranked = sorted(rows, key=combined_score, reverse=True)\n    return [row.Trace for row in ranked[:limit]]\n\n# HNSW index for fast approximate nearest neighbor\n# CREATE INDEX ON traces USING hnsw (embedding vector_cosine_ops)\n# WITH (m = 16, ef_construction = 64);\n```\n\nOver-fetch (`ann_limit=100`) then re-rank allows combining vector similarity with domain-specific scores (trust, recency). HNSW index makes vector search O(log N) instead of O(N). `cosine_distance` returns values in [0, 2]; 0 = identical, 2 = opposite.",
    "tags": [
      "postgresql",
      "pgvector",
      "semantic-search",
      "sqlalchemy",
      "embeddings"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "FastAPI middleware for request tracing and metrics",
    "context": "Need to track request latency, status code distribution, and active request counts across all endpoints. Want Prometheus metrics that work with Grafana dashboards, without duplicating instrumentation in every route handler.",
    "solution": "Add Starlette middleware that instruments all requests with Prometheus counters and histograms:\n\n```python\n# app/middleware/metrics.py\nimport time\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST\nfrom fastapi import FastAPI\nfrom fastapi.responses import PlainTextResponse\n\n# Metrics (defined at module level \u2014 registered once)\nHTTP_REQUESTS_TOTAL = Counter(\n    'http_requests_total',\n    'Total HTTP requests',\n    ['method', 'endpoint', 'status_code']\n)\nHTTP_REQUEST_DURATION = Histogram(\n    'http_request_duration_seconds',\n    'HTTP request duration',\n    ['method', 'endpoint'],\n    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],\n)\nHTTP_REQUESTS_IN_FLIGHT = Gauge(\n    'http_requests_in_flight',\n    'HTTP requests currently being processed',\n)\n\nclass MetricsMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next) -> Response:\n        # Skip metrics endpoint itself\n        if request.url.path == '/metrics':\n            return await call_next(request)\n\n        # Normalize path to avoid high cardinality (e.g., /traces/uuid)\n        path = self._normalize_path(request.url.path)\n        method = request.method\n\n        HTTP_REQUESTS_IN_FLIGHT.inc()\n        start = time.perf_counter()\n\n        try:\n            response = await call_next(request)\n            status = response.status_code\n        except Exception:\n            status = 500\n            raise\n        finally:\n            duration = time.perf_counter() - start\n            HTTP_REQUESTS_IN_FLIGHT.dec()\n            HTTP_REQUEST_DURATION.labels(method=method, endpoint=path).observe(duration)\n            HTTP_REQUESTS_TOTAL.labels(method=method, endpoint=path, status_code=status).inc()\n\n        return response\n\n    def _normalize_path(self, path: str) -> str:\n        # Replace UUIDs with {id} to reduce cardinality\n        import re\n        path = re.sub(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', '{id}', path)\n        return path\n\n# app/main.py\napp = FastAPI()\napp.add_middleware(MetricsMiddleware)\n\n@app.get('/metrics')\nasync def metrics():\n    return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n```\n\nPath normalization prevents cardinality explosion from UUID-containing paths. Place `MetricsMiddleware` before other middleware so it measures total request time. Histograms are more useful than Averages for latency (p95, p99 percentiles).",
    "tags": [
      "fastapi",
      "prometheus",
      "metrics",
      "observability",
      "middleware"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pydantic settings with multiple env sources and precedence",
    "context": "Application needs configuration from multiple sources: environment variables override .env file, which overrides defaults. Also need nested settings objects (database config, Redis config) and type coercion from string env vars to typed Python objects.",
    "solution": "Use pydantic-settings with customized model_config and nested models:\n\n```python\nfrom pydantic import Field, SecretStr, AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass DatabaseSettings(BaseSettings):\n    host: str = 'localhost'\n    port: int = 5432\n    name: str = 'myapp'\n    user: str = 'postgres'\n    password: SecretStr = SecretStr('')\n    pool_size: int = 10\n    max_overflow: int = 20\n\n    @property\n    def url(self) -> str:\n        pwd = self.password.get_secret_value()\n        return f'postgresql+asyncpg://{self.user}:{pwd}@{self.host}:{self.port}/{self.name}'\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file='.env',\n        env_file_encoding='utf-8',\n        env_nested_delimiter='__',  # DB__HOST=localhost -> database.host\n        case_sensitive=False,\n    )\n\n    app_name: str = 'MyApp'\n    debug: bool = False\n    secret_key: SecretStr = Field(default=..., description='JWT secret key')\n    allowed_hosts: list[str] = ['localhost']\n\n    # Nested settings\n    database: DatabaseSettings = DatabaseSettings()\n\n    # Validator for derived values\n    @property\n    def is_production(self) -> bool:\n        return not self.debug\n\n# .env file\n# SECRET_KEY=my-secret-key-here\n# DB__HOST=prod-postgres.example.com\n# DB__PASSWORD=super-secret-password\n# ALLOWED_HOSTS=[\"api.example.com\",\"www.example.com\"]\n\nsettings = Settings()\nprint(settings.database.url)\nprint(settings.secret_key.get_secret_value())  # Access secret\n```\n\n`env_nested_delimiter='__'` allows `DB__HOST` to set `database.host`. `SecretStr` prevents accidental logging \u2014 `.get_secret_value()` is the only way to access it. `list[str]` env vars accept JSON-encoded arrays: `[\"a\",\"b\"]`.",
    "tags": [
      "pydantic",
      "settings",
      "configuration",
      "python",
      "environment-variables"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy relationship cascade and orphan deletion",
    "context": "Deleting a parent record (User) should automatically delete child records (Traces, Votes). Without proper cascade configuration, SQLAlchemy either raises a foreign key constraint error or leaves orphaned rows in the database.",
    "solution": "Configure cascade on the parent relationship and add ON DELETE CASCADE at the DB level:\n\n```python\nfrom sqlalchemy import ForeignKey, String\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\n\nclass User(Base):\n    __tablename__ = 'users'\n    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)\n    email: Mapped[str] = mapped_column(String(255), unique=True)\n\n    # cascade='all, delete-orphan': when User is deleted, delete all Traces\n    # passive_deletes=True: let the DB handle deletion via ON DELETE CASCADE\n    traces: Mapped[list['Trace']] = relationship(\n        'Trace',\n        back_populates='contributor',\n        cascade='all, delete-orphan',\n        passive_deletes=True,  # Don't load children to delete them\n    )\n\nclass Trace(Base):\n    __tablename__ = 'traces'\n    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)\n    contributor_id: Mapped[UUID] = mapped_column(\n        ForeignKey('users.id', ondelete='CASCADE')  # DB-level cascade\n    )\n    contributor: Mapped[User] = relationship('User', back_populates='traces')\n\n# Alembic migration\ndef upgrade() -> None:\n    op.create_table('traces',\n        sa.Column('id', postgresql.UUID(), nullable=False),\n        sa.Column('contributor_id', postgresql.UUID(), nullable=False),\n        sa.ForeignKeyConstraint(\n            ['contributor_id'], ['users.id'],\n            ondelete='CASCADE'  # Critical: must be on the FK constraint\n        ),\n        sa.PrimaryKeyConstraint('id'),\n    )\n\n# Usage\nasync def delete_user(user_id: str, session: AsyncSession) -> None:\n    user = await session.get(User, user_id)\n    await session.delete(user)  # Cascades to all traces\n    await session.commit()\n```\n\n`passive_deletes=True` tells SQLAlchemy to not load and delete children in Python \u2014 let the DB handle it via `ON DELETE CASCADE`. Without `passive_deletes`, SQLAlchemy issues SELECT then DELETE for each child. Both ORM cascade AND DB-level `ondelete='CASCADE'` are needed for correct behavior.",
    "tags": [
      "sqlalchemy",
      "postgresql",
      "cascade",
      "orm",
      "database"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript mapped types and template literal types",
    "context": "Building a type-safe event system where events are named 'resource:action' (e.g., 'trace:created', 'user:updated'). Need TypeScript to enforce valid event names and infer payload types from event names automatically.",
    "solution": "Use template literal types and mapped types for a type-safe event system:\n\n```typescript\n// Define resources and their actions\ntype TraceEvents = {\n  'trace:created': { traceId: string; title: string };\n  'trace:validated': { traceId: string; trustScore: number };\n  'trace:deleted': { traceId: string };\n};\n\ntype UserEvents = {\n  'user:registered': { userId: string; email: string };\n  'user:promoted': { userId: string; oldScore: number; newScore: number };\n};\n\n// Merge all events into one type\ntype AppEvents = TraceEvents & UserEvents;\n\n// Event name is a key of AppEvents\ntype EventName = keyof AppEvents;\n\n// Payload type is inferred from event name\ntype EventPayload<T extends EventName> = AppEvents[T];\n\n// Type-safe event emitter\nclass EventBus {\n  private handlers: { [K in EventName]?: ((payload: AppEvents[K]) => void)[] } = {};\n\n  on<T extends EventName>(event: T, handler: (payload: EventPayload<T>) => void): void {\n    (this.handlers[event] ??= []).push(handler as any);\n  }\n\n  emit<T extends EventName>(event: T, payload: EventPayload<T>): void {\n    this.handlers[event]?.forEach(h => h(payload as any));\n  }\n}\n\nconst bus = new EventBus();\n\n// TypeScript enforces correct payload types\nbus.on('trace:created', ({ traceId, title }) => {\n  // traceId: string, title: string \u2014 correctly inferred\n  console.log(`New trace: ${title}`);\n});\n\nbus.emit('trace:created', { traceId: '123', title: 'Test' });  // OK\nbus.emit('trace:created', { traceId: '123', score: 5 });       // TypeScript error!\n\n// Template literal types for CSS-like APIs\ntype Spacing = 0 | 1 | 2 | 4 | 8 | 16;\ntype SpacingProp = `p${'' | 'x' | 'y' | 't' | 'r' | 'b' | 'l'}-${Spacing}`;\n\n// Mapped type: make all properties optional with undefined\ntype PartialUndefined<T> = { [K in keyof T]?: T[K] | undefined };\n\n// Mapped type: prefix all keys\ntype Prefixed<T, P extends string> = { [K in keyof T as `${P}${string & K}`]: T[K] };\ntype PrefixedTraceFields = Prefixed<{ id: string; title: string }, 'trace_'>;\n// = { trace_id: string; trace_title: string }\n```\n\nTemplate literal types (`\\`${P}${K}\\``) enable precise string pattern types. Mapped types with `as` rename keys. The `[K in EventName]?` pattern with different value types per key is called a homomorphic mapped type.",
    "tags": [
      "typescript",
      "mapped-types",
      "template-literal-types",
      "generics"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Async Python job queue with ARQ and Redis",
    "context": "Background tasks are blocking the API event loop \u2014 image processing, email sending, report generation take seconds. FastAPI BackgroundTasks run in the same event loop. Need a proper job queue that runs workers separately and survives API restarts.",
    "solution": "Use ARQ (async Redis Queue) for background job processing:\n\n```python\n# app/worker.py\nimport asyncio\nfrom arq import cron\nfrom arq.connections import RedisSettings\nfrom app.config import settings\n\n# Job functions\nasync def embed_traces(ctx: dict, trace_ids: list[str]) -> int:\n    session = ctx['session']\n    embedded = 0\n    for trace_id in trace_ids:\n        trace = await session.get(Trace, trace_id)\n        if trace and trace.embedding is None:\n            trace.embedding = await embed_text(trace.context_text + ' ' + trace.solution_text)\n            embedded += 1\n    await session.commit()\n    return embedded\n\nasync def send_validation_email(ctx: dict, user_id: str, trace_id: str) -> None:\n    user = await ctx['session'].get(User, user_id)\n    await ctx['email_service'].send_validation_notification(user.email, trace_id)\n\n# Worker settings\nclass WorkerSettings:\n    functions = [embed_traces, send_validation_email]\n    redis_settings = RedisSettings.from_dsn(settings.redis_url)\n    max_jobs = 10\n    job_timeout = 300  # 5 minutes max per job\n    keep_result = 86400  # Keep results for 1 day\n\n    # Periodic jobs (cron)\n    cron_jobs = [\n        cron(embed_traces, minute={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55}),\n    ]\n\n    async def on_startup(ctx: dict) -> None:\n        ctx['session'] = async_sessionmaker(engine)()\n        ctx['email_service'] = EmailService()\n\n    async def on_shutdown(ctx: dict) -> None:\n        await ctx['session'].close()\n\n# Enqueue jobs from the API\nfrom arq import create_pool\nfrom arq.connections import RedisSettings\n\nasync def get_arq_pool(request: Request) -> ArqRedis:\n    return request.app.state.arq\n\n# In FastAPI lifespan\nasync def lifespan(app: FastAPI):\n    app.state.arq = await create_pool(RedisSettings.from_dsn(settings.redis_url))\n    yield\n    await app.state.arq.close()\n\n# Enqueue from route handler\n@router.post('/traces/{trace_id}/process')\nasync def process_trace(\n    trace_id: str,\n    arq: ArqRedis = Depends(get_arq_pool),\n):\n    job = await arq.enqueue_job('embed_traces', [trace_id])\n    return {'job_id': job.job_id}\n```\n\n```bash\n# Run worker\npython -m arq app.worker.WorkerSettings\n```\n\nARQ uses Redis as the broker \u2014 jobs survive API restarts. Workers run in a separate process from the API. `cron` decorator schedules periodic tasks. `ctx` dict is passed to every job and populated in `on_startup`.",
    "tags": [
      "python",
      "arq",
      "redis",
      "background-tasks",
      "async-queue"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Nginx proxy configuration for FastAPI with WebSockets",
    "context": "FastAPI application runs behind Nginx as a reverse proxy. Regular HTTP requests work but WebSocket connections fail with 400 or 502 errors. Need Nginx configured to properly proxy WebSocket upgrade requests.",
    "solution": "Add the required upgrade headers to the Nginx location block for WebSocket support:\n\n```nginx\n# /etc/nginx/conf.d/default.conf\n\n# Shared proxy settings\nmap $http_upgrade $connection_upgrade {\n    default upgrade;\n    '' close;\n}\n\nupstream fastapi_backend {\n    server api:8000;  # Docker service name\n    keepalive 32;     # Persistent connections to upstream\n}\n\nserver {\n    listen 80;\n    server_name api.example.com;\n\n    # Redirect HTTP to HTTPS\n    return 301 https://$host$request_uri;\n}\n\nserver {\n    listen 443 ssl;\n    server_name api.example.com;\n\n    ssl_certificate /etc/letsencrypt/live/api.example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/api.example.com/privkey.pem;\n\n    # Regular HTTP API\n    location /api/ {\n        proxy_pass http://fastapi_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_http_version 1.1;\n        proxy_read_timeout 30s;\n    }\n\n    # WebSocket endpoint\n    location /ws/ {\n        proxy_pass http://fastapi_backend;\n        proxy_http_version 1.1;\n        # Critical: these headers enable WebSocket upgrade\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        # Longer timeout for persistent WebSocket connections\n        proxy_read_timeout 3600s;\n        proxy_send_timeout 3600s;\n    }\n\n    # Server-Sent Events\n    location /events/ {\n        proxy_pass http://fastapi_backend;\n        proxy_http_version 1.1;\n        proxy_set_header Connection '';\n        proxy_cache off;\n        proxy_buffering off;  # Critical: disable buffering for SSE\n        proxy_read_timeout 3600s;\n        add_header X-Accel-Buffering no;\n    }\n}\n```\n\nThe `map` block dynamically sets `Connection: upgrade` only when an `Upgrade` header is present. Without `proxy_http_version 1.1`, keepalive and WebSocket upgrades don't work. For SSE, `proxy_buffering off` is mandatory \u2014 buffering delays event delivery.",
    "tags": [
      "nginx",
      "websocket",
      "fastapi",
      "proxy",
      "configuration"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "React form validation with react-hook-form and Zod",
    "context": "Building a complex form with nested fields, cross-field validation, and async validation (e.g., check if username is taken). Using controlled components with useState for each field is cumbersome and unperformant.",
    "solution": "Use react-hook-form with Zod resolver for type-safe form validation:\n\n```typescript\nimport { useForm } from 'react-hook-form';\nimport { zodResolver } from '@hookform/resolvers/zod';\nimport { z } from 'zod';\n\n// Define schema with Zod\nconst traceSchema = z.object({\n  title: z.string()\n    .min(10, 'Title must be at least 10 characters')\n    .max(200, 'Title is too long'),\n  context: z.string()\n    .min(20, 'Context must be at least 20 characters'),\n  solution: z.string()\n    .min(50, 'Solution must be at least 50 characters'),\n  tags: z.array(z.string())\n    .min(1, 'Add at least one tag')\n    .max(5, 'Maximum 5 tags'),\n  agentModel: z.enum(['claude-opus-4-6', 'gpt-4o', 'gemini-pro']),\n});\n\n// Infer TypeScript type from schema\ntype TraceForm = z.infer<typeof traceSchema>;\n\nexport function CreateTraceForm() {\n  const {\n    register,\n    handleSubmit,\n    formState: { errors, isSubmitting },\n    setError,\n    watch,\n  } = useForm<TraceForm>({\n    resolver: zodResolver(traceSchema),\n    defaultValues: {\n      agentModel: 'claude-opus-4-6',\n      tags: [],\n    },\n  });\n\n  const onSubmit = async (data: TraceForm) => {\n    try {\n      await api.createTrace(data);\n    } catch (err) {\n      if (err instanceof ApiError && err.status === 409) {\n        setError('title', { message: 'A trace with this title already exists' });\n      }\n    }\n  };\n\n  return (\n    <form onSubmit={handleSubmit(onSubmit)}>\n      <div>\n        <input {...register('title')} placeholder=\"Descriptive title\" />\n        {errors.title && <p className=\"error\">{errors.title.message}</p>}\n      </div>\n\n      <div>\n        <textarea {...register('context')} rows={4} />\n        {errors.context && <p className=\"error\">{errors.context.message}</p>}\n      </div>\n\n      <div>\n        <select {...register('agentModel')}>\n          <option value=\"claude-opus-4-6\">Claude Opus 4.6</option>\n          <option value=\"gpt-4o\">GPT-4o</option>\n        </select>\n      </div>\n\n      <button type=\"submit\" disabled={isSubmitting}>\n        {isSubmitting ? 'Submitting...' : 'Create Trace'}\n      </button>\n    </form>\n  );\n}\n```\n\n`zodResolver` integrates Zod validation with react-hook-form. `register()` attaches ref and onChange/onBlur handlers \u2014 no controlled state needed. `setError` allows server-side errors to display inline. `watch()` lets you observe field values for dependent validation.",
    "tags": [
      "react",
      "react-hook-form",
      "zod",
      "forms",
      "typescript"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python abstract base classes for service interfaces",
    "context": "Multiple service implementations (email: Resend, SendGrid; storage: S3, R2, local) need to be swappable. Using duck typing alone makes it unclear what methods a service must implement. Need formal interface contracts.",
    "solution": "Use `abc.ABC` and `abstractmethod` to define required interfaces:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import BinaryIO\n\nclass EmailService(ABC):\n    @abstractmethod\n    async def send(\n        self,\n        to: str,\n        subject: str,\n        html: str,\n        from_address: str = 'noreply@commontrace.dev',\n    ) -> None:\n        \"\"\"Send an email. Raises EmailDeliveryError on failure.\"\"\"\n        ...\n\n    @abstractmethod\n    async def send_batch(self, messages: list[dict]) -> list[str]:\n        \"\"\"Send multiple emails. Returns list of message IDs.\"\"\"\n        ...\n\nclass StorageService(ABC):\n    @abstractmethod\n    async def upload(\n        self, key: str, data: bytes | BinaryIO, content_type: str\n    ) -> str:\n        \"\"\"Upload file, return public URL.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete(self, key: str) -> None: ...\n\n    @abstractmethod\n    async def get_url(self, key: str, expires_in: int = 3600) -> str: ...\n\n# Concrete implementations\nclass ResendEmailService(EmailService):\n    def __init__(self, api_key: str):\n        import resend\n        resend.api_key = api_key\n        self._resend = resend\n\n    async def send(self, to: str, subject: str, html: str, from_address: str = 'noreply@commontrace.dev') -> None:\n        self._resend.Emails.send({'from': from_address, 'to': [to], 'subject': subject, 'html': html})\n\n    async def send_batch(self, messages: list[dict]) -> list[str]:\n        results = self._resend.Emails.send_batch(messages)\n        return [r['id'] for r in results]\n\nclass LocalStorageService(StorageService):\n    \"\"\"File system storage for development.\"\"\"\n    def __init__(self, base_dir: str = '/tmp/uploads'):\n        from pathlib import Path\n        self.base_dir = Path(base_dir)\n        self.base_dir.mkdir(exist_ok=True)\n\n    async def upload(self, key: str, data: bytes | BinaryIO, content_type: str) -> str:\n        path = self.base_dir / key\n        path.parent.mkdir(parents=True, exist_ok=True)\n        content = data if isinstance(data, bytes) else data.read()\n        path.write_bytes(content)\n        return f'/uploads/{key}'\n\n    async def delete(self, key: str) -> None:\n        (self.base_dir / key).unlink(missing_ok=True)\n\n    async def get_url(self, key: str, expires_in: int = 3600) -> str:\n        return f'/uploads/{key}'  # No expiry for local\n\n# Cannot instantiate ABC directly\ntry:\n    svc = EmailService()  # TypeError: Can't instantiate abstract class\nexcept TypeError as e:\n    print(e)\n```\n\nABC raises `TypeError` at instantiation time if any `@abstractmethod` is unimplemented \u2014 catches missing methods early. Combine with `Protocol` when you need structural subtyping without inheritance (e.g., for third-party classes you can't subclass).",
    "tags": [
      "python",
      "abc",
      "abstract-classes",
      "interfaces",
      "patterns"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Next.js incremental static regeneration (ISR) for dynamic content",
    "context": "Blog posts and documentation pages are generated server-side on every request (SSR), causing slow TTFB. Content changes infrequently. Need static generation benefits (CDN caching, fast delivery) with the ability to update content without full rebuilds.",
    "solution": "Use Next.js ISR to statically generate pages that revalidate in the background:\n\n```typescript\n// app/traces/[id]/page.tsx\nimport { notFound } from 'next/navigation';\n\n// Generate static paths at build time\nexport async function generateStaticParams() {\n  // Pre-build the 100 most popular traces\n  const traces = await api.getTopTraces({ limit: 100 });\n  return traces.map(t => ({ id: t.id }));\n}\n\ninterface Props {\n  params: { id: string };\n}\n\n// Page component \u2014 statically generated + revalidated\nexport default async function TracePage({ params }: Props) {\n  const trace = await fetch(\n    `${process.env.API_URL}/api/v1/traces/${params.id}`,\n    {\n      next: { revalidate: 300 },  // Revalidate every 5 minutes\n    }\n  ).then(r => r.ok ? r.json() : null);\n\n  if (!trace) notFound();\n\n  return <TraceDetail trace={trace} />;\n}\n\n// On-demand revalidation from API route\n// app/api/revalidate/route.ts\nimport { revalidatePath, revalidateTag } from 'next/cache';\nimport { NextRequest } from 'next/server';\n\nexport async function POST(request: NextRequest) {\n  const secret = request.nextUrl.searchParams.get('secret');\n  if (secret !== process.env.REVALIDATE_SECRET) {\n    return Response.json({ error: 'Invalid token' }, { status: 401 });\n  }\n\n  const { path, tag } = await request.json();\n\n  if (path) revalidatePath(path);\n  if (tag) revalidateTag(tag);\n\n  return Response.json({ revalidated: true });\n}\n\n// Tag-based revalidation (revalidate all pages fetching 'traces')\nasync function getTrace(id: string) {\n  return fetch(`${process.env.API_URL}/api/v1/traces/${id}`, {\n    next: { tags: ['traces', `trace-${id}`] }\n  }).then(r => r.json());\n}\n\n// Trigger revalidation from your backend when a trace is updated:\n// POST /api/revalidate?secret=xxx  {\"tag\": \"trace-{id}\"}\n```\n\nISR serves stale content immediately (fast) then revalidates in the background. `revalidate: 300` means pages rebuild at most every 5 minutes. On-demand revalidation via `revalidateTag` lets you purge specific pages when data changes (webhook-based invalidation).",
    "tags": [
      "nextjs",
      "isr",
      "static-generation",
      "caching",
      "performance"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Database connection pooling configuration for production",
    "context": "FastAPI application with SQLAlchemy has database connection issues under load: timeouts, 'too many connections' PostgreSQL errors, and connection leaks. Default connection pool settings are not tuned for production traffic.",
    "solution": "Configure `create_async_engine` pool settings to match PostgreSQL limits and traffic patterns:\n\n```python\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\nfrom sqlalchemy.pool import AsyncAdaptedQueuePool\n\ndef create_engine(database_url: str) -> AsyncEngine:\n    return create_async_engine(\n        database_url,\n        # Pool configuration\n        poolclass=AsyncAdaptedQueuePool,\n        pool_size=10,           # Persistent connections (tune: cpu_count * 2)\n        max_overflow=20,        # Extra connections under high load\n        pool_timeout=30,        # Wait up to 30s for a connection\n        pool_recycle=3600,      # Recycle connections every hour (avoids stale connections)\n        pool_pre_ping=True,     # Test connection before use (catches dropped connections)\n        # Performance\n        echo=False,             # Set True for query logging in dev\n        connect_args={\n            'command_timeout': 30,      # Query timeout (asyncpg)\n            'server_settings': {\n                'application_name': 'commontrace-api',\n                'statement_timeout': '30000',  # 30s in milliseconds\n            }\n        },\n    )\n\n# Lifespan management\nasync def lifespan(app: FastAPI):\n    engine = create_engine(settings.database_url)\n    session_factory = async_sessionmaker(\n        engine,\n        expire_on_commit=False,\n        autoflush=False,\n    )\n    app.state.engine = engine\n    app.state.session_factory = session_factory\n    yield\n    await engine.dispose()  # Close all connections on shutdown\n\n# Check pool status\nfrom sqlalchemy import event\n\n@event.listens_for(engine.sync_engine, 'connect')\ndef connect(dbapi_connection, connection_record):\n    logger.info('db_connection_created')\n\n@event.listens_for(engine.sync_engine, 'checkout')\ndef checkout(dbapi_connection, connection_record, connection_proxy):\n    pool_status = engine.pool.status()\n    if engine.pool.checkedout() > pool_size * 0.8:\n        logger.warning('connection_pool_high', status=pool_status)\n```\n\nRule of thumb: `pool_size` = number of CPU cores * 2. `max_overflow` = 2x `pool_size`. Total max connections = `pool_size + max_overflow` \u2014 must be less than PostgreSQL's `max_connections` (default 100). Set `pool_recycle` lower (300-600s) if load balancers or firewalls have idle connection timeouts.",
    "tags": [
      "sqlalchemy",
      "postgresql",
      "connection-pooling",
      "performance",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions path filtering for monorepo CI",
    "context": "Monorepo contains multiple services (api/, frontend/, mcp-server/). Every push runs all CI pipelines even when only one service changed. Need to run only the relevant pipelines based on which files changed.",
    "solution": "Use `paths` filter and the `dorny/paths-filter` action for selective CI:\n\n```yaml\n# .github/workflows/api-ci.yml\nname: API CI\n\non:\n  push:\n    branches: [main, 'feat/**']\n    paths:\n      - 'api/**'\n      - '.github/workflows/api-ci.yml'\n  pull_request:\n    paths:\n      - 'api/**'\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: api\n    steps:\n      - uses: actions/checkout@v4\n      - name: Install uv\n        uses: astral-sh/setup-uv@v3\n      - run: uv sync --frozen\n      - run: uv run pytest\n\n# For complex path-based conditions in a single workflow:\n# .github/workflows/ci.yml\nname: Monorepo CI\n\non:\n  push:\n    branches: [main]\n\njobs:\n  detect-changes:\n    runs-on: ubuntu-latest\n    outputs:\n      api: ${{ steps.filter.outputs.api }}\n      frontend: ${{ steps.filter.outputs.frontend }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          filters: |\n            api:\n              - 'api/**'\n              - 'docker-compose.yml'\n            frontend:\n              - 'frontend/**'\n\n  api-tests:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.api == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run API tests\n        working-directory: api\n        run: make test\n\n  frontend-tests:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.frontend == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run frontend tests\n        working-directory: frontend\n        run: npm test\n\n  # Always runs to provide a consistent required status check\n  ci-success:\n    needs: [api-tests, frontend-tests]\n    if: always()\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"CI complete\"\n```\n\nThe `paths` filter at the `on:` level skips the workflow entirely. `dorny/paths-filter` gives per-job control within a workflow. Always include a final `ci-success` job with `if: always()` to satisfy required status checks when earlier jobs are skipped.",
    "tags": [
      "github-actions",
      "monorepo",
      "ci-cd",
      "path-filtering"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "SQLAlchemy async session dependency in FastAPI with rollback on error",
    "context": "FastAPI database dependency using SQLAlchemy async session. Need automatic transaction rollback when route handlers raise exceptions, and ensure sessions are always closed without leaking connections. Current implementation doesn't roll back on unhandled exceptions.",
    "solution": "Use `async with session.begin()` inside the dependency for automatic transaction management:\n\n```python\n# app/dependencies.py\nfrom sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, AsyncEngine\nfrom fastapi import Depends, Request\nfrom typing import AsyncGenerator\n\ndef get_session_factory(request: Request) -> async_sessionmaker:\n    return request.app.state.session_factory\n\nasync def get_db(\n    session_factory: async_sessionmaker = Depends(get_session_factory),\n) -> AsyncGenerator[AsyncSession, None]:\n    async with session_factory() as session:\n        try:\n            yield session\n            await session.commit()  # Commit if no exception\n        except Exception:\n            await session.rollback()  # Rollback on any exception\n            raise  # Re-raise to let FastAPI handle the response\n        # Session is closed automatically by the context manager\n\n# Alternative: use begin() for automatic commit/rollback\nasync def get_db_with_transaction(\n    session_factory: async_sessionmaker = Depends(get_session_factory),\n) -> AsyncGenerator[AsyncSession, None]:\n    async with session_factory() as session:\n        async with session.begin():  # Automatically commits or rolls back\n            yield session\n\n# For read-only routes (no commit needed)\nasync def get_db_readonly(\n    session_factory: async_sessionmaker = Depends(get_session_factory),\n) -> AsyncGenerator[AsyncSession, None]:\n    async with session_factory() as session:\n        yield session\n        # No commit \u2014 read-only, session closed by context manager\n\n# Usage in routes\n@router.post('/traces', status_code=201)\nasync def create_trace(\n    data: TraceCreate,\n    session: AsyncSession = Depends(get_db),\n) -> TraceResponse:\n    trace = Trace(**data.model_dump())\n    session.add(trace)\n    await session.flush()  # Get ID without committing\n    return TraceResponse.model_validate(trace)\n    # get_db commits after this returns\n    # If an exception occurs, get_db rolls back\n\n# Test override\nasync def get_test_db(test_session):\n    yield test_session  # Don't commit in tests \u2014 use rollback fixture\n\napp.dependency_overrides[get_db] = lambda: get_test_db(test_session)\n```\n\nThe `try/except` pattern in `get_db` guarantees rollback on any exception (including HTTPException). Use `session.flush()` to get auto-generated IDs without committing \u2014 the dependency handles the final commit. `async with session_factory()` ensures the session is closed even if `yield` raises.",
    "tags": [
      "fastapi",
      "sqlalchemy",
      "async",
      "dependency-injection",
      "transactions"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Python retry logic with exponential backoff using tenacity",
    "context": "Calling external APIs (OpenAI, Stripe, GitHub) that occasionally return 429 (rate limit) or 503 (transient errors). Need automatic retry with exponential backoff, jitter to avoid thundering herd, and different retry behavior per error type.",
    "solution": "Use the `tenacity` library for declarative retry configuration:\n\n```python\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_exponential,\n    wait_random_exponential,\n    retry_if_exception_type,\n    retry_if_result,\n    before_sleep_log,\n    after_log,\n)\nimport httpx\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Basic exponential backoff\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10),  # 1s, 2s, 4s... capped at 10s\n)\nasync def call_external_api(url: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        response.raise_for_status()\n        return response.json()\n\n# Full-featured: jitter + logging + selective retry\n@retry(\n    retry=retry_if_exception_type((httpx.TimeoutException, httpx.ConnectError)),\n    wait=wait_random_exponential(min=1, max=60),  # Random jitter prevents thundering herd\n    stop=stop_after_attempt(5),\n    before_sleep=before_sleep_log(logger, logging.WARNING),\n    reraise=True,  # Re-raise last exception after all attempts exhausted\n)\nasync def embed_with_retry(text: str) -> list[float]:\n    client = AsyncOpenAI()\n    response = await client.embeddings.create(\n        input=text,\n        model='text-embedding-3-small',\n    )\n    return response.data[0].embedding\n\n# Retry on specific HTTP status codes\nasync def is_retryable(response: httpx.Response) -> bool:\n    return response.status_code in (429, 500, 502, 503, 504)\n\n@retry(\n    retry=retry_if_result(is_retryable),\n    wait=wait_exponential(min=1, max=30),\n    stop=stop_after_attempt(4),\n)\nasync def api_call_with_status_retry(url: str) -> httpx.Response:\n    async with httpx.AsyncClient() as client:\n        return await client.get(url)  # Returns response even on retryable status\n\n# Respect Retry-After header\nimport asyncio\nasync def call_with_retry_after(url: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        for attempt in range(5):\n            response = await client.get(url)\n            if response.status_code == 429:\n                retry_after = int(response.headers.get('Retry-After', 60))\n                await asyncio.sleep(retry_after)\n                continue\n            response.raise_for_status()\n            return response.json()\n    raise RuntimeError('Max retries exceeded')\n```\n\n`wait_random_exponential` adds jitter to prevent multiple clients retrying simultaneously (thundering herd). `before_sleep` logs each retry attempt. `reraise=True` ensures the original exception propagates after all retries fail.",
    "tags": [
      "python",
      "retry",
      "tenacity",
      "resilience",
      "api"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "alembic batch migration for SQLite and PostgreSQL compatibility",
    "context": "Alembic migration needs to modify an existing column (change type, add constraint) in a way that works on both PostgreSQL (production) and SQLite (tests/development). PostgreSQL supports ALTER COLUMN but SQLite does not.",
    "solution": "Use Alembic's batch operations which work on both databases:\n\n```python\n# alembic/versions/0003_change_status_column.py\n\"\"\"change status to varchar(20) with constraint\n\nRevision ID: 0003\nDowns revision: '0002'\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade() -> None:\n    # batch_alter_table works on SQLite (which doesn't support ALTER COLUMN)\n    with op.batch_alter_table('traces', schema=None) as batch_op:\n        # Change column type\n        batch_op.alter_column(\n            'status',\n            existing_type=sa.String(),\n            type_=sa.String(20),\n            existing_nullable=False,\n        )\n        # Add index within batch context\n        batch_op.create_index('idx_traces_status', ['status'])\n\n    # Add a column with a default value\n    with op.batch_alter_table('traces') as batch_op:\n        batch_op.add_column(\n            sa.Column('confirmation_count', sa.Integer(), nullable=False, server_default='0')\n        )\n        batch_op.drop_column('old_column')\n\n\ndef downgrade() -> None:\n    with op.batch_alter_table('traces') as batch_op:\n        batch_op.drop_index('idx_traces_status')\n        batch_op.alter_column(\n            'status',\n            existing_type=sa.String(20),\n            type_=sa.String(),\n        )\n        batch_op.add_column(sa.Column('old_column', sa.String()))\n        batch_op.drop_column('confirmation_count')\n\n# alembic.ini or env.py \u2014 configure for SQLite in tests\n# context.configure(\n#     render_as_batch=True,  # Enable batch mode globally\n#     ...\n# )\n```\n\nSQLite doesn't support `ALTER TABLE ... ALTER COLUMN` \u2014 batch operations work around this by creating a new table, copying data, and renaming. On PostgreSQL, batch operations issue direct DDL. Use `render_as_batch=True` in `env.py` to apply this automatically based on the database dialect.",
    "tags": [
      "alembic",
      "migrations",
      "sqlite",
      "postgresql",
      "python"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Alembic batch operations for SQLite compatibility",
    "context": "Need to run Alembic migrations that work on both PostgreSQL and SQLite. SQLite doesn't support ALTER TABLE for column renames or type changes, causing migration failures in test environments.",
    "solution": "Use Alembic batch operations context manager:\n```python\nwith op.batch_alter_table('users') as batch_op:\n    batch_op.alter_column('name', new_column_name='display_name')\n    batch_op.alter_column('age', type_=sa.String(10))\n```\nBatch mode recreates the table with changes applied atomically. Set `render_as_batch=True` in `env.py` for automatic batch wrapping.",
    "tags": [
      "python",
      "alembic",
      "sqlite",
      "migrations",
      "database"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Rate limiting with Redis sliding window counter",
    "context": "Need to implement rate limiting for an API. Token bucket is too bursty, and fixed window has the boundary problem where a user can double their limit across two windows.",
    "solution": "Use a Redis sorted set sliding window:\n```python\nimport time, redis\nr = redis.Redis()\ndef is_allowed(user_id: str, limit: int, window_secs: int) -> bool:\n    key = f'rl:{user_id}'\n    now = time.time()\n    pipe = r.pipeline()\n    pipe.zremrangebyscore(key, 0, now - window_secs)\n    pipe.zadd(key, {str(now): now})\n    pipe.zcard(key)\n    pipe.expire(key, window_secs)\n    _, _, count, _ = pipe.execute()\n    return count <= limit\n```\nSliding window gives smooth rate enforcement without boundary spikes.",
    "tags": [
      "python",
      "redis",
      "rate-limiting",
      "api",
      "backend"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript conditional types for API response handling",
    "context": "Building a typed API client where response types differ based on request parameters. Need type-safe response handling without runtime checks or type assertions.",
    "solution": "Use conditional types with generic constraints:\n```typescript\ntype ApiResponse<T extends 'list' | 'detail'> =\n  T extends 'list' ? { items: Item[]; total: number }\n  : T extends 'detail' ? { item: Item; related: Item[] }\n  : never;\n\nasync function fetchApi<T extends 'list' | 'detail'>(\n  endpoint: string, mode: T\n): Promise<ApiResponse<T>> {\n  const resp = await fetch(`${endpoint}?mode=${mode}`);\n  return resp.json();\n}\nconst list = await fetchApi('/items', 'list'); // typed as { items, total }\n```",
    "tags": [
      "typescript",
      "generics",
      "api",
      "type-safety"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions matrix strategy with fail-fast disabled",
    "context": "CI pipeline uses matrix builds for multiple Python versions but stops all jobs when one fails. Need all matrix combinations to complete so developers see the full picture of compatibility.",
    "solution": "Disable fail-fast in the matrix strategy:\n```yaml\njobs:\n  test:\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: ['3.10', '3.11', '3.12']\n        os: [ubuntu-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n      - run: pytest --tb=short\n```\nWith `fail-fast: false`, all combinations run to completion regardless of individual failures.",
    "tags": [
      "github-actions",
      "ci-cd",
      "testing",
      "python",
      "devops"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Railway deployment fails with ModuleNotFoundError despite dependencies in pyproject.toml",
    "context": "MCP server (FastMCP 3.0.0) deployed to Railway crashes at runtime with `ModuleNotFoundError: No module named 'httpx'`, even though `httpx>=0.27` is listed under `[project].dependencies` in pyproject.toml. The Dockerfile runs `pip install .` which completes without error, but none of the declared dependencies are actually installed. A secondary issue compounds the problem: Railway's health check is configured to hit `/` which returns 404 because FastMCP serves its endpoint at `/mcp`, so Railway marks the deployment as FAILED even if the server manages to start.",
    "solution": "Two root causes, both in project configuration:\n\n**1. Missing `[build-system]` table in pyproject.toml**\n\nWithout a `[build-system]` table, `pip install .` uses a legacy fallback that silently skips dependency installation. Add the table:\n\n```toml\n[build-system]\nrequires = [\"setuptools>=75.0\", \"wheel\"]\nbuild-backend = \"setuptools.backends._legacy:_Backend\"\n```\n\nOr, for belt-and-suspenders safety, also pre-install critical deps explicitly in the Dockerfile before `pip install .`:\n\n```dockerfile\nRUN pip install httpx>=0.27 fastmcp>=2.0\nRUN pip install .\n```\n\n**2. Health check path mismatch**\n\nFastMCP serves at `/mcp`, not `/`. Configure Railway's health check to match:\n\n- Set health check path to `/mcp` in Railway service settings\n- Or add a root health endpoint in your server code:\n\n```python\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\n\nasync def health(request):\n    return JSONResponse({\"status\": \"ok\"})\n\n# Add to your ASGI app's routes\n```\n\nKey points:\n- `pip install .` without `[build-system]` is a silent failure  the build succeeds but deps are missing\n- Always verify installed packages with `pip list` in a Dockerfile `RUN` step during debugging\n- Railway health checks default to `/`  confirm the actual serve path of your framework",
    "tags": [
      "python",
      "railway",
      "docker",
      "pyproject-toml",
      "fastmcp"
    ],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  }
]
