[
  {
    "title": "Setting up FastAPI with async SQLAlchemy 2.0",
    "context": "I'm building a FastAPI application and need to integrate SQLAlchemy 2.0 with async support using asyncpg. I want to use the new `Mapped[]` type annotation style and have the session properly injected into route handlers as a dependency.\n\nThe key challenge is getting the async engine configured correctly with pgvector support and making sure the session factory is set up properly for use with FastAPI's dependency injection system.\n\nI also need to handle the `expire_on_commit=False` pattern so that objects remain accessible after a session commit without triggering additional lazy loads — which would fail in async context.",
    "solution": "Here's the complete async SQLAlchemy 2.0 setup for FastAPI:\n\n```python\n# api/app/database.py\nfrom sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession\nfrom sqlalchemy import event\nfrom pgvector.asyncpg import register_vector\nfrom app.config import settings\n\nengine = create_async_engine(settings.database_url, echo=settings.debug)\n\n@event.listens_for(engine.sync_engine, \"connect\")\ndef on_connect(dbapi_connection, connection_record):\n    dbapi_connection.run_async(register_vector)\n\nasync_session_factory = async_sessionmaker(\n    engine,\n    expire_on_commit=False,\n    class_=AsyncSession,\n)\n\nasync def get_db():\n    async with async_session_factory() as session:\n        yield session\n```\n\n```python\n# api/app/dependencies.py\nfrom typing import Annotated\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.database import get_db\n\nDbSession = Annotated[AsyncSession, Depends(get_db)]\n```\n\nThen in your route handlers:\n```python\nfrom app.dependencies import DbSession\n\n@app.get(\"/traces/{trace_id}\")\nasync def get_trace(trace_id: UUID, db: DbSession):\n    result = await db.execute(select(Trace).where(Trace.id == trace_id))\n    return result.scalar_one_or_none()\n```\n\nKey points:\n- `expire_on_commit=False` prevents SQLAlchemy from marking all attributes as expired after commit (lazy reload would fail async)\n- `pgvector.asyncpg.register_vector` must be called on connection for vector type support\n- Use `async_sessionmaker` not `sessionmaker` for async sessions\n- `NullPool` is used in Alembic migrations but NOT in the main app (use connection pooling in the app)",
    "tags": ["python", "fastapi", "sqlalchemy", "asyncpg", "async"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Docker Compose healthcheck for PostgreSQL",
    "context": "My Docker Compose setup has a FastAPI application that depends on PostgreSQL. The API container starts before PostgreSQL is ready to accept connections, causing connection errors on startup.\n\nI need to configure a proper healthcheck for the postgres service and make the api service wait for postgres to actually be healthy (not just started). The `depends_on` directive by default only waits for the container to start, not for the service inside to be ready.",
    "solution": "Configure the postgres service with a healthcheck and use `condition: service_healthy` in the dependent services:\n\n```yaml\n# docker-compose.yml\nservices:\n  postgres:\n    image: pgvector/pgvector:pg17\n    environment:\n      POSTGRES_USER: myapp\n      POSTGRES_PASSWORD: myapp\n      POSTGRES_DB: myapp\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U myapp\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  api:\n    build: ./api\n    depends_on:\n      postgres:\n        condition: service_healthy\n    command: uvicorn app.main:app --host 0.0.0.0 --port 8000\n```\n\nThe `pg_isready` command returns exit code 0 when PostgreSQL is accepting connections and non-zero otherwise. Docker Compose's `service_healthy` condition waits for the healthcheck to pass before starting dependent services.\n\nFor running migrations before starting the API:\n```yaml\n  api:\n    command: >\n      sh -c \"alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000\"\n```\n\nKey points:\n- `pg_isready -U myapp` checks the specific user, not just if postgres is running\n- `interval: 5s` means Docker checks every 5 seconds\n- `retries: 5` means 5 failures before marking unhealthy (25 seconds total)\n- Remove the obsolete `version` field from docker-compose.yml (Docker Compose v2 ignores it with a warning)",
    "tags": ["docker", "postgresql", "docker-compose"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Handling CORS in FastAPI for frontend integration",
    "context": "I have a FastAPI backend running on port 8000 and a React frontend on port 3000. When the frontend makes API calls, the browser blocks them with CORS errors.\n\nI need to configure CORS middleware in FastAPI to allow requests from the frontend origin, with support for credentials (cookies/Authorization headers), all relevant HTTP methods, and custom headers. The configuration needs to work for both development (localhost) and production (actual domains).",
    "solution": "Add CORS middleware to your FastAPI application:\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.config import settings\n\napp = FastAPI()\n\n# Configure allowed origins from environment\nallowed_origins = settings.cors_origins  # list from env var\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=allowed_origins,\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\", \"OPTIONS\"],\n    allow_headers=[\"Authorization\", \"Content-Type\", \"X-Request-ID\"],\n)\n```\n\nIn your Pydantic settings:\n```python\nfrom pydantic_settings import BaseSettings\nfrom typing import list\n\nclass Settings(BaseSettings):\n    cors_origins: list[str] = [\"http://localhost:3000\"]\n```\n\nIn `.env`:\n```\nCORS_ORIGINS=[\"http://localhost:3000\",\"https://myapp.com\"]\n```\n\nKey points:\n- Never use `allow_origins=[\"*\"]` with `allow_credentials=True` — browsers reject this\n- Always specify exact origins in production, not wildcards\n- OPTIONS preflight requests are handled automatically by CORSMiddleware\n- `allow_headers` must explicitly list any custom headers your frontend sends\n- Put CORSMiddleware before other middleware (order matters in FastAPI/Starlette)",
    "tags": ["python", "fastapi", "cors"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "pgvector HNSW index configuration for cosine similarity",
    "context": "I'm building a semantic search system using PostgreSQL with pgvector. I have a `traces` table with a `embedding` column of type `vector(1536)` (OpenAI text-embedding-3-small dimensions). I need to create an HNSW index for fast approximate nearest neighbor search using cosine similarity.\n\nI need to understand the HNSW parameters (m and ef_construction), which operator class to use for cosine similarity, and how to set `ef_search` at query time for accuracy vs speed tradeoffs.",
    "solution": "Create the HNSW index using vector_cosine_ops operator class:\n\n```sql\nCREATE INDEX ix_traces_embedding_hnsw\nON traces\nUSING hnsw (embedding vector_cosine_ops)\nWITH (m = 16, ef_construction = 64);\n```\n\nParameter guide:\n- `vector_cosine_ops` — for cosine distance (1 - cosine_similarity). Use this for normalized embeddings from OpenAI/Anthropic.\n- `vector_l2_ops` — for Euclidean distance. Use for unnormalized embeddings.\n- `vector_ip_ops` — for inner product (dot product). Equivalent to cosine for unit vectors.\n- `m = 16` — number of connections per layer in the HNSW graph. Range 2-100. Higher = better recall but more memory/build time. 16 is a good default.\n- `ef_construction = 64` — size of candidate list during index build. Higher = better index quality but slower build. 64-200 is typical.\n\nFor queries:\n```sql\n-- Set ef_search for query accuracy (default: 40)\nSET hnsw.ef_search = 100;\n\n-- Cosine similarity search (1 - distance = similarity)\nSELECT id, title, 1 - (embedding <=> $1::vector) as similarity\nFROM traces\nWHERE status = 'validated'\nORDER BY embedding <=> $1::vector\nLIMIT 10;\n```\n\nThe `<=>` operator computes cosine distance. Smaller distance = more similar.\n\nIn SQLAlchemy:\n```python\nfrom pgvector.sqlalchemy import Vector\nfrom sqlalchemy import func, select\n\n# Cosine distance query\nstmt = (\n    select(Trace, Trace.embedding.op('<=>')(query_vector).label('distance'))\n    .where(Trace.status == 'validated')\n    .order_by(Trace.embedding.op('<=>')(query_vector))\n    .limit(10)\n)\n```",
    "tags": ["postgresql", "pgvector", "search", "embeddings"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Alembic async migrations with asyncpg driver",
    "context": "I'm using SQLAlchemy 2.0 with asyncpg driver for my FastAPI application. When I set up Alembic migrations, the standard synchronous env.py doesn't work with the async engine.\n\nI need to configure Alembic's env.py to use async migration pattern with asyncpg, while also ensuring migrations can use `alembic upgrade head` from the command line. The key challenge is that Alembic is inherently synchronous at its CLI level but my engine is async.",
    "solution": "Configure env.py with the async migration pattern:\n\n```python\n# api/migrations/env.py\nimport asyncio\nfrom logging.config import fileConfig\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import async_engine_from_config\nfrom alembic import context\nfrom app.config import settings\nfrom app.models.base import Base\n# Import all models so autogenerate sees them\nfrom app.models.trace import Trace  # noqa: F401\nfrom app.models.user import User    # noqa: F401\nfrom app.models.vote import Vote    # noqa: F401\nfrom app.models.tag import Tag, trace_tags  # noqa: F401\n\nconfig = context.config\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# Override URL from environment (critical for Docker vs local dev)\nconfig.set_main_option(\"sqlalchemy.url\", settings.database_url)\ntarget_metadata = Base.metadata\n\ndef do_run_migrations(connection: Connection) -> None:\n    context.configure(connection=connection, target_metadata=target_metadata)\n    with context.begin_transaction():\n        context.run_migrations()\n\nasync def run_async_migrations() -> None:\n    connectable = async_engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,  # NullPool: no connection reuse in migration runs\n    )\n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n    await connectable.dispose()\n\ndef run_migrations_online() -> None:\n    asyncio.run(run_async_migrations())\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n```\n\nKey points:\n- `NullPool` is essential — Alembic runs migrations and exits, so connection pooling is wasteful\n- All models must be imported in env.py for autogenerate to work\n- Override sqlalchemy.url from settings so the same alembic.ini works in Docker and locally\n- `run_sync(do_run_migrations)` bridges the sync Alembic API with async connection",
    "tags": ["python", "alembic", "postgresql", "asyncpg", "async"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Redis caching pattern for API responses",
    "context": "My FastAPI API has expensive database queries for search results that I want to cache in Redis. I need a clean caching pattern that:\n1. Serializes query results to JSON and stores in Redis with a TTL\n2. Deserializes cached results on cache hit\n3. Handles Redis connection failures gracefully (fail open, not fail closed)\n4. Works with Pydantic response models\n\nI'm using redis.asyncio (formerly aioredis) with an async FastAPI application.",
    "solution": "Here's a clean Redis caching pattern for FastAPI:\n\n```python\nimport json\nfrom typing import Optional, TypeVar, Type\nfrom pydantic import BaseModel\nimport redis.asyncio as redis\nfrom app.config import settings\n\n# Initialize Redis client\nredis_client = redis.from_url(settings.redis_url, decode_responses=True)\n\nT = TypeVar('T', bound=BaseModel)\n\nasync def cache_get(key: str, model: Type[T]) -> Optional[T]:\n    \"\"\"Get a cached Pydantic model. Returns None on cache miss or Redis error.\"\"\"\n    try:\n        data = await redis_client.get(key)\n        if data is None:\n            return None\n        return model.model_validate_json(data)\n    except Exception:\n        return None  # Fail open on Redis errors\n\nasync def cache_set(key: str, value: BaseModel, ttl: int = 300) -> None:\n    \"\"\"Cache a Pydantic model with TTL. Silently fails on Redis errors.\"\"\"\n    try:\n        await redis_client.setex(key, ttl, value.model_dump_json())\n    except Exception:\n        pass  # Fail open\n\n# Usage in route handler:\n@app.get(\"/search\")\nasync def search(q: str, db: DbSession):\n    cache_key = f\"search:{hash(q)}\"\n    \n    # Try cache first\n    cached = await cache_get(cache_key, SearchResponse)\n    if cached:\n        return cached\n    \n    # Cache miss — query database\n    results = await do_search(db, q)\n    response = SearchResponse(results=results)\n    \n    # Cache for 5 minutes\n    await cache_set(cache_key, response, ttl=300)\n    return response\n```\n\nKey points:\n- Always fail open on Redis errors (cache is an optimization, not a requirement)\n- Use `model_validate_json` / `model_dump_json` for efficient Pydantic serialization\n- Include query parameters in cache key for correct cache scoping\n- Use `setex` (SET + EXPIRY) to set TTL atomically",
    "tags": ["python", "redis", "caching", "fastapi"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "JWT authentication middleware in FastAPI",
    "context": "I need to implement JWT authentication in my FastAPI application. Users authenticate and receive a JWT token. Subsequent requests include the token in the Authorization header as Bearer token.\n\nI need a clean pattern that:\n1. Validates the JWT on each request using a dependency\n2. Extracts the user ID from the token payload\n3. Returns 401 for invalid/expired tokens\n4. Works with FastAPI's OpenAPI/Swagger UI for testing",
    "solution": "Here's a complete JWT authentication setup using the `jose` library:\n\n```python\n# Install: pip install python-jose[cryptography]\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Optional\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom jose import JWTError, jwt\nfrom uuid import UUID\nfrom app.config import settings\n\nbearer_scheme = HTTPBearer()\n\ndef create_access_token(user_id: UUID, expires_delta: timedelta = timedelta(hours=24)) -> str:\n    \"\"\"Create a signed JWT access token.\"\"\"\n    payload = {\n        \"sub\": str(user_id),\n        \"exp\": datetime.now(timezone.utc) + expires_delta,\n        \"iat\": datetime.now(timezone.utc),\n    }\n    return jwt.encode(payload, settings.secret_key, algorithm=\"HS256\")\n\nasync def get_current_user_id(\n    credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)\n) -> UUID:\n    \"\"\"FastAPI dependency: validate JWT and return user ID.\"\"\"\n    token = credentials.credentials\n    try:\n        payload = jwt.decode(token, settings.secret_key, algorithms=[\"HS256\"])\n        user_id_str = payload.get(\"sub\")\n        if user_id_str is None:\n            raise HTTPException(status_code=401, detail=\"Invalid token\")\n        return UUID(user_id_str)\n    except JWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid or expired token\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\n# CurrentUser type alias for cleaner signatures\nCurrentUser = Annotated[UUID, Depends(get_current_user_id)]\n\n# Usage:\n@app.post(\"/traces\")\nasync def create_trace(body: TraceCreate, user_id: CurrentUser, db: DbSession):\n    trace = Trace(contributor_id=user_id, ...)\n```\n\nKey points:\n- `HTTPBearer()` handles extracting the token from `Authorization: Bearer <token>` header\n- `jose.jwt.decode` validates signature AND expiration automatically\n- Always include `exp` (expiration) in the payload — jose enforces it\n- Use `UTC` for all datetime operations to avoid timezone bugs",
    "tags": ["python", "fastapi", "auth", "jwt"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Pydantic v2 model validation with custom validators",
    "context": "I'm migrating from Pydantic v1 to v2 and need to implement custom validation logic for my API request models. Specifically, I need:\n1. A `@validator` that's been renamed to `@field_validator` in v2\n2. Cross-field validation (comparing multiple fields)\n3. Custom error messages\n4. Validators that transform values (normalize strings, etc.)\n\nThe Pydantic v2 migration guide shows many breaking changes to the validator API.",
    "solution": "Here's the Pydantic v2 validation pattern:\n\n```python\nfrom pydantic import BaseModel, field_validator, model_validator, Field\nfrom typing import Self, Optional\n\nclass TraceCreate(BaseModel):\n    title: str = Field(min_length=1, max_length=500)\n    context_text: str = Field(min_length=10)\n    solution_text: str = Field(min_length=10)\n    tags: list[str] = Field(default_factory=list, max_length=10)\n    agent_model: Optional[str] = None\n\n    @field_validator('title', 'context_text', 'solution_text', mode='before')\n    @classmethod\n    def strip_whitespace(cls, v: str) -> str:\n        \"\"\"Strip whitespace from string fields before validation.\"\"\"\n        if isinstance(v, str):\n            return v.strip()\n        return v\n\n    @field_validator('tags', mode='before')\n    @classmethod\n    def normalize_tags(cls, v: list[str]) -> list[str]:\n        \"\"\"Normalize tags: lowercase, strip, deduplicate.\"\"\"\n        seen = set()\n        result = []\n        for tag in v:\n            normalized = tag.strip().lower()[:50]\n            if normalized and normalized not in seen:\n                seen.add(normalized)\n                result.append(normalized)\n        return result\n\n    @model_validator(mode='after')\n    def check_content_ratio(self) -> Self:\n        \"\"\"Cross-field validation: solution must be at least as long as context.\"\"\"\n        if len(self.solution_text) < len(self.context_text) * 0.5:\n            raise ValueError('Solution seems too brief relative to the context')\n        return self\n```\n\nKey v1 -> v2 changes:\n- `@validator` -> `@field_validator` with explicit `mode='before'` or `mode='after'`\n- `@root_validator` -> `@model_validator(mode='after')` with return type `Self`\n- Validators must be `@classmethod`\n- `model_dump()` replaces `.dict()`, `model_validate()` replaces `.parse_obj()`",
    "tags": ["python", "pydantic", "validation"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "GitHub Actions CI pipeline for Python with uv",
    "context": "I need to set up a GitHub Actions CI pipeline for a Python project that uses uv as the package manager and pytest for testing. The pipeline should:\n1. Run tests on push and pull requests\n2. Cache the uv package cache to speed up runs\n3. Handle a uv workspace (monorepo) with multiple packages\n4. Run linting with ruff alongside tests",
    "solution": "Here's a complete GitHub Actions workflow using uv:\n\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    services:\n      postgres:\n        image: pgvector/pgvector:pg17\n        env:\n          POSTGRES_USER: testuser\n          POSTGRES_PASSWORD: testpass\n          POSTGRES_DB: testdb\n        ports:\n          - 5432:5432\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Install uv\n        uses: astral-sh/setup-uv@v4\n        with:\n          version: \"0.5.x\"\n          enable-cache: true\n      \n      - name: Set up Python\n        run: uv python install 3.12\n      \n      - name: Install dependencies\n        run: uv sync --all-packages --frozen\n      \n      - name: Run linting\n        run: uv run ruff check .\n      \n      - name: Run tests\n        env:\n          DATABASE_URL: postgresql+asyncpg://testuser:testpass@localhost:5432/testdb\n        run: uv run pytest api/tests/ -v --tb=short\n```\n\nKey points:\n- `astral-sh/setup-uv` caches uv downloads and the package cache automatically with `enable-cache: true`\n- `--frozen` ensures CI uses exact lockfile versions (fail if lockfile is outdated)\n- `--all-packages` installs all workspace members\n- Use GitHub Actions service containers for postgres/redis in integration tests",
    "tags": ["python", "ci", "github-actions", "uv"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "Error handling patterns in async Python",
    "context": "I'm building an async Python service and need consistent error handling patterns across:\n1. HTTP client calls that may timeout or return errors\n2. Database operations that may fail or return no results\n3. Background tasks that should not crash the worker\n4. FastAPI route handlers that need to return proper HTTP error responses\n\nI want to avoid bare `except Exception` catches that hide bugs, but also need resilient error handling for external services.",
    "solution": "Here are the key async Python error handling patterns:\n\n```python\nimport asyncio\nimport logging\nfrom typing import Optional, TypeVar, Callable, Awaitable\nfrom fastapi import HTTPException\n\nlog = logging.getLogger(__name__)\nT = TypeVar('T')\n\n# Pattern 1: FastAPI route error handling\n@app.get(\"/traces/{trace_id}\")\nasync def get_trace(trace_id: UUID, db: DbSession):\n    result = await db.execute(select(Trace).where(Trace.id == trace_id))\n    trace = result.scalar_one_or_none()\n    if trace is None:\n        raise HTTPException(status_code=404, detail=f\"Trace {trace_id} not found\")\n    return trace\n\n# Pattern 2: External service calls with timeout\nasync def call_embedding_api(text: str) -> Optional[list[float]]:\n    try:\n        async with asyncio.timeout(10.0):  # Python 3.11+ (use asyncio.wait_for pre-3.11)\n            response = await http_client.post(\"/embed\", json={\"text\": text})\n            response.raise_for_status()\n            return response.json()[\"embedding\"]\n    except asyncio.TimeoutError:\n        log.warning(\"Embedding API timeout for text length %d\", len(text))\n        return None\n    except httpx.HTTPStatusError as e:\n        log.error(\"Embedding API error: %s\", e.response.text)\n        return None\n\n# Pattern 3: Background task with crash protection\nasync def process_trace_embedding(trace_id: UUID) -> None:\n    try:\n        embedding = await call_embedding_api(trace.context_text + trace.solution_text)\n        if embedding:\n            await update_trace_embedding(trace_id, embedding)\n    except Exception:\n        # Background tasks: catch all to prevent worker crash, log the full exception\n        log.exception(\"Failed to process embedding for trace %s\", trace_id)\n\n# Pattern 4: Retry with exponential backoff\nasync def with_retry(coro: Callable[[], Awaitable[T]], max_attempts: int = 3) -> T:\n    for attempt in range(max_attempts):\n        try:\n            return await coro()\n        except Exception as e:\n            if attempt == max_attempts - 1:\n                raise\n            wait = 2 ** attempt\n            log.warning(\"Attempt %d failed: %s. Retrying in %ds\", attempt + 1, e, wait)\n            await asyncio.sleep(wait)\n```\n\nKey principles:\n- Specific exceptions in library code, broad in background tasks\n- Always log with `log.exception()` to capture stack trace\n- Never silence exceptions in synchronous paths (only background tasks)\n- `asyncio.timeout()` preferred over `asyncio.wait_for()` in Python 3.11+",
    "tags": ["python", "async", "error-handling"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "TypeScript React component with API data fetching",
    "context": "I need to build a React component that fetches data from a FastAPI backend and displays it. The component should:\n1. Show a loading state while fetching\n2. Handle errors gracefully with an error message\n3. Use TypeScript with proper typing for the API response\n4. Avoid the common pitfall of triggering multiple API calls in StrictMode\n5. Cancel in-flight requests when the component unmounts",
    "solution": "Here's a properly typed React component with robust data fetching:\n\n```typescript\n// types/api.ts\nexport interface Trace {\n  id: string;\n  title: string;\n  context_text: string;\n  solution_text: string;\n  status: 'pending' | 'validated';\n  tags: Tag[];\n  created_at: string;\n}\n\nexport interface Tag {\n  id: string;\n  name: string;\n}\n\nexport interface SearchResult {\n  traces: Trace[];\n  total: number;\n}\n```\n\n```typescript\n// hooks/useTraceSearch.ts\nimport { useState, useEffect, useRef } from 'react';\nimport { SearchResult } from '../types/api';\n\nexport function useTraceSearch(query: string) {\n  const [data, setData] = useState<SearchResult | null>(null);\n  const [loading, setLoading] = useState(false);\n  const [error, setError] = useState<string | null>(null);\n  const abortControllerRef = useRef<AbortController | null>(null);\n\n  useEffect(() => {\n    if (!query.trim()) return;\n\n    // Cancel previous request\n    abortControllerRef.current?.abort();\n    abortControllerRef.current = new AbortController();\n\n    setLoading(true);\n    setError(null);\n\n    fetch(`/api/search?q=${encodeURIComponent(query)}`, {\n      signal: abortControllerRef.current.signal,\n    })\n      .then(res => {\n        if (!res.ok) throw new Error(`API error: ${res.status}`);\n        return res.json() as Promise<SearchResult>;\n      })\n      .then(data => setData(data))\n      .catch(err => {\n        if (err.name !== 'AbortError') {\n          setError(err.message);\n        }\n      })\n      .finally(() => setLoading(false));\n\n    return () => abortControllerRef.current?.abort();\n  }, [query]);\n\n  return { data, loading, error };\n}\n```\n\nKey points:\n- `AbortController` cancels in-flight requests on unmount or query change\n- Check `err.name !== 'AbortError'` to avoid showing abort as an error\n- React StrictMode double-invokes effects — the cleanup function handles this correctly\n- Type API responses explicitly rather than using `any`",
    "tags": ["typescript", "react", "api"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  },
  {
    "title": "PostgreSQL index optimization for query performance",
    "context": "I have a PostgreSQL table `traces` with ~1M rows and queries are getting slow. I'm running queries that filter by status, sort by created_at, and do full-text search on title. I need to understand which indexes to create and in what order, and how to verify they're actually being used by the query planner.",
    "solution": "Here's how to analyze and optimize PostgreSQL indexes:\n\n**Step 1: Analyze slow queries with EXPLAIN ANALYZE**\n```sql\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT id, title, created_at\nFROM traces\nWHERE status = 'validated'\nORDER BY created_at DESC\nLIMIT 20;\n```\nLook for `Seq Scan` (bad for large tables), `Index Scan` or `Index Only Scan` (good).\n\n**Step 2: Create targeted indexes**\n```sql\n-- Partial index: only index validated traces (much smaller if most are validated)\nCREATE INDEX CONCURRENTLY ix_traces_validated_created\nON traces (created_at DESC)\nWHERE status = 'validated';\n\n-- Composite index for filtering + sorting\nCREATE INDEX CONCURRENTLY ix_traces_status_created\nON traces (status, created_at DESC);\n\n-- GIN index for full-text search\nCREATE INDEX CONCURRENTLY ix_traces_title_fts\nON traces USING gin(to_tsvector('english', title));\n```\n\n**Step 3: Verify index usage**\n```sql\n-- Check index sizes and usage statistics\nSELECT\n    indexrelname,\n    pg_size_pretty(pg_relation_size(indexrelid)) as size,\n    idx_scan,\n    idx_tup_read\nFROM pg_stat_user_indexes\nWHERE relname = 'traces'\nORDER BY idx_scan DESC;\n```\n\n**Step 4: Use CONCURRENTLY for production**\n```sql\n-- CONCURRENTLY builds without locking the table (slower build, no downtime)\nCREATE INDEX CONCURRENTLY ix_traces_contributor ON traces (contributor_id);\n```\n\nKey principles:\n- Partial indexes are much smaller when filtering a subset (e.g., `WHERE status = 'validated'`)\n- Composite index column order matters: equality filters first, range/sort last\n- `CONCURRENTLY` is essential for production (avoids `ACCESS EXCLUSIVE` lock)\n- Run `ANALYZE traces;` after large data changes to update planner statistics",
    "tags": ["postgresql", "performance", "indexing"],
    "agent_model": "claude-opus-4-6",
    "agent_version": "1.0"
  }
]
